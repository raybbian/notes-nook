\lecture{2}{Thu 11 Jan 2024 12:32}{Divide and Conquer}

\section{Divide and Conquer}

The main idea of divide and conquer is to split the problem into smaller subproblems, and solve those subproblems by doing the same. Finally, we combine the solutions together to solve the main problem.

\begin{eg}
	One example of a divide and conquer is Merge Sort.
\end{eg}

% \usepackage{algorithm,algorithmicx,algpseudocode}
\begin{algorithm}[H]
	\floatname{algorithm}{Algorithm}
	\algrenewcommand\algorithmicrequire{\( A \), \textbf{an array}}
	\algrenewcommand\algorithmicensure{\textbf{Sorted array}}
	\caption{Merge Sort}\label{alg:merge-sort}
	\begin{algorithmic}[1]
		\Require $input$
		\Ensure $output$
		\If{\( n=1 \)}
			\State \textbf{return} \( A \)
		\EndIf
		\State left = MergeSort(\( A[0\ldots \left\lceil \frac{n}{2} \right\rceil ] -1\))
		\State right = MergeSort(\( A[\left\lceil \frac{n}{2} \right\rceil \ldots n-1] \))
		\State \textbf{return} Merge(left, right)
	\end{algorithmic}
\end{algorithm}

% \usepackage{algorithm,algorithmicx,algpseudocode}
\begin{algorithm}[H]
	\floatname{algorithm}{Algorithm}
	\algrenewcommand\algorithmicrequire{\( L,R \), \textbf{two sorted arrays}}
	\algrenewcommand\algorithmicensure{\textbf{Merged sorted array}}
	\caption{Merge}\label{alg:merge-algo}
	\begin{algorithmic}[1]
		\Require $input$
		\Ensure $output$
		\State \( l=0, r=0, i=0 \)
		\While{\( l<a \) and \( r<b \)}
			\If{\( L[l] < R[r] \)}
				\State output[\( i \)] = \( L[l] \)
				\State l += 1 
			\Else
				\State output[\( i \)] = \( R[r] \)
				\State r += 1
			\EndIf
			\State i += 1
		\EndWhile
		\If{\( l=a \)}
			\State output.append(\( R[r\ldots b] \))
		\Else
			\State output.append(\( L[l\ldots a] \))
		\EndIf
		\State \textbf{return} $state$
	\end{algorithmic}
\end{algorithm}

However, how do we prove that such an algorithm is correct?

\begin{lemma}
	Merge(\( L,R \)) correctly merges \( L \) and \( R \) such that the output is sorted if \( L \) and \( R \) are sorted.
\end{lemma}
\begin{proof}
	We wish to show that after the \( i \)-th (1 indexed) iteration of the while loop, output[\( 1\ldots i \)] is sorted. We will proceed with induction on \( i \).
	\begin{description}
		\item[Base case] \( i = 0 \). In this case, nothing has occurred yet, and nothing is in the output, so the output is sorted.
		\item[Inductive step] By the inductive hypothesis, output[\( 0\ldots i-1 \)] is sorted. Let \( l,r \) be the value of variables \( l,r \) at the start of the \( i \)-th iteration. Note that output\( [i-1]  \) is either\[
				\begin{cases}
					L[l-1] &= \min(L[l-1],R[r]) \\ &\le \min(L[l], R[r]) \\ &= \text{output}[i] \text{ if prev. was } L \\
					R[r-1] &= \min(L[l],R[r-1]) \\ &\le \min(L[l], R[r]) \\ &= \text{output}[i] \text{ if prev. was } R
				\end{cases}
			,\] which means that output[\( i-1 \)] \( \le  \) output[i], which means that output[\( 0\ldots i \)] is also sorted.
			After the while loop, output[\( i-1 \)] is either \[
				\begin{cases}
					L[a] &\le R[r] \\ &\implies \text{out.append(\( R[r\ldots b] \)) sorted} \\
					R[b] &\le L[l] \\ &\implies \text{out.append(\( R[l\ldots a] \)) sorted} \\
				\end{cases}
			.\] Therefore, the output is a sorted array.
	\end{description}
\end{proof}

\begin{theorem}
	Merge sort is correct.
\end{theorem}
\begin{proof}
	We will process with induction on \( n \).
	\begin{description}
		\item[Base case] \( n=1 \). Then, the array \( A \) is just one element, such that \( A \) is sorted. MergeSort returns \( A \), so it returns a sorted array.
		\item[Inductive step] We assume that merge sort works for all arrays with size \( < n \). Let \( A \) be an array of length \( n \). Note that \( A[0\ldots \left\lceil \frac{n}{2} \right\rceil ]-1 \) and \( A[\left\lceil \frac{n}{2} \right\rceil \ldots n-1] \) are both shorter arrays of length \( <n \). Therefore, by the inductive hypothesis, we know these two subarrays must be sorted. Since merge is correct, the output must also be sorted.
	\end{description}
\end{proof}

Now, let us argue about the time complexity of merge sort.

\begin{theorem}
	Merge sort has time complexity \( n\log n \).
\end{theorem}
\begin{proof}
	Let \( T(n) \) be the time complexity for sorting an \( n \)-length array. Note that \( T(n)=2\cdot T(\frac{n}{2}) + O(n) \). This is because merge is linear. We have the base case of \( T(1)=1 \). 

	Let's look at the recursion tree. At the \( i \)-th level, there are \( 2^i \) calls to merge sort, and the input has length \( \frac{n}{2^i} \). Each merge takes \( O(\frac{n}{2^i}) \) time, which implies that all merges performed at level \( i \) take \( 2^i \cdot O(\frac{n}{2^i}) = O(n)\) time.

	Because we have \( \log_2(n) \) levels, and each level takes \( O(n) \) time, then merge sort takes \( O(n\log n) \) time.
\end{proof}

\begin{theorem}
	In the general case, with \( a \) recursive calls and each subproblem of length \( \frac{n}{b} \), and \( O(n^c) \) time to combine solutions, we have \( T(n) = a \cdot T(\frac{n}{b}) + O(n^c) \).
\end{theorem}
\begin{proof}
	Then, the total time for level \( i \) is \( O(a^i \cdot \left(\frac{n}{b^i}\right)^c) \), and the total time is \( O(\sum_{i=0}^{n^{\log_b(n)}} a^i \left( \frac{n}{b^i} \right)^c ) \), which has three cases.
	\[
		\text{Total Time} = \begin{cases}
			O(n^{\log _b(a)} ), &\text{ if } a > b^{c} \\ 
			O(n^{c}\log (n) ), &\text{ if } a = b^{c} \\
			O(n^{c} ), &\text{ if } a < b^{c} 
		\end{cases}
	.\] 
\end{proof}
