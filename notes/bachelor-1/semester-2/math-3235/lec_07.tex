\lecture{7}{Tue 30 Jan 2024 14:02}{}

\begin{eg}
	Suppose that \( n+m \) independent trials with probability of success \( p \) are performed. If \( X \) is the number of successes of the first \( n \), and \( Y \) is the number of successes of the last \( m \), then \( X \) and \( Y \) are independent.
\end{eg}
\begin{explanation}
	Look at \( p_{x,y}(X=x,Y=y) \). We wish to show that this equals \( p_X(x) \cdot p_Y(y) \). Let 1 be success, 0 be failure. Then, \( \Omega = \{0,1\} ^{n+m} = (a = \{0,1\} ^{n}  , b=\{0,1\} ^{m}  )  \). Then 
	\begin{align*}
		\mathbb{P}((a, b)) &= p^{x+y} \cdot (1-p)^{m + n - (x + y)}  \\
		&= p^{x}\cdot (1-p)^{n-x} \cdot p^{y}\cdot (1-p)^{m-y}
	.\end{align*}
	Therefore, 
	\begin{align*}
		\mathbb{P}(x, y) &= \sum_{(a,b) \in \{X=x,Y=y\}  } \mathbb{P}((a,b)) \\
										 &= \binom{n}{x} \binom{m}{y} \underbrace{p^{x} (1-p)^{n-x} p^{y}(1-p)^{n-y}}_{\mathbb{P}((a, b))}     \\
		&= \binom{n}{x} p^{x}(1-p)^{n-x} \binom{m}{y}p^{y}(1-p)^{m-y}    \\
		&= p_X(x) \cdot p_Y(y)
	.\end{align*}
\end{explanation}

\begin{theorem}
	Discrete random variables \( X,Y \) are independent iff \[
		\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))
	.\] 
\end{theorem}
\begin{proof}
	We have that 
	\begin{align*}
		\mathbb{E}(g(X)h(Y)) &=  \sum_{x,y} g(x)h(y)\mathbb{P}(X=x,Y=y) \\
		&= \sum_{x,y} g(x)h(y)\mathbb{P}(X=x)\mathbb{P}(Y=y) \\
		&= \mathbb{E}(g(X)) + \mathbb{E}(h(Y))
	.\end{align*}
	In the other direction, assume that \[
		\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y)).
	.\] We must show that \[
		p_{X,Y}(x,y) = p_{X}(a)p_{Y}(b)
	.\] for all real numbers \( a,b \). Define the indicator functions \[
		g(x) = \begin{cases}
			1, &\text{ if }x=a\\
			0 &\text{ if }x\neq a
		\end{cases} ~ h(y) = \begin{cases}
			1, &\text{ if }y=b\\
			0 &\text{ if } y\neq b
		\end{cases}
	.\] Then, we have that 
	\begin{align*}
		\mathbb{E}(g(X)h(Y)) &= \sum_{x,y}g(x)h(y)\mathbb{P}(X=x,Y=y) \\
		&= \mathbb{P}(X=a,Y=b)
	.\end{align*}
	We also have that 
	\begin{align*}
		\mathbb{E}(g(X))\mathbb{E}(h(Y)) = \mathbb{P}(X=a)\mathbb{P}(Y=b)
	.\end{align*}
	Putting these two together, we have that \( X,Y \) are independent, as desired.
\end{proof}

\begin{eg}
	Suppose that \( X \) has distribution given by \( \mathbb{P}(X=-1) = \mathbb{P}(X=0) = \mathbb{P}(X=1) = \frac{1}{3} \) and \( Y \) is given by \[
		Y = \begin{cases}
			0, &\text{ if }X=0\\
			1 &\text{ if }X\neq 0
		\end{cases}
	.\] 
\end{eg}
\begin{explanation}
	We have that \( \mathbb{E}X \mathbb{E}Y = \mathbb{E}XY \) if (not only if) \( X,Y \) independent. However, \( X \) and \( Y \) are dependent here.
	\begin{align*}
		\mathbb{E}[XY] &= \mathbb{E}[X \cdot |X|] \\
		&=\frac{1}{3} \cdot -1\cdot |-1| + \frac{1}{3}\cdot 0\cdot |0|+\frac{1}{3} \cdot 1\cdot |1| \\
		&= 0 
	.\end{align*}
	and 
	\begin{align*}
		\mathbb{E}X &= \sum_{x}x\cdot \mathbb{P}(X=x) \\
		&= -1\cdot \frac{1}{3}+0\cdot \frac{1}{3}+1\cdot \frac{1}{3} = 0 
	.\end{align*}
	and 
	\begin{align*}
		\mathbb{E}Y &= \mathbb{E}(|X|) = \frac{2}{3} 
	.\end{align*}
	This means that \[
		\mathbb{E}XY = \mathbb{E}X\mathbb{E}Y 
	.\] which shows that this property is not bidirectional.
\end{explanation}

\begin{theorem}
	(Convolution Formula) Set \( Z=X+Y \), \( X,Y \) independent. Then for all \( z \in \mathbb{R} \),
	\begin{align*}
		\mathbb{P}(Z=z) &= \sum_{x} \mathbb{P}(X=x,Y=z-x) \\ &= \sum_{x}\mathbb{P}(X=x)\mathbb{P}(Y=z-x) 
	.\end{align*}
\end{theorem}

\begin{eg}
	If \( X \) and \( Y \) are independent discrete random variables, \( X \) having the Poisson distribution with parameter \( \lambda  \) and \( Y \) has Poisson disrubtion with parameter \( \mu  \), show that \( X+Y \) has poisson distrubtion with parameter \( \lambda +\mu  \).
\end{eg}
\begin{explanation}
	Let \( Z=X+Y \). Remember that \[
		\mathbb{P}(X=x) = \frac{e^{-\lambda }\lambda ^{x}  }{x!} \quad \text{and} \quad \mathbb{P}(Y=y) = \frac{e^{-\mu }\mu ^{y}  }{y!}
	.\] Then, 
	\begin{align*}
		\mathbb{P}(Z=z) &= \sum_{x=0}^{\infty} \mathbb{P}(X=x)\mathbb{P}(Y=z-x)\\
		&= \sum_{x=0}^{z}\mathbb{P}(X=x)\mathbb{P}(Y=z-x)  \\
		&= \sum_{x=0}^{z}\frac{e^{-\lambda }\lambda ^{x}  }{x!}\cdot \frac{e^{-\mu }\mu ^{z-x}  }{(z-x)!}  \\
		&= \sum_{x=0}^{z} e^{-\lambda -\mu }\frac{1}{z!}\frac{z!}{x!(z-x)!} \cdot \lambda ^{x}\mu ^{z-x}   \\
		&= e^{-\lambda -\mu }\frac{1}{z!}\sum_{x=0}^{z} \binom{z}{x}\lambda ^{x}\mu ^{z-x}    \\
		&= e^{-\lambda -\mu }\frac{1}{z!}(\lambda +\mu )^{z}  
	.\end{align*}
	which is precisely the Poisson distribution with \( \lambda +\mu  \).
\end{explanation}

\begin{theorem}
	Let \( A_{1},A_{2}\ldots A_n \) be events. Then, we have that \[
		\sum_{i=0}^{n} \mathbb{I}_{A_i}(\omega ) = \text{number of events that \( \omega  \) occurs}
	.\] 
\end{theorem}

\begin{eg}
	The \( 2n \) seats around a circular table are numbered clockwise. Queens sit in odd numbered seats and Kings in even numbesr. Let \( N \) be the number of queens sitting next to their king. Find the mean and variance of \( N \).
\end{eg}
\begin{explanation}
	Let \( A_i \) be the event that the \( i \)-th pair sit together. Then, \[
		N = \sum_{i=1}^{n} \mathbb{I}_{A_i}
	.\] Note that \( \mathbb{P}(A_i) =\frac{2}{n}\). Think of a fixed king permutation, then there are two spots out of \( n \) spots for the queen to sit. Next, 
	\begin{align*}
		\mathbb{E}N &= \mathbb{E}\left( \sum_{i=1}^{n} \mathbb{I}_{A_i} \right) \\
		&= \sum_{i=1}^{n} \mathbb{E} \mathbb{I}_{A_i} \\
		&= \sum_{i=1}^{n} \mathbb{P}(A_i) \\
		&= \sum_{i=1}^{n} \frac{2}{n} = 2 
	.\end{align*}
	The variance calculation is more involved. Remember that \[
		\Var(N) = \mathbb{E}N^{2} - (\mathbb{E}N)^{2} 
	.\] Then, we have 
	\begin{align*}
		\mathbb{E}(N^{2} ) &= \mathbb{E}\left( \left[ \sum_{i=1}^{n} \mathbb{I}_{A_i} \right]^{2}  \right) \\
		&= \mathbb{E}\left( \sum_{i=1}^{n} \sum_{j=1}^{n} \mathbb{I}_{A_i}\mathbb{I}_{A_j} \right)  \\
		&= \mathbb{E}\left( \sum_{i=1}^{n} \mathbb{I}_{A_i} \mathbb{I}_{A_i} + 2 \sum_{1 \le i<j\le n} \mathbb{I}_{A_i} \mathbb{I}_{A_j} \right)  \\
		&= \sum_{i=1}^{n} \mathbb{E}(\mathbb{I}_{A_i}\mathbb{I}_{A_i}) + 2 \sum_{1 \le i<j\le n} \mathbb{E}(\mathbb{I}_{A_i}\mathbb{I}_{A_j}) \\
		&= \sum_{i=1}^{n} \mathbb{E}(\mathbb{I}_{A_i}) + 2 \sum_{1 \le i<j\le n} \mathbb{E}(\mathbb{I}_{A_i}\mathbb{I}_{A_j}) \\
		&= \sum_{i=1}^{n} \underbrace{\mathbb{P}(A_i)}_{\frac{2}{n}} + 2 \sum_{1 \le i<j\le n} \mathbb{P}(A_i \cap A_j) 
	.\end{align*}
	From here, we need to calculate \( \mathbb{P}(A_i \cap A_j) \).
\end{explanation}
