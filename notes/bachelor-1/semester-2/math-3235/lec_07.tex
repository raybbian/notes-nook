\lecture{7}{Tue 30 Jan 2024 14:02}{}

\begin{eg}
	Suppose that \( n+m \) independent trials with probability of success \( p \) are performed. If \( X \) is the number of successes of the first \( n \), and \( Y \) is the number of successes of the last \( m \), then \( X \) and \( Y \) are independent.
\end{eg}
\begin{explanation}
	Look at \( p_{x,y}(X=x,Y=y) \). We wish to show that this equals \( p_X(x) \cdot p_Y(y) \). Let 1 be success, 0 be failure. Then, \( \Omega = \{0,1\} ^{n+m} = (a = \{0,1\} ^{n}  , b=\{0,1\} ^{m}  )  \). Then 
	\begin{align*}
		\mathbb{P}((a, b)) &= p^{x+y} \cdot (1-p)^{m + n - (x + y)}  \\
		&= p^{x}\cdot (1-p)^{n-x} \cdot p^{y}\cdot (1-p)^{m-y}
	.\end{align*}
	Therefore, 
	\begin{align*}
		\mathbb{P}(x, y) &= \sum_{(a,b) \in \{X=x,Y=y\}  } \mathbb{P}((a,b)) \\
										 &= \binom{n}{x} \binom{m}{y} \underbrace{p^{x} (1-p)^{n-x} p^{y}(1-p)^{n-y}}_{\mathbb{P}((a, b))}     \\
		&= \binom{n}{x} p^{x}(1-p)^{n-x} \binom{m}{y}p^{y}(1-p)^{m-y}    \\
		&= p_X(x) \cdot p_Y(y)
	.\end{align*}
\end{explanation}

\begin{theorem}
	Discrete random variables \( X,Y \) are independent iff \[
		\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))
	.\] 
\end{theorem}
\begin{proof}
	We have that 
	\begin{align*}
		\mathbb{E}(g(X)h(Y)) &=  \sum_{x,y} g(x)h(y)\mathbb{P}(X=x,Y=y) \\
		&= \sum_{x,y} g(x)h(y)\mathbb{P}(X=x)\mathbb{P}(Y=y) \\
		&= \mathbb{E}(g(X)) + \mathbb{E}(h(Y))
	.\end{align*}
	In the other direction, assume that \[
		\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y)).
	.\] We must show that \[
		p_{X,Y}(x,y) = p_{X}(a)p_{Y}(b)
	.\] for all real numbers \( a,b \). Define the indicator functions \[
		g(x) = \begin{cases}
			1, &\text{ if }x=a\\
			0 &\text{ if }x\neq a
		\end{cases} ~ h(y) = \begin{cases}
			1, &\text{ if }y=b\\
			0 &\text{ if } y\neq b
		\end{cases}
	.\] Then, we have that 
	\begin{align*}
		\mathbb{E}(g(X)h(Y)) &= \sum_{x,y}g(x)h(y)\mathbb{P}(X=x,Y=y) \\
		&= \mathbb{P}(X=a,Y=b)
	.\end{align*}
	We also have that 
	\begin{align*}
		\mathbb{E}(g(X))\mathbb{E}(h(Y)) = \mathbb{P}(X=a)\mathbb{P}(Y=b)
	.\end{align*}
	Putting these two together, we have that \( X,Y \) are independent, as desired.
\end{proof}

\begin{eg}
	Suppose that \( X \) has distribution given by \( \mathbb{P}(X=-1) = \mathbb{P}(X=0) = \mathbb{P}(X=1) = \frac{1}{3} \) and \( Y \) is given by \[
		Y = \begin{cases}
			0, &\text{ if }X=0\\
			1 &\text{ if }X\neq 0
		\end{cases}
	.\] 
\end{eg}
\begin{explanation}
	We have that \( \mathbb{E}X \mathbb{E}Y = \mathbb{E}XY \) if (not only if) \( X,Y \) independent. However, \( X \) and \( Y \) are dependent here.
	\begin{align*}
		\mathbb{E}[XY] &= \mathbb{E}[X \cdot |X|] \\
		&=\frac{1}{3} \cdot -1\cdot |-1| + \frac{1}{3}\cdot 0\cdot |0|+\frac{1}{3} \cdot 1\cdot |1| \\
		&= 0 
	.\end{align*}
	and 
	\begin{align*}
		\mathbb{E}X &= \sum_{x}x\cdot \mathbb{P}(X=x) \\
		&= -1\cdot \frac{1}{3}+0\cdot \frac{1}{3}+1\cdot \frac{1}{3} = 0 
	.\end{align*}
	and 
	\begin{align*}
		\mathbb{E}Y &= \mathbb{E}(|X|) = \frac{2}{3} 
	.\end{align*}
	This means that \[
		\mathbb{E}XY = \mathbb{E}X\mathbb{E}Y 
	.\] which shows that this property is not bidirectional.
\end{explanation}
