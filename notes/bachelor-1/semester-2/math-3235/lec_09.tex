\lecture{9}{Tue 06 Feb 2024 14:16}{}

\begin{theorem}
	We have that \( G_X(s) = \mathbb{E}(s^{X} ) \).
\end{theorem}
\begin{proof}
	\begin{align*}
		\mathbb{E}(s^{X} ) &= \sum_{i=0}^{\infty} \mathbb{P}(X=i) \cdot {S^i}  \\
		&=  G_X(s) 
	.\end{align*}
\end{proof}

\begin{eg}
	What is the PGF for \( X\equiv 0 \)?
\end{eg}
\begin{explanation}
	We have that \( p_i = \mathbb{P}(X=i) \), such that \( p_{0} =1\), \( p_i = 0 \) for all \( i>0 \). Then, 
	\begin{align*}
		\mathbb{E}(s^{X} ) &= p_{0}s^{0} + \ldots  + p_i s^{i} + \ldots  = 1 \\
		&= G_X(s) 
	.\end{align*}
\end{explanation}

\begin{eg}
	What is the PGF for \( X \sim \text{Bernoulli}(p) \)?
\end{eg}
\begin{explanation}
	Remember that \( p \) is the probability for success, and \( 1-p \) is the probability for failure (Binomial). We have that \( p_{0} = 1-p \) and \( p_{1}=p \). Then, 
	\begin{align*}
		\mathbb{E}(s^{X} ) = G_X(s) &= p_{0}s^{0}+ p_{1}s^{1}   \\
		&= (1 - p) + p \cdot  s 
	.\end{align*}
\end{explanation}

\begin{theorem}
	Given \( X \) with geometric distribution with parameter \( p \), we have that \[
		\mathbb{P}(X = k) = pq^{k-1} 
	.\] where \( p + q = 1 \), and \( X  \) has probability generating function \[
		\frac{p}{q}\cdot \frac{1}{1-qs}
	.\] 
\end{theorem}

\begin{definition}
	Let \( k\ge 1 \). The \( k \)th \textbf{moment} of the random variable \( X \) is the quantity \( \mathbb{E}(X^{k} ) \).
\end{definition}

\begin{theorem}
	The \( r \)th derivative of \( G_X(s) \) for \( s=1 \) is \( \mathbb{E}(X[X-1]\ldots [X-r+1]) \) for \( r = 1,2,\ldots  \). In other words, with \( s=1, r=1 \) we can get \( G_X'(1) = \mathbb{E}(X) \).
\end{theorem}

\begin{eg}
	Use the method of generating functions to show that a random variable with Poisson distribution with parameter \( \lambda  \) has both mean and variance equal to \( \lambda  \).
\end{eg}
\begin{explanation}
	Note that \( G''(1) = \mathbb{E}(X[X-1]) = \mathbb{E}X^{2} - \mathbb{E}X \). Then,
	\begin{align*}
		\mathbb{E}X^{2}  &= \mathbb{E}[X(X-1) + X] \\
		&= \mathbb{E}(X(X-1)) + \mathbb{E}X \\
		&= G_X''(1) + G_X'(1)
	.\end{align*}
	Which means that 
	\begin{align*}
		\Var X &= \mathbb{E}(X^{2} ) - (\mathbb{E}X)^{2}  \\
		&= G_X''(1) + G_X'(1) - (G_X'(1))^{2} 
	.\end{align*}
	Recall that \( G_X(s) = e^{\lambda (s-1)}  \). Note that \( G_X'(1) = \lambda  \), which is the expectation (mean). Also, \( G_X''(1) = \lambda ^{2}  \) which means that 
	\begin{align*}
		\Var X &= \lambda ^{2} + \lambda - \lambda ^{2} = \lambda 
	.\end{align*}
\end{explanation}

\begin{theorem}
	Suppose we have \( X, Y \) such that \[
		G_X(s) = G_Y(s) 
	.\] This means that \[
		\mathbb{P}(X = k) = \mathbb{P}(Y =k ) \quad \forall k
	.\] 
\end{theorem}

\begin{theorem}
	If \( X \) and \( Y \) are independent random variables, then \( X+Y \) has generating function \[
		G_{X+Y}(s) = G_X(s) G_Y(s)
	.\] 
\end{theorem}
\begin{proof}
	\begin{align*}
		G_{X+Y}(s) &= \mathbb{E}(s^{X+Y} ) \\
		&= \mathbb{E}(s^{X}s^{Y}  ) \\
		&= \mathbb{E}(s^{X} )\mathbb{E}(s^{Y} ) \tag{Independence}\\
		&= G_X(s)G_Y(s) 
	.\end{align*}
\end{proof}

\begin{theorem}
	(Random sum formula) Let \( N \) and \( X_{1}, X_{2}\ldots  \) be independent random variables taking values in \( \mathbb{Z}_{>0} \). If \( X_i \) are identically distributed with common PGF \( G_X \), then \[
		S = X_{1}+X_{2}+\ldots +X_N
	.\] has PGF \[
		G_S(s) = G_N(G_X(s))
	.\] 
\end{theorem}
\begin{proof}
	Note that \( G_S(t) = \mathbb{E}t ^{S}  \). Recall that for partitions \( E_i \) of \( \Omega  \), \[
		\mathbb{E}X = \sum_{i=1}^{\infty} \mathbb{E}(X \mid E_i)\mathbb{P}(E_i)
	.\] Applying that \[
		G_S(t) = \mathbb{E}t ^{S} = \mathbb{E}(t ^{X_{1}+X_{2}+\ldots +X_N} )
	.\] we have that 
	\begin{align*}
		G_S(t) &= \sum_{n=0}^{\infty} \mathbb{E}(t ^{X_{1}+\ldots +X_N} \mid N=k)\mathbb{P}(N=n) \\
		&= \sum_{n=0}^{\infty} \mathbb{E}(t ^{X_{1}+\ldots +X_n} ) \mathbb{P}(N=n) \\
		&= \sum_{n=0}^{\infty} \mathbb{E}(t ^{X_{1}} )\ldots \mathbb{E}(t ^{X_n} ) \mathbb{P}(N=n) \\
		&= \sum_{n=0}^{\infty} (\mathbb{E}(t ^{X_{1}} ))^{n} \mathbb{P}(N=n)  \\
		&= \sum_{n=0}^{\infty} G_{X_1}(t)^{n} \mathbb{P}(N=n)  \\
		&= G_N(G_{X_{1}}(t))
	.\end{align*}
\end{proof}

\begin{eg}
	The hutch in the garden contains 20 pregnant rabbits. The hutch is insecure and each rabbit has a \( \frac{1}{2} \) chance of escaping overnight. The next morning, each remaining rabbit gives birth to a litter, with each mother having a random number of offspring with Poisson distribution with parameter 3.
\end{eg}
\begin{explanation}
	Let \( S \) be the number of baby bunnies. We wish to compute \( G_S(t) \) and \( \mathbb{E}S \). Let \( X_i \) be the number of rabbits in the \( i \)th litter. Let \( N \) be the number of rabbits in the hutch the next morning. Note that \( N \) is binomial with \( p=\frac{1}{2} \) and 20 trials.

	Then, \( S = X_{1}+\ldots +X_N \). Then, 
	\begin{align*}
		G_S(t) &= G_N(G_X(t)) \\
		&= G_N(e^{3(t-1)} ) 
	.\end{align*}
	Also, 
	\begin{align*}
		G_N(t) = \left(\frac{1}{2} + \frac{1}{2}t\right)^{20} 
	.\end{align*}
	Therefore, 
	\begin{align*}
		\mathbb{E}(S) &= G_S'(1) \\
									&= G_X'(1) G_N'(G_X(1)) \\
									&= 3e^{3(1-1)} \cdot \frac{1}{2}\cdot 20 \left( \frac{1}{2} + \frac{1}{2}e^{3(1-1) }  \right)^{19}  \tag{at \( t=1 \)} \\
		&= 3 \cdot \frac{1}{2} \cdot 20 \cdot 1 = 30 
	.\end{align*}
\end{explanation}
