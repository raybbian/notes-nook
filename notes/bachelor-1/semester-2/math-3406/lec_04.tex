\lecture{4}{Thu 18 Jan 2024 17:02}{Transpose, Permutations, Spaces}

\begin{definition}
	If \( A \) is an \( n\times m \) matrix, then the \textbf{transpose} \( A^{T}  \) is \[
		(A^{T})_{ij} =  A_{ji}
	.\] 
	If \( A \) is \( n\times m \), then \( A^{T}  \) is \( m\times n \).
\end{definition}

\begin{eg}
	If \( x = \begin{bmatrix}
		1 \\ 2 \\ 3
	\end{bmatrix} \), then \( x^{T} = \begin{bmatrix}
	1 & 2 & 3
	\end{bmatrix}  \)
\end{eg}

\begin{prop}
	\( (AB)^{T}=B^{T}A^{T}    \)
\end{prop}
\begin{proof}
	How do we compute \( (AB)^{T}  \)? Assume that \( B \) is just a vector \( x \). This means that \( Ax \) is just a vector
	\begin{align*}
		Ax &= x_{1}A^{1}+x_{2}A^{2} + \ldots  + x_n A^{n}
	.\end{align*} Subsequently, 
	\begin{align*}
		(Ax)^{T} = x_{1}(A^{1} )^{T} + \ldots + x_{n}(A^{n} )^{T} 
	.\end{align*}
	where \( (A^{3} )^{T}  \) is the transpose of the 3rd column, which is just the 3rd row. In other words, 
	\begin{align*}
		(Ax)^{T} &= x_{1}(A^{T} )_{1} + \ldots + x_{n}(A^{T} )_{n}  \\
		&= x^{T}A^{T}
	.\end{align*}
	With this definition, then \( (AB)^{T} = \begin{bmatrix}
		AB^{1} & AB^{2} & \ldots  & AB^{n}   
	\end{bmatrix}^{T}  \) which equals \[
		\begin{bmatrix}
			(AB^{1} )^{T}  \\
			(AB^{2} )^{T}  \\
			\ldots  \\
			(AB^{n} )^{T}  \\
		\end{bmatrix} = \begin{bmatrix}
		(B^{1} )^{T}A^{T}   \\
		(B^{2} )^{T}A^{T}   \\
		\ldots \\
		(B^{n} )^{T}A^{T}   \\
		\end{bmatrix} = B^{T}A^{T}  
	.\] There is another way to prove this, by looking at the value at \( (AB)^{T}_{ij}  \).
\end{proof}

Note that this fact can be expanded, such that \( (ABC)^{T}=C^{T}B^{T}A^{T}     \).

\begin{prop}
	Let \( x \) and \( y \) be vectors. Then, \( x^{T}y = (x \cdot y)  \).
\end{prop}
\begin{proof}
	Let \( x = \begin{bmatrix}
		1 \\ 2 \\ 3
	\end{bmatrix} \) and \( y = \begin{bmatrix}
		2 \\ 4 \\ 6
	\end{bmatrix} \). Then, we have \(
		x^{T} = \begin{bmatrix}
			1 & 2 & 3
		\end{bmatrix} 
	\) such that \[
		x^{T}y = 1 \cdot 2 + 2 \cdot 4 + 3 \cdot  6 
	.\] which is the dot product.
\end{proof}

What about the other way? Note that \( x y^{T}  \) is \( 3\times 3 \) and is a rank 1 matrix. To elaborate, let \(
	A = xy^{T} 
\)  and \( z \) be any vector. Then, we have that \[
	Az = x(y^{T}z )= (y \cdot z) x
.\] which is a multiple of \( x \).

\begin{definition}
	\( A \) is \textbf{rank} 1 because the image of \( A \) contains a line (\( x \), dimension 1).
\end{definition}

\begin{prop}
	\( (A^{-1} )^{T} = (A^{T} )^{-1}   \)
\end{prop}
\begin{proof}
	Proof with the identity.
\end{proof}

\begin{prop}
	\( x^{T}(Ay) = (x \cdot Ay) = (Ay)^{T}x = (A^{T}y\cdot x )  \) for every vector \( x,y \). Note that this can be taken as the definition of the transpose.
\end{prop}

\begin{definition}
	An \( n\times n \) matrix \( S \) is \textbf{symmetric} if \( S^{T}=S  \).
\end{definition}

In row reduction, we saw that exchanging two rows is represented by the matrix \( P_{ij} \).

\begin{eg}
	\[
		\begin{bmatrix}
			1 & 0 & 0 & 0 \\
			1 & 1 & 0 & 0 \\
			0 & 0 & 0 & 1 \\
			0 & 0 & 1 & 0 
		\end{bmatrix} = P_{34}
	.\] 
\end{eg}

\begin{definition}
	In a \textbf{permutation matrix}, all entries are 1 or 0, and there is exactly one 1 on every row. More formally, for every \( i \), the exists \( j \) such that \( P_{ij}= 1 \) and \( P_{ij'}= 0 \) for all \( j' \neq  j \).
\end{definition}

For a permutation \( \sigma  \), the permutation matrix is defined as \( P_{i\sigma (i)}=1 \) and otherwise \( P_{ij}= 0 \). Also note that if \( P,Q \) are permutation matrices, then \( PQ \) is also a permutation matrix.

Note that elimination and row exchange can be done in reverse order. Simply permute the rows, then eliminate, rather than eliminate the rows, then permute.

\begin{prop}
	If \( S \) is symmetric, we have \[
		S = LDU
	.\] and \[
		S^{T} = U^{T} D L^{T}  = S
	.\] which means we can write \( S \) as \[
		S = L D L^{T} 
	.\] 
\end{prop}

\section{Vector Spaces}

\begin{eg}
	Let \( f(x) \) be a continuous function from \( [0,1]\to R \). This is a vector space.
\end{eg}

\begin{eg}
	Let \( p(x)  \) be a polynomial of degree \( \le n \). This is also a vector space.
\end{eg}

In theory, we can have a vector space much more generally than \( \mathbb{R}^{n}  \). 

\begin{definition}
	Let \( x_{1},\ldots ,x_m \) be vectors. Then, \( V = \Span\{x_{1},\ldots ,x_m\}   \) is a \textbf{subspace}.
\end{definition}

\begin{definition}
	Let \( V \) be a vector space such that \( x_{1},\ldots ,x_m \in V \). Suppose that \( \Span \{x_{1},\ldots ,x_m\} =V  \). Then, \( V \) has \textbf{finite dimension}.
\end{definition}

\begin{definition}
	The vectors \( x_{1},\ldots ,x_m \) are a \textbf{generating set}.
\end{definition}

\begin{eg}
	\( V \), the vector space of all continuous functions, is not finite.
\end{eg}
\begin{eg}
	\( V \), the vector space of all polynomials with degree \( \le n \), is finite. Consider the span of \( 1,x,x^{2},\ldots ,x^{n}   \).
\end{eg}
