\lecture{3}{Tue 16 Jan 2024 17:00}{Matrix Algebra}

\begin{eg}
	Solve
	\[
		\underbrace{\begin{bmatrix}
			2 & 4 & -2 \\ 4 & 9 & 4 \\ -2 & -3 & 7
		\end{bmatrix}}_{A} \begin{bmatrix}
			x_{1} \\ x_{2} \\ x_{3}
		\end{bmatrix}
		= \underbrace{\begin{bmatrix}
			2 \\ 8 \\ 10
		\end{bmatrix}}_{b}
	.\] 
\end{eg}
\begin{explanation}
	\[
		x = \begin{bmatrix}
			-1 \\ 2 \\ 2
		\end{bmatrix}
	.\] 
	Let \[
		E_{12} = \begin{bmatrix}
			1 & 0 & 0 \\ -2 & 1 & 0 \\ 0 & 0 & 1
		\end{bmatrix}
	.\] Then, we have \[
		E_{12} \begin{bmatrix}
			2 \\ 8 \\ 10
		\end{bmatrix} = \begin{bmatrix}
			2 \\ 4 \\ 10
		\end{bmatrix}
	.\] Note that this is also \( E_{12}(Ax) = E_{12}b = (E_{12}A)x \)
\end{explanation}

\begin{definition}
	AB is such that \[
		A(Bx) = (AB)x
	.\] for every vector \( x \). It is defined as \[
	AB = [AB^1, AB^2, \ldots ,AB^n]
	.\] where \( B^{i}  \) is the \( i \)-th column of \( B \).
\end{definition}

\begin{theorem}
	\( Ax=b \implies (CA)x = Cb \)
\end{theorem}

\begin{theorem}
	Let \( \mathbb{R}^{n}  \) be a vector space and \( A,B : \mathbb{R}^{n} \to  \mathbb{R}^{n}   \) linear mappings. Then, \[
		A \circ B : \mathbb{R}^{n} \to  \mathbb{R}^{n}  
	.\] is also a linear transformation. Also \[
		A \circ B (x) = ABx
	.\] 
\end{theorem}

\begin{theorem}
	If \( \hat{A} \) is a linear map from \( \mathbb{R}^{n} \to  \mathbb{R}^{n}   \) then \( \hat{A}(x) = Ax \) for a matrix \( A \).
\end{theorem}
\begin{proof}
	For a linear map, we have \( \hat{A}(x+y) = \hat{A}(x) + \hat{A}(y) \) and \( \hat{A}(\alpha x) = \alpha \hat{A}(x) \). We want to show that any linear mapping is a matrix multiplication. Let \[ e_i = \begin{bmatrix}
		0 \\ 0 \\ \ldots \\ 1 \\ \ldots \\ 0
	\end{bmatrix}\] where the 1 is in the \( i\)th place. Let \( A^{i}=\hat{A}(e_i)  \). Let \( A = \begin{bmatrix}
	A^{1} & A^{2} & \ldots  & A^{n} 
	\end{bmatrix} \). Then, by construction 
	\begin{align*}
		\hat{A}(x)&=\hat{A}(x_{1}e_{1} + x_{2}e_{2}) + \ldots  + x_ne_n ) \\ &= x_{1}\hat{A}(e_{1}) + x_{2}\hat{A}(e_{2}) + \ldots + x_n \hat{A}e_n)\\ &= x_{1}A^{1} + x_{2}A^{2} + \ldots  + x_nA^{n}\\  &= Ax
	.\end{align*}
\end{proof}

We can also calculate matrix multiplication as \( (AB)_{i,j} = \sum_k A_{i,k} \cdot B_{k,j}  \).

\begin{theorem}
	Suppose we take a third matrix \( C \). Then, \[
		A(BC) = (AB)C
	.\] This is the \textbf{associative property}.
\end{theorem}
\begin{proof}
	We saw that \[
		A(Bx) = (AB)x
	.\] Applying this, we have:
	\begin{align*}
		(AB)C &= \begin{bmatrix}
			(AB)C^{1} & \ldots & (AB)C^{n}  
		\end{bmatrix} \\
		&= \begin{bmatrix}
			A(BC^{1} ) & \ldots & A(BC^{n} )
		\end{bmatrix} \\
		&= A \begin{bmatrix}
			BC^{1} & \ldots & BC^{n}  
		\end{bmatrix}\\
		&= A(BC)
	.\end{align*}
\end{proof}

With this information, row reduction is just a series of matrix multiplications. Note that in row reduction, we can also have permutation matrices that switches the rows.

\begin{theorem}
	\( AB \neq  BA \).
\end{theorem}
\begin{proof}
	\[
		\begin{pmatrix}
			0 & 1 \\ 0 & 0
		\end{pmatrix} \begin{pmatrix}
			0 & 0 \\ 1 & 0
		\end{pmatrix} = \begin{pmatrix}
			1 & 0 \\ 0 & 0
		\end{pmatrix}
	.\] but not the other way around.
\end{proof}

To summarize matrix operations, we have 
\begin{align*}
	A + B &= B + A \\
	\alpha (A + B) &= \alpha A + \alpha B \\ \\
	(AB)C &=  A(BC) \\
	(A+B)C &= AC+BC \\
	C(A+B) &= CA + CB 
.\end{align*} By these properties, space of matrices is a vector space, and an algebra. However, we are missing division (the inverse)!

Note that a mapping \( A : \mathbb{R}^{n} \to \mathbb{R}^{m}   \) such that \( m<n \) cannot be invertible, as there are many solutions to \( Ax=b\) and therefore cannot be a bijection. The same can be said when \( n>m \), because \( Ax=b \) will have no solutions. Therefore, \( A \) is an invertible if \( n=m \).

\begin{definition}
	The \textbf{inverse} \( A^{-1}  \) of \( A \) is defined such that \[
		A^{-1} A x = x \quad \forall x
	.\] as well as \( A A^{-1}=I   \) and \( A^{-1}  \) must be unique.
\end{definition}

\begin{theorem}
	\[
		(AB)^{-1}  = B^{-1} A^{-1} 
	.\] 
\end{theorem}
\begin{proof}
	\begin{align*}
		(B^{-1}A^{-1}  )(AB) &= B^{-1}(A^{-1}A )B  \\
		&= B^{-1} I B  \\
		&= B^{-1} B  \\
		&= I
	.\end{align*}
	This is the only inverse.
\end{proof}

\begin{eg}
	The inverse of \[
		E_{12} = \begin{pmatrix}
			1 & 0 & 0 \\ -2 & 1 & 0 \\ 0 & 0 & 1
		\end{pmatrix}
	.\] is just \[
		E_{12}^{-1}  = \begin{pmatrix}
			1 & 0 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 1
		\end{pmatrix}
	.\] (you add back the two first rows you subtracted from the second).
\end{eg}

From the elimination example earlier, we have \[
	E_{23}E_{13}E_{12}A = \begin{bmatrix}
		2 & 4 & -2 \\
		0 & 1 & 7 \\
		0 & 0 & -2  
	\end{bmatrix} = U
.\] which is now upper triangular. Flipping this around, \( A = \underbrace{E_{12}^{-1}E_{13}^{-1}E_{23}^{-1}}_{L}U \). Note that all \( E_{i,j} \) are lower triangular, such that \( L \) is also lower triangular. This is \textbf{LU Factorization}.

We can use this to solve \( Ax=b \) by first writing \( A=LU\implies Ux = L^{-1}B  \), from which you do backwards substitution to solve the problem, reducing the number of operations from a magnitude of \( n^{3}  \) to \( n^{2}  \). However, getting \( A^{-1}  \) is still \( n^{3}  \), so it should only be precomputed if we solve equations \( Ax=b \) \( n \) times.
