\lecture{1}{Tue 09 Jan 2024 17:01}{Review}

\section{Vectors and Matrices}

For the time being, everything indicated in this course is in \( \mathbb{R} \).

\begin{definition}
	A \textbf{vector} will be defined as a column vector, e.g. \[
		u=\begin{bmatrix}
			x_{1} \\
			x_{2} \\
			x_{3} \\
		\end{bmatrix}
		\in \mathbb{R}^3
	.\] 
\end{definition}

\begin{notation}
	Sometimes, they will be written as a column vector lying down, e.g. \( (x_{1},x_{2},x_{3}) \in \mathbb{R}^3 \)
\end{notation}

\begin{definition}
	Let \( a \) be a scalar. Then multiplication between vector and scalar is defined as \[
		au = \begin{bmatrix}
			a\cdot x_{1}\\
			a\cdot x_{2}\\
			a\cdot x_{3}\\
		\end{bmatrix}
	.\] 
\end{definition}

\begin{definition}
	Let \(u = \begin{bmatrix}
			x_{1}\\
			x_{2}\\
			x_{3}\\
		\end{bmatrix} \) and \( v = \begin{bmatrix}
			y_{1}\\
			y_{2}\\
			y_{3}\\
		\end{bmatrix}
	\). Then addition between vectors is defined as \[
		u+v = \begin{bmatrix}
			x_{1}+y_{1}\\
			x_{2}+y_{2}\\
			x_{3}+y_{3}\\
		\end{bmatrix}
	.\] 
\end{definition}

\begin{definition}
	If \( u,v \) are vectors and \( a,b \) are scalars, then any \( au+bv \) is a \textbf{linear combination} of \( u \) and \( v \).
\end{definition}

\begin{remark}
	A \textbf{vector space} \( V \) is a set of objects \( u,v \) such that \( au+bv \in V\).
\end{remark}

\begin{eg}
	Polynomials of degree \( \le 2 \) in one variable can form a vector space.
\end{eg}
\begin{explanation}
	Let \( p(x) = a_{0}+a_{1}x+a_{2}x^2 \), and \( q(x) = b_{0}+b_{1}x+b_{2}x^2  \). Multiplying by scalars and adding are defined. Note that \( p(x) \to \begin{bmatrix}
		a_{0}\\
		a_{1}\\
		a_{2}\\
	\end{bmatrix} \).
\end{explanation}

\begin{eg}
	Let \( f(x) : [0,1] \to \mathbb{R} \) be a continuous function. We can multiply such functions by scalars and add together such functions, so they form a vector space as well.
\end{eg}

Suppose we have two vectors \( u, v \in \mathbb{R}^3\). Looking at the set of all linear combinations of \( u,v \),
\begin{itemize}
	\item if both \( u \) and \( v \) are the zero vectoor, then \( W=\{0\}   \).
	\item if \( u = \lambda v \), \( v\neq 0 \), then \( W \) is the line of all multiples of \( v \).
	\item if \( u \) and \( v \) are \textbf{linearly independent}, then \( W \) is a plane in \( \mathbb{R}^3 \).
\end{itemize}

\begin{definition}
	Vectors \( u_{1},u_{2},u_{3} \) are \textbf{linearly independent} if and only if \[
		a_{1}u_{1} + a_{2}u_{2} + a_{3}u_{3} = 0 \implies a_{1}=a_{2}=a_{3}=0
	.\] 
\end{definition}

\begin{definition}
  Let \( V,W \) be a vector spaces such that \( W \subseteq V \). Then, \( W \) is called a \textbf{subspace} of \( V \).
\end{definition}

\begin{eg}
	Let \( W=\{\begin{bmatrix}
		x_{1}\\
		x_{2}\\
		0
	\end{bmatrix} : x_{1},x_{2} \in \mathbb{R}\}   \). Then, \( W \) is a subspace of \( \mathbb{R}^3 \).
\end{eg}

\begin{theorem}
	If \( u,v \in V \), then the set of linear combinations of \( u \) and \( v \) is a subspace.
\end{theorem}
\begin{proof}
	Let \( W= \Span\{u, v\} \). We must show that \( w_{1},w_{2} \in W \implies c_{1}w_{1}+c_{2}w_{2} \in W \). By assumption, \( w_{1}=a_{1}u+b_{1}v \), and \( w_{2}=a_{2}u+b_{2}v \), such that \( w=(c_{1}a_{1}+c_{2}a_{2})u + (c_{1}b_{1}+c_{2}b_{2})v \). Therefore, \( w \) is a linear combination of \( u,v \).
\end{proof}

\begin{eg}
	Let \( u=\begin{bmatrix}
		1\\2\\3
	\end{bmatrix} \), and \( v=\begin{bmatrix}
		0\\2\\0
	\end{bmatrix} \). Then, \( \Span\{u,v\}   \) is a proper subspace of \( \mathbb{R}^3 \).
\end{eg}

\begin{definition}
\( u\cdot v = x_{1}y_{1}+x_{2}y_{2}+x_{3}y_{3} \) is the dot product of the vectors \( u=\begin{bmatrix}
	x_{1}\\x_{2}\\x_{3}\\
\end{bmatrix} \) and \( v=\begin{bmatrix}
	y_{1}\\y_{2}\\y_{3}
\end{bmatrix} \)
\end{definition}

\begin{definition}
	We say that \( u \perp v \) if \( u\cdot v=0 \).
\end{definition}

\begin{definition}
	The length or \textbf{norm} of a vector \( u \) is \( \sqrt{u\cdot u} = \Vert u \Vert \)
\end{definition}

\begin{theorem}
	The \textbf{Cauchyâ€“Schwarz inequality} states that \( |u \cdot  v| \le \Vert u \Vert \Vert v\Vert  \).
\end{theorem}
\begin{proof}
	\begin{align*}
		(u + \lambda v) \cdot (u + \lambda v) &\ge 0 \\
		u \cdot u + \lambda^2 v \cdot v + 2\lambda u \cdot v &\ge 0
	.\end{align*}
	The minimum lambda is \( \frac{-b}{2a} = \frac{-u \cdot v}{v\cdot v}\), which results in this inequality being true. Therefore, all greater values for lambda will result in this inequality being true.
\end{proof}

\begin{theorem}
	The \textbf{triangle inequality theorem} states that \( \Vert u + v \Vert \le  \Vert u \Vert + \Vert v \Vert \).
\end{theorem}

\begin{definition}
	The \textbf{unit vector} of a vector \( u \), \( \hat{u} \) is given by \( \frac{u}{\Vert u \Vert} \).
\end{definition}

\begin{theorem}
	If \( u \) and \( v \) are vectors such that \( \Vert u \Vert = \Vert v \Vert = 1 \), then \( u\cdot v = \cos(\theta) \) where \( \theta \) is the angle between \( u \) and \( v \).
\end{theorem}

\begin{corollary}
	If \( u \) and \( v \) are vectors, then \( u\cdot v = \Vert u \Vert \Vert v \Vert \cos (\theta)\). Note that \( u\cdot v =0\) when \( \theta =\frac{\pi}{2} \) or \( \frac{3\pi}{2}  \).
\end{corollary}

