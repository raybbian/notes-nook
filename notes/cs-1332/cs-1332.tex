% updated 2023-11-15 13:12:28
\documentclass[a4paper]{article}
\input{../preamble.tex}
\title{cs-1332}
\begin{document}
    \maketitle
    \tableofcontents\lecture{1}{Wed 04 Oct 2023 13:01}{Arrays, ArrayList}

\lecture{2}{Wed 04 Oct 2023 13:01}{LinkedList}

\lecture{3}{Wed 04 Oct 2023 13:01}{DLL, CSLL}

\lecture{4}{Wed 04 Oct 2023 13:02}{Recursion, Stacks}

\lecture{5}{Wed 04 Oct 2023 13:02}{Queues}

\lecture{6}{Wed 04 Oct 2023 13:02}{Deques}

\lecture{7}{Wed 04 Oct 2023 13:02}{Intro to Trees}

\lecture{8}{Wed 04 Oct 2023 13:02}{Tree Traversals}

\lecture{9}{Wed 04 Oct 2023 13:03}{Level Order, BST Operations}

\lecture{10}{Wed 04 Oct 2023 13:03}{BST Remove}

\lecture{11}{Wed 04 Oct 2023 13:03}{Heaps}

\lecture{12}{Wed 04 Oct 2023 13:03}{Build Heap}

\lecture{13}{Wed 04 Oct 2023 13:03}{Hashmaps; External Chaining}

\lecture{14}{Wed 04 Oct 2023 13:03}{Linear Probing}

\lecture{15}{Wed 04 Oct 2023 13:04}{Quadratic Probing}

\lecture{16}{Wed 04 Oct 2023 13:04}{Skip Lists}

\lecture{17}{Wed 04 Oct 2023 15:24}{AVL}
\section{AVL Tree}

What if we want guaranteed \( O(\log(n)) \) operations on everything?
\begin{definition}
	An AVL is a BST that is \textit{always} balanced. It stores a balance factor that is the difference between the left node's and right node's height.
\end{definition}

If the balance factor has magnitude greater than or equals to 2, then it is learning too far left (positive) or too far right (negative). We can maintain the tree's balance with rotations.

\begin{note}
	Getting the height of a BST is \( O(n) \). However, we can store the height of a node in the node itself. This makes getting the height of a node \( O(1) \).
\end{note}

\subsection{Operations}
\subsubsection{Update}
When we add or remove data, the heights and balance factors of each ancestor will change. Therefore, we must update these values.

\begin{algorithm}[H]
	\KwIn{curr, the node to update}
	curr.height = max(kids.height) + 1\;
	curr.bf = curr.left.height - curr.right.height; \tcp{Children must be updated first!}
	\caption{Update} 
\end{algorithm}
\subsubsection{Add}
How do we add data?
\begin{enumerate}
	\item We add the data to the leaf position, as you would in a BST.
	\item If the tree is no longer balanced, we then rotate the tree to balance it.
\end{enumerate}
Note that when a node is added, at most \( \log (n) \) nodes (its ancestors) have a new balance factor.


\begin{algorithm}[H]
	\uIf{curr = null} {
		add new node\;
	}
	\uElseIf{curr.data < data}{
		recurse left\;
	}
	\uElseIf{curr.data > data}{
		recurse right\;
	}
	\ElseIf{curr.data = data}{
		\tcp{Duplicate, do nothing}
	}
	\ForEach{node : curr \( \to \) root}{
		update(node)\;
		\If{node.bf is bad}{
			rotate
		}
	}
	\caption{Add}
\end{algorithm}

\begin{note}
You should update after recursion so the height values for the children are correct.
\end{note}

\subsection{Rotations}
There are four types of rotations: left, right, left-right, right-left.
\subsubsection{Left Rotation}
This type of rotation is used when:
\begin{itemize}
	\item The node is leaning right (balance factor is -2)
	\item The right child is also leaning right (balance factor is -1)
\end{itemize}

\begin{algorithm}[H]
	\caption{Left Rotation}
	\KwIn{A, the root of the tree}
	\KwOut{the new root of the tree}
	B = A.right\;
	A.right = B.left\;
	B.left = A\;
	update(A)\;
	update(B)\;
	\Return{B}
\end{algorithm}

\begin{note}
This is an O(1) operation.
\end{note}

\subsubsection{Right Rotation}
This type of rotation is used when:
\begin{itemize}
	\item The node is leaning left (balance factor is 2)
		\item The left child is also leaning left (balance factor is 1)
\end{itemize}

\begin{algorithm}[H]
	\caption{Right Rotation}
	\KwIn{A, the root of the tree}
	\KwOut{the new root of the tree}
	B = A.left\;
	A.left = B.right\;
	B.right = A\;
	update(A)\;
	update(B)\;
	\Return{B}
\end{algorithm}

\lecture{18}{Fri 06 Oct 2023 15:22}{AVL Continued}

\subsubsection{Right Left Rotation}
This type of rotation is used when:
\begin{itemize}
	\item The node is leaning right (balance factor is -2)
	\item The right child is leaning left (balance factor is 1)
\end{itemize}
We rotate the right child to the right. Then, we rotate the root to the left. Even though this is a combination of two rotations, it is still considered a single operation.

\subsubsection{Left Right Rotation}
This type of rotation is used when:
\begin{itemize}
	\item The node is leaning left (balance factor is 2)
	\item The left child is leaning right (balance factor is -1)
\end{itemize}
This is just the mirror of the previous operation: we rotate the left child to the left, and the root to the right.

\subsection{Runtime}
How long do operations on an AVL tree take?
\begin{itemize}
	\item When you add data into an AVL tree, you do at most one rotation, so the runtime is \( O(\log (n)) \).
	\item When you remove from an AVL tree, you do at most \( \log (n) \) rotations. Even still, the runtime is still \( O(\log (n)) \).
	\item Remember that each rotation is \( O(1) \).
\end{itemize}

\begin{note}
	An AVL can be at most \( 1.44\log (n) =\log (n)\) tall. This is derived from \( \frac{1}{\log_2(\phi )} \).
\end{note}

\exercise{1}
What does the following AVL tree look like after these operations?
\begin{itemize}
	\item add(56, 75, 61, 88, 93, 77)
	\item remove(61)
\end{itemize}

\lecture{19}{Wed 11 Oct 2023 15:31}{2-4 Trees}

\section{2-4 Trees}

Another \( O(\log n) \) data structure.
\begin{definition}
	A \textbf{perfect} binary search tree is a full binary search tree where all leaves are at the same depth.
\end{definition}

We maintain the perfect shape for any operation on a 2-4 tree. However, this means that they can only have 1, 3, 7, 15, 31 nodes, etc. Our solution is to add multiple data items to each node.

\begin{property}
	Every node has 1-3 data items, and 2-4 children. Every internal node always has one more child than the number of data items.
\end{property}

\begin{note}
	Note that 2-4 trees are not binary trees.
\end{note}

\begin{property}
	Let \( d_{1}, d_{2}, d_{3} \) be the data in the node, and \( t_{1}, t_{2}, t_{3}, t_{4} \) be the data of the children.
	\begin{itemize}
		\item The data in each node must be sorted (\( d_{1} < d_{2} < d_{3} \)).
		\item The data of each children must be sorted between the node's data: \( t_{1} < d_{1} \), \( d_{1} < t_{2} < d_{2} \), \( d_{2} < t_{3} < d_{3} \), \( d_{3} < t_{4} \).
	\end{itemize}
\end{property}

\subsection{Operations}

\subsubsection{Contains}
Very similar to a binary search tree.

\begin{algorithm}[H]
	\caption{Contains}
	\uIf{data < \( d_{1} \)}{
		explore \( t_{1} \)\;
	}\uElseIf{data < \( d_{2} \)}{
		explore \( t_{2} \)\;
	}\uElseIf{data < \( d_{3} \)}{
		explore \( t_3 \)\;
	}\Else{
		explore \( t_{4} \)\;
	}
\end{algorithm}

\subsubsection{Add}
A little bit harder. We must maintain the property that all leaves have the same height. We use the same logic as the contains algorithm, and add it to the node. \par
If there are already three data items in the leaf (overflow), we solve it with a process called promotion. We push one data item into the parent (either \( d_{2},d_{3} \)), and split the rest of the data into two leaves around the promoted data. \par
If the parent is now too full, repeat the process again. If the root is too full, then the promoted data becomes the new root, and the root splits into two children.

\subsubsection{Remove}
There are several cases to consider:
\begin{description}
	\item[Case 1:] Removing from a leaf with 2 or 3 data items. This case is very easy, as we just remove the data from the leaf.
	\item[Case 2:] Removing from an internal node. This case is simple as well, as we maintain the structure of the tree, replacing the data with its predecessor or successor. Because the predecessor or successor is always in a leaf, we can use Case 1, 3 or 4 to remove the predecessor or successor.
\end{description}

\lecture{20}{Fri 13 Oct 2023 15:32}{2-4 Trees Continued}

Remove operation continued:

\begin{description}
	\item[Case 3:] We want to remove from a leaf with only one data item, but the siblings have spare data. This operation is called a \textbf{transfer} or \textbf{rotation}. We take the spare data directly to the left or right, promote it into the parent, and pull the parent into the removed leaf. Note that if you transfer an internal node, you must move the subtrees as well!
	\item[Case 4:] We want to remove from a leaf with only one data item, and there is no spare data in the left or right siblings. Then, we pull down from the parent (left or right) and \textbf{fuse} two nodes together. \par
		However, you might empty the parent, which requires you pull from the grandparent. However, the grandparent might be empty after that operation as well! Therefore, we stop when the parent has 2 or 3 data items, we use a transfer, or the root becomes empty.
\end{description}

Still confused: here is a handly flowchart:
\begin{figure}[ht]
    \centering
    \incfig{2-4-tree-remove-flowchart}
    \caption{2-4 Tree Remove Flowchart}
    \label{fig:2-4-tree-remove-flowchart}
\end{figure}

\subsection{Runtime}
The runtime of a 2-4 tree is as follows:
\begin{itemize}
	\item Adding a node may require up to \( \log n \) promotions, which is \( O(\log n) \).
	\item Removing a node may require up to \( \log n \) fusions, which is \( O(\log n) \).
\end{itemize}

\begin{note}
	Note that even though an AVL and a 2-4 tree have the same runtime complexity, the depth of a 2-4 tree is less than or equal to that of an AVL tree. Therefore, it is used when going down the tree is costly - for example, when the data is stored on disk. For example, databases use a generation called a B-tree (where there are many items per node).
\end{note}


\lecture{21}{Mon 16 Oct 2023 15:31}{Sorting}

\section{Sorting}
What is an algorithm?

\begin{itemize}
	\item A sequential list of instructions to accomplish some goal.
\end{itemize}

For each sorting algorithm, we will look at:
\begin{itemize}
	\item Time complexity (best, average, worst)
	\item Stability: equal valued items maintain their relative order. This is important because it allows us to sort by multiple criteria.
	\item Adaptivity: an algorithm is faster if data is (partially) sorted.
	\item In place: uses O(1) extra memory to sort the data.
\end{itemize}

There are also two types of sorting algorithms:
\begin{itemize}
	\item Iterative: uses loops to iterate over the data. It is easier to implement and is usually in-place, but only sorts one item at a time.
	\item Divide and Conquer: (usually) uses recursion that splits the data into smaller pieces, sorts them, and then merges them. It is usually faster, but requires more memory.
	\item Non-Comparitive: uses special properties of the data to sort it. It is usually faster, but is only applicable to certain data.
\end{itemize}

\subsection{Bubble Sort}

For bubble sort, we iterate through the array from beginning to end, comparing pairs of adjacent items, which are swapped if they are out of order. We repeat this process, stopping at an earlier ending value until no swaps are made.

\begin{algorithm}
	\caption{Bubble Sort}
	\KwIn{A, the data to be sorted}
	\For{i from \( 0 \to n-1 \)}{
		\For{j from \( 0 \to n-1-i \)}{
			\If{A[j] > A[j+1]}{
				swap(A[j], A[j+1])\;
			}
		}
	}
\end{algorithm}

\subsubsection{Optimizations}
\begin{enumerate}
	\item If we make no changes, we know that the array is sorted. This means that we can stop the algorithm early.
	\item Track the last index where a swap was made, and only iterate up to that index. We can do this because we know that all elements after that index are sorted.
\end{enumerate}

\begin{algorithm}
	\caption{Optimized Bubble Sort}
	\KwIn{A, the data to be sorted}
	end = n - 2\;
	\While{end > -1}{
		lastSwap = -1\;
		\For{i : \( 0 \to end \)}{
			\If{A[i] > A[i+1]}{
				swap(A[i], A[i+1])\;
				lastSwap = i\;
			}
		}
		end = lastSwap - 1\;
	}
\end{algorithm}

\subsubsection{Runtime}
The best case for bubble sort occurs when the data is already in sorted order. We still have to check that the data is sorted, so the best case is \( O(n) \).

The worst case for bubble sort occurs when the data is in reverse sorted order. We then do \( n + n - 1 + n - 2 + \ldots + 1 = \frac{n}{2}(n+1) = \frac{n^2 + n}{2}\) operations, which is \( O(n^2) \).

The average case is, unfortunately, very close to the worst case. Therefore, it is \( O(n^2) \). Why is this?

If we randomly order the data, then on average half of the pairs are out of order. Every swap in our algorithm fixes one pair. There are on average \( \frac{n(n-1)}{4} \) pairs that are out of order, therefore our runtime is \( O(n^2) \) on average.

\subsubsection{Conclusion}
Bubble sort is stable, adaptive, and in-place.

\subsection{Insertion Sort}
We iterate over the elements, swapping the current element backwards until they are at the proper position in the first part of the array.

\subsubsection{Runtime}
The best case for insertion sort occurs when the data is already in sorted order. We iterate over all data, and we do one comparison for each item, which results in a runtime of \( O(n) \).

The worst case for insertion sort is when the data is in reverse sorted order. Similar to bubble sort, the worst case is \( O(n^2) \).

The average case, also like bubble sort, is \( O(n^2) \).

\subsubsection{Conclusion}
Insertion sort is stable, adaptive, and in-place.

\subsection{Selection Sort}
We find the largest item in our data, and swap it to the end of the array. We then repeat this process, swapping the next largest item to the second to last position, and so on until the data is sorted.

\subsubsection{Runtime}
All cases for selection sort is \( O(n^2) \). We don't do anything depending on the data, so our runtime is quadratic even if our data is sorted!

\lecture{22}{Fri 20 Oct 2023 15:33}{Sorting Continued}

Continuing on with selection sort:

\begin{algorithm}[H]
	\caption{Selection Sort}
	\KwIn{A, the array to sort}
	\For{$i = A.length -1$ \KwTo \( 2 \)}{
		Swap max(A[1..i]) with A[i]\;
	}
\end{algorithm}

\subsubsection{Conclusion}

Selection sort is in-place. However, it is not adaptive and not stable. Even still, selection sort uses the fewest swaps, which makes it applicable for applications where writing to memory is costly.

\subsection{Cocktail Shaker Sort}
Bubble sort is (kind of) adaptive. The Cocktail Shaker sort builds upon this, becoming adaptive for arrays like [2, 3, 4, 5, 6, 7, 8, 1].

In one iteration of the Cocktail Shaker sort, we run bubble sort from left-to-right, then once again from right-to-left. Note that as a result, both sides of the array will be sorted.

\subsubsection{Optimizations}
We can track the last swap in both directions. Then, we can stop the algorithm once we reach the last swap in both directions.

\subsubsection{Runtime}
The best case for this sort (sorted data) is \( O(n) \).

The average case for this sort is \( O(n^2) \).

The worst case for this sort (reverse sorted data) is \( O(n^2) \).

\subsubsection{Conclusion}
Just like Bubble sort, it is in-place, stable, and adaptive.

\begin{note}
	Why do we study bad sorting algorithms? They are stable, adaptive, and in-place. Fast sorts often sacrifice one of these properties, and are often harder to implement. Complex sorts use these sorts as building blocks, and they aren't even the worst (Bogosort, Stalinsort, etc)!
\end{note}

\begin{note}
	Theoretical computer scientists have found that the best possible worst case sorting algorithm sorts in \( O(n\log (n)) \).
\end{note}

\subsection{Heap Sort}
We can achieve an \( O(n\log (n)) \) sort by adding everything to a heap in \( O(n) \), and removing data one at a time in \(  O(log(n)) \).

\subsubsection{Conclusion}
Heap sort is not in-place: while you can define and apply the heap logic on any backing array, more often than not you will be inserting your data into an external heap, which is \( O(n) \) extra memory.

Because heap sort is \( O(n\log (n)) \) for every case, it is not adaptive.

\begin{note}
	What other data structures can we sort with? Are they good? Well, we can use an AVL/BST, add all the data in \( O(n\log (n)) \)/\( O(n^2) \), and get the inorder traversal in \( O(n) \). A similar implementation can be created with skiplists.
\end{note}

\lecture{23}{Mon 23 Oct 2023 15:33}{Merge Sort}

\subsection{Merge Sort}

Merge sort is an example of a divide-and-conquer sort. It breaks a large problem into smaller problems, solves them, and combines the solutions into one larger solution.

It takes an array of data, splits them into two, sorts them (using merge sort yet again), and merges the two sorted lists back together.

\begin{algorithm}
	\caption{Merge Sort}
	\If{array.length == 1}{
		return\;
	}
	left = arr[0:arr.length/2]\;
	right = arr[arr.length/2:arr.length]\;
	mergeSort(left)\;
	merseSort(right)\;
	merge(left, right, arr)\;
\end{algorithm}

How do we merge the two sorted sublists? We track \( i \) and \( j \), which both start at 0. We compare the two elements at left[\( i \)] and right[\( j \)], and add the smaller one to the array. We then increment the index of the smaller element. 

\begin{note}
	If there  are two equal values, we take the element from the left sublist to maintain stability.
\end{note}

\subsubsection{Runtime}
The operation list of merge sort is \( 2\log (n) \) tall, and there are \( n \) operations per level. Therefore, the runtime is (always) \( O(n\log (n)) \).

\begin{note}
	Note that one such "worst case" (most comparisons) of merge sort for an array of size 8 is \([5, 1, 7, 3, 2, 8, 6, 4]\). This is because we have to compare every element in the first half with every element in the second half, and so on for each merge. Even still, this is \( O(n\log (n)) \).
\end{note}

\subsubsection{Conclusion}
Merge sort is stable, but not adaptive (same runtime for all case). We also need to create a second array to split and merge the arrays, so it is not in-place.

\subsection{Quicksort}
Quicksort is also a divide-and-conquer algorithm. However, unlike merge sort, it is in place. It also can be done non-recursively, which is more efficient.

The overall plan of Quicksort is as follows:
\begin{enumerate}
	\item Pick one data as the pivot.
	\item Partition the data into two sublists: one with elements less than the pivot, and one with elements greater than the pivot.
	\item We repeat this plan in each of the sublists, until the data is sorted.
\end{enumerate}


\lecture{24}{Wed 25 Oct 2023 15:30}{QuickSort}

How do we partition the list? After picking the pivot,
\begin{enumerate}
	\item We swap the pivot with arr[0],
	\item Then we maintain two pointers \( i \) and \( j \) and walk them towards the center until both see two elements that are in the wrong spot.
	\item We swap arr[i] and arr[j]
	\item We then stop once the two pointers cross each other, and swap the pivot with \( j \).
\end{enumerate}

\subsubsection{Runtime}
The average runtime is \( O(n \log n) \), and the worst case is \( O(n^2) \). Note that the best pivot is the median. Below are some sample pivots:

\begin{itemize}
	\item arr[0] as the pivot runs in \( O(n^2) \) if the data is already sorted.
	\item Median as the pivot runs in \( O(n^2) \) when we have a malicious user.
	\item Random pivot runs in \( O(n \log n) \) on average. But you could pick the worst pivot every time, which would have worst case \( O(n^2) \).
\end{itemize}

\begin{remark}
	In this class, QuickSort refers to QuickSort with a random pivot.
\end{remark}

\begin{note}
	There is an algorithm called Median-of-medians (grad algo), which gives you a guaranteed ``good enough'' pivot and runs fast enough such that the worst case is \( O(n\log (n)) \). However, due to constant time factors, it is not used in practice.
\end{note}

\subsubsection{Conclusion}
QuickSort is in-place, but not adaptive or stable. Note that QuickSort with recursion is technically not in-place due to the \( \log (n) \) to \( n \) memory complexity of the call stack.

\subsubsection{QuickSelect}
Given an array of \( n \) elements, find the \( k \)-th smallest element in the array. Note that if \( k=1 \) or \( k=n \), we can find the smallest or largest element in \( O(n) \) timewith linear search. Also, if the array was already sorted, we can find the \( k \)-th smallest element in \( O(1) \) time.

QuickSelect is important because it beats the obvious plan of first sorting the array and getting the \( k \)-th element, instead running in \( O(n) \) time on average. Instead of recursing to both sides as in QuickSort, we recurse only to the side with the \( k \)-th smallest element. 

\begin{enumerate}
	\item Same as steps of QuickSort
	\item If \( j=k+1 \), we return the pivot. If \( j > k-1 \), we find the \( k \)-th smallest element in the left subarray. If \( j < k+1 \), we find the \( k-j-1 \)-th smallest element in the right subarray.
\end{enumerate}

Note that because each time we pick an array, we do half as much work, such that we have a runtime of \[
	n + \frac{n}{2} + \frac{n}{4} + \ldots = 2n = O(n)
.\] on average. In the worst case, we have a runtime of \( O(n^2) \).

\lecture{25}{Fri 27 Oct 2023 15:31}{RadixSort}

\subsection{RadixSort}

RadixSort has an average case runtime of \( O(n) \)! But the best case we can do with sorting is \( n\log (n) \), so what's the catch?

\begin{note}
	The bound of \( n\log (n) \) only applies to comparison-based sorting algorithms.
\end{note}

Because RadixSort cannot do comparisons, it can only sort string like things, including integers. Suppose you have a ton of integers. The idea of RadixSort is to sort by ones digit, then tens digit, then hundreds digit, etc. This order guarantees stability of the sort.

However, we don't use the other sorts to sort by digits. Instead, we create buckets/queues and place items in their proper buckets.

\begin{algorithm}[H]
	\caption{RadixSort}
	\For{i = 0 to max number of digits}{
		\For{each item}{
			enqueue item into bucket corresponding to digit i
		}
		\For{each bucket}{
			dequeue all items into original list
		}
	}
\end{algorithm}

\begin{note}
	Note for negative digits, you can use 19 buckets to store all possible digits. 
\end{note}

How are we going to manipulate the digits of a number? We could convert this string to a char array, but that would be expensive. Instead, we can use the mod operator and integer division to get the digits.

\begin{property}
	For digit number \( i \), we can divide the number by \( 10^{i-1} \) and then mod by 10. This will give us the digit we want.
\end{property}

What base should we choose for RadixSort? Note that if we choose a super large base, such as 2000, then we need to use 2000 buckets! The optimal base is usually \( 2^8 \), but varies depending on use case.

\subsubsection{Runtime}

We have to make \( n \) operations for each digit, and we have to do this for each digit. So the runtime is \( O(nk) \), where \( k \) is the number of digits in the largest number. 

\subsubsection{Conclusion}

RadixSort is stable, but it is not adaptive and in-place.

\lecture{26}{Mon 30 Oct 2023 15:31}{Pattern Matching}

\section{Pattern Matching}

What is our motivation for this problem? Imagine we want to find a pattern in a text. What is the most efficient way of doing so?

\begin{eg}
	Google searches the entire internet for a pattern: your name. We can also search very long DNA for specific codons (the pattern).
\end{eg}

\begin{notation}
	Let the length of the text be \( n \). Let the length of the pattern be \( m \).
\end{notation}

\begin{definition}
	Let the \textbf{alphabet} be the set of valid characters for the text.
\end{definition}

There are two types of pattern matching, which differ on what to do when we find the pattern. If we stop, that means we are searching for a single occurence. If we continue, that means we are searching for all occurences.

\subsection{Brute Force}

Brute force is not just for pattern matching. Instead, we try every possible solution. For pattern matching specifically, we have \( O(n) \) (\( O(n-m) \)) choices for the start of the pattern. We check each starting spot for the pattern.


\begin{algorithm}[H]
	\caption{Brute Force Pattern Matching}
	i = 0\;
	\While{i < len(text) - len(pattern)}{
		j = 0\;
		\While{j < len(pattern) and text[i+j] = pattern[j]}{
			j++\;
		}
		\If{j = len(pattern)}{
			return True\;
		}{
			j = 0\;
			i++\;
		}
	}
\end{algorithm}

\subsubsection{Runtime}
The best case for our brute force pattern matching is \( O(n) \). The worst case is \( O(nm) \), because we need to check every \( n-m \) starting positions at most \( m-1 \) times. For English, the average case is \( O(n) \). 

Note that we don't usually care about the average case because english text is not random. Even though brute force is fine, is there a better way to search for patterns?

\begin{note}
	Java String.contains() uses brute force.
\end{note}

\subsection{Boyer-Moore}

The idea for this algorithm is to use the wrong character to shift the pattern by a certain amount with a lookup table. Our lookup table stores the last appearance of each character in the pattern. If a character has not appeared, this index is -1.

\begin{algorithm}[H]
	\caption{Boyer-Moore Pattern Matching}
	\KwIn{text, pattern}
	\KwOut{True if pattern in text, False otherwise}
	\For{i < len(pattern)}{
			lookup[pattern[i]] = i\;
	}
	i = 0\;
	\While{i < len(text) - len(pattern)}{
		j = len(pattern) - 1\;
		\While{j >= 0 and text[i+j] = pattern[j]}{
			j -= 1\;
		}
		\If{j = -1}{
			return True\;
		}{
			i += max(1, j - lookup[text[i+j]])\;
		}
	}
	return False\;
\end{algorithm}

\lecture{27}{Wed 01 Nov 2023 15:32}{Pattern Matching Continued}

\subsubsection{Runtime}

The worst case runtime is \( O(mn) \). The best case for finding a single occurence is \( O(m) \). The best case for failing to find an occurence or finding all occurences is \( O(\frac{n}{m}) \). However, be careful that building the lookup table takes \( O(m) \) time, so technically our best case runtime for these two cases is \( O(\frac{n}{m} +m) \).

\begin{note}
	We should use Boyer-Moore whenever the text has characters not in the pattern. This is more likely as the alphabet grows, so it is better for larger alphabets. 
\end{note}

Traditional Boyer-Moore includes a good suffix rule, which improves big O runtime, but doesn't really speed up runtime in real life scenarios. It is also quite similar to KMP, but we will not cover it.

\subsection{Knuth-Morris-Pratt}

What is the longest word that you can think of which has no repeated letters? Demographic.

Note that when we try to find pattern	``demographic'' in a text, we can shift, no matter what, the d all the way to under the c. However, if there are repeated patterns, we can shift the word to align with another pattern to save work. That is the core idea of KMP.

\lecture{28}{Fri 03 Nov 2023 15:31}{KMP Continued}

Depending on which letter we failed on when checking the pattern, we can shift by a predetermined amount. Note that we keep track of \( j - \text{shift} - 1 \) (\( j \) is index of pattern) and insert this value into the failure table into \( j-1 \), as this is the number of characters in our pattern that are already matched for the next iteration of the algorithm.

How do we get the failure table? We need to know how much of the beginning matches the end, i.e. is there a matching prefix/suffix? More formally, the failure table at \( j \) is the length of the longest string which is both a prefix of the pattern and a proper suffix of \( p[0\ldots 1] \).

\begin{eg}
	Let the pattern be ``axabaxax''. The failure table is as follows:
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Prefix} & \textbf{Longest Prefix/Suffix} \\
			\hline
			a & 0 \\
			ax & 0 \\
			axa & 1 \\
			axab & 0 \\
			axaba & 1 \\
			axabax & 2 \\
			axabaxa & 3 \\
			axabaxax & 2 \\
			\hline
		\end{tabular}
	\end{center}
\end{eg}

\begin{eg}
	Let the pattern be ``eminem''. The failure table is as follows:
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Prefix} & \textbf{Longest Prefix/Suffix} \\
			\hline
			e & 0 \\
			em & 0 \\
			emi & 1 \\
			emin & 0 \\
			emine & 1 \\
			eminem & 2 \\
			\hline
		\end{tabular}
	\end{center}
\end{eg}

\begin{eg}
	Let the pattern be ``ababaab''. The failure table is as follows:
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Prefix} & \textbf{Longest Prefix/Suffix} \\
			\hline
			a & 0 \\
			ab & 0 \\
			aba & 1 \\
			abab & 2 \\
			ababa & 3 \\
			ababaa & 1 \\
			ababaab & 2 \\
			\hline
		\end{tabular}
	\end{center}
\end{eg}

\begin{eg}
	Let the pattern be ``salsas''. The failure table is as follows:
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Prefix} & \textbf{Longest Prefix/Suffix} \\
			\hline
			s & 0 \\
			sa & 0 \\
			sal & 0 \\
			sals & 1 \\
			salsa & 2 \\
			salsas & 1 \\
			\hline
		\end{tabular}
	\end{center}
\end{eg}

How do we run KMP?
\begin{enumerate}
	\item We iterate L to R through the pattern
	\item If we fail at an index \( j \) (in the pattern), suppose \( FT[j-1]=k \).
	\item If \( k > 0  \), we can save some matched characters, so we shift until \( k \) characters of the patterns still overlap with previous matched characters (i.e. shift by \( j - k + 1 \)). Note that we don't need to recompare these still overlapping characters!
	\item If \( k=0 \), then there is no useful repetition. Therefore, we should align the pattern with the mismatched character.
	\item If \( j=0 \), then we cannot access the failure table. Therefore, we should just shift by one.
	\item If we have a complete match, use the last entry in the failure table (i.e. \( FT[m-1] \) where \( m \) is the length of the pattern).
\end{enumerate}

\begin{algorithm}[H]
	\caption{Failure Table Generation}
	\KwIn{Our pattern}
	i = 0 \tcp*{prefix}
	j = 1 \tcp*{suffix, also current ft entry index}
	ft = int[len(pattern)]\;
	ft[0] = 0\;
	\While{j < len(pattern)}{
		\uIf{pattern[i] == pattern[j]}{
			ft[j] = i + 1\;
			i++; j++\;
		}
		\Else{
			\uIf{i == 0}{
				ft[j] = 0\;
				j++\;
			}
			\Else{
				i = ft[i-1] \tcp*{progress before, try smaller prefix suffix pair}
			}
		}
	}
\end{algorithm}


\lecture{29}{Mon 06 Nov 2023 15:44}{KMP Continued Continued}

Here is the pseudocode for KMP:

\begin{algorithm}[H]
	\caption{KMP}
	\KwIn{Pattern $P$ of length $m$, text $T$ of length $n$}
	\KwOut{All indices $i$ such that $T[i..i+m-1] = P$}
	ft = makeFT(\( P \))\;
	i = 0\;
	j = 0\;
	\While{\( i \le n - m \)}{
		\uIf{T[i] == P[j]}{
			\uIf{j == m - 1}{
				j = ft[j]\;
				i++\;
			}
			\Else{
				i++\;
				j++\;
			}
		}
		\Else{
			\uIf{j == 0}{
				i++\;
			}
			\Else{
				j = ft[j] - 1\;
			}
		}
	}
\end{algorithm}

\subsubsection{Runtime}

The worst case time is \( O(m+n) \). Note that the trivial best case is when you are trying to find one occurrence and it is at the start of the case, which is \( O(m) \).

\begin{note}
	Note that KMP can get stuck sometimes.
\end{note}

\begin{eg}
	Let the text be aaab..., and the pattern be aaaa. Then, we would check the pattern 4 times here, which is less efficient that Boyer-Moore. In other words, KMP does not recognize that b is not in the pattern.
\end{eg}

\begin{note}
	Boyer Moore has a better best case with \( O(\frac{n}{m} + m) \), but a worse worst case with \( O(mn) \). Of the two, KMP is preferred for smaller alphabets, and Boyer-Moore is preferred for large alphabets.
\end{note}

\lecture{30}{Wed 08 Nov 2023 15:33}{Robin Karp}

\subsection{Robin Karp}

Instead of comparing character by character, we can use a rolling hash function to compare substrings. For each alignment, compare \( hash(pattern) \) to \( hash(text[i\ldots i+m-1]) \). If the hashes are different, we know the two strings are different. However, if they are the same, then you have to check character by character. Can we optimize finding the hash of the substring \( O(1) \)?

\subsubsection{Rolling Hash}
The first hash we calculate is \( O(m) \), but updating the hash is \( O(1) \). Each hash is calculated from the previous hash in \( O(1) \) time. A potential hash is to add all the algorithms together. Then, we can subtract the first one and add the next one to roll the hash. However, this leads to many collisions.

The actual hash uses a base \( B \), which is prime. Then, we treat the string as a base \( B \) number. Then, abcd would be represented as \[
	a \times B^3 + b \times B^2 + c \times B + d
.\] Note that we can roll our hash by subtracting \( a \times B^3 \), multiplying what remains by \( B \), and then adding the new character \( e \). 

\subsubsection{Runtime}
The best case runtime is when we find one occurence at the beginning, which is \( O(m) \). For all occurences, the hash never matches, so we have a runtime of \( O(n+m) \). In the worst case, the hash always matches the text, so we have to compare character by character, which is \( O(nm) \). 

\subsection{Galil Rule}
The Galil rule is a combination of KMP and Boyer Moore. Essentially, when we have a full match in Boyer Moore, we can shift by the period (\( m-ft[m-1] \)) as we do in KMP. This optimizes the worst case in which the text consists of many patterns.

\lecture{31}{Fri 10 Nov 2023 15:42}{Graphs}

\section{Graphs}

For motivation, think about landlocked and doubly landelocked countries. How would we find the number of doubly landlocked countries? We could use a graph where countries are nodes, and edges are placed between two things that are touching.

\begin{definition}
	A \textbf{vertex} is a node in a graph. 
\end{definition}

\begin{definition}
	An \textbf{edge} connects two vertices in a graph.
\end{definition}

\begin{note}
	The way you draw the graph doesn't matter!
\end{note}

\begin{definition}
	Two vertices are \textbf{adjacent} if they share an edge.
\end{definition}

\begin{definition}
	A vertex is \textbf{incident} to an edge if it is an endpoint of that edge.
\end{definition}

\begin{definition}
	The \textbf{degree} of a vertex is the number of vertices it is adjacent to.
\end{definition}

\begin{definition}
	A vertex is \textbf{isolated} if its degree is 0.
\end{definition}

\begin{definition}
	A \textbf{simple} graph is a graph without self-loops and multiple edges.
\end{definition}

\begin{definition}
	The number of vertices in \( G \), denoted the \textbf{order} of \( G \), is \( |V| \).
\end{definition}

\begin{definition}
	The number of edges in \( G \), denoted the \textbf{size} of \( G \), is \( |E| \).
\end{definition}

\begin{definition}
	A \textbf{directed} graph is a graph where edges have a direction.
\end{definition}

\begin{eg}
	Currency exchange rates can be modeled with a directed graph.
\end{eg}

\begin{eg}
	Flights can be modeled with a directed graph.
\end{eg}

\begin{eg}
	Pathfinding between locations in a map.
\end{eg}

How do we traverse a graph?

\begin{definition}
	A \textbf{walk} is a traversal across a graph through a series of edges.
\end{definition}

\begin{definition}
	A \textbf{path} is a walk in which no vertex or edge is repeated.
\end{definition}

\begin{definition}
	A \textbf{trail} is a walk in which no edge is repeated.
\end{definition}

\begin{definition}
	A \textbf{cycle} is a path in which the first and last vertices are the same.
\end{definition}

\begin{definition}
	A \textbf{circuit} is a trail in which the first and last vertices are the same.
\end{definition}

\begin{definition}
	Two vertices are \textbf{connected} if there is a path from one to another.
\end{definition}

\begin{definition}
	A graph is \textbf{connected} if every pair of vertices is connected.
\end{definition}

\begin{definition}
	A \textbf{tree} is an acyclic connected graph.
\end{definition}

\begin{note}
	All trees have \( |V|-1 \) edges.
\end{note}

\begin{definition}
	A \textbf{clique} is a graph with the maximum number of edges.
\end{definition}

\begin{note}
	All cliques have \( \frac{n(n-1)}{2} \approx O(|V|^2)\) edges.
\end{note}

\begin{definition}
	A \textbf{sparse} graph has \( \approx |V| \) edges (tree).
\end{definition}

\begin{definition}
	A \textbf{dense} graph has \( \approx |V|^2 \) edges (clique).
\end{definition}

\lecture{32}{Mon 13 Nov 2023 15:32}{DFS}

\subsection{Graph Representations}

How do we store a graph in a computer? There are to main ideas: adjacency matrices and adjacency lists. The rows are the ``out'' vertices and the columns are the ``in'' vertices, and the matrix itself can store the label or weight or some other value of the edge exists.

\begin{note}
	An undirected graph will have a symmetric adjacency matrix.
\end{note}

The major appeal of using a matrix is having O(1) operations to remove, add, or set edges. However, there is a lot of space used for nothing - we will always use \( O(|V|^2) \) memory. Also, adding or removing new vertices to the graph is expensive, also time \( O(|V|^2) \) as we need to copy our data to a new array. Also, getting the list of adjacent vertices is time \( O(|V|) \).

An adjacency list is a map from vertices to a list of edges incident to the vertex. With this representation, we can add/remove a vertex in time \( O(1) \). The worst case for removing an edge is \( O(\deg(V)) \approx O(|V|) \). And, we can get edges incident to this vertex in time \( O(1) \), and the vertices at the end of the edges in time \( O(\deg(V)) \approx O(|V|) \)

\begin{note}
	An adjacency matrix should be used for dense graphs, and an adjacency list should be used for sparse graphs.
\end{note}

We can also use an edge list, which is a lot worse. This is because we need to look through every single edge in our list to find specific information, which is \( O(|E|) \).

Starting at a vertex in the graph, which vertices are reachable?

The naive solution is to wander through the grarph randomly, and note when a new vertex is seen. This is very slow because it repeats vertices. We can optimize this by marking nodes as we visit them. However, we can get trapped and miss some nodes.

The actual plan is, as we traverse through the graph, we will store the vertices seen but not traversed into a to-do list. When we have nothing to do, we will check the to-do list to jump to the next vertex. If we insert new items into the to-do list at the front, we have DFS. If we insert new items to the back of the to-do list, we have BFS.

\subsection{Depth-First Search}

\begin{algorithm}
	\caption{DFS}
	\KwIn {Graph \(G = (V, E)\), start vertex \(v\)}
	\For{\( w \) adj to \( v \)}{
		\If{\( w \) is not marked}{
			mark \( w \) as seen\;
			DFS\( (G, w) \)\;
		}
	}
\end{algorithm}

\begin{note}
	In this class, the tiebreaker for which node to visit first is alphabetical.
\end{note}

DFS can be used for finding connected components, cycles, spanning trees, as well as finding ``a'' path between vertices.

\subsubsection{Runtime}

DFS has a time complexity of \( O(|V|+|E|) \).

\end{document}