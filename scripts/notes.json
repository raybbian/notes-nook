{
    "cs-1332": {
        "lec_01.tex": "\\lecture{1}{Wed 04 Oct 2023 13:01}{Arrays, ArrayList}\n",
        "lec_02.tex": "\\lecture{2}{Wed 04 Oct 2023 13:01}{LinkedList}\n",
        "lec_03.tex": "\\lecture{3}{Wed 04 Oct 2023 13:01}{DLL, CSLL}\n",
        "lec_04.tex": "\\lecture{4}{Wed 04 Oct 2023 13:02}{Recursion, Stacks}\n",
        "lec_05.tex": "\\lecture{5}{Wed 04 Oct 2023 13:02}{Queues}\n",
        "lec_06.tex": "\\lecture{6}{Wed 04 Oct 2023 13:02}{Deques}\n",
        "lec_07.tex": "\\lecture{7}{Wed 04 Oct 2023 13:02}{Intro to Trees}\n",
        "lec_08.tex": "\\lecture{8}{Wed 04 Oct 2023 13:02}{Tree Traversals}\n",
        "lec_09.tex": "\\lecture{9}{Wed 04 Oct 2023 13:03}{Level Order, BST Operations}\n",
        "lec_10.tex": "\\lecture{10}{Wed 04 Oct 2023 13:03}{BST Remove}\n",
        "lec_11.tex": "\\lecture{11}{Wed 04 Oct 2023 13:03}{Heaps}\n",
        "lec_12.tex": "\\lecture{12}{Wed 04 Oct 2023 13:03}{Build Heap}\n",
        "lec_13.tex": "\\lecture{13}{Wed 04 Oct 2023 13:03}{Hashmaps; External Chaining}\n",
        "lec_14.tex": "\\lecture{14}{Wed 04 Oct 2023 13:03}{Linear Probing}\n",
        "lec_15.tex": "\\lecture{15}{Wed 04 Oct 2023 13:04}{Quadratic Probing}\n",
        "lec_16.tex": "\\lecture{16}{Wed 04 Oct 2023 13:04}{Skip Lists}\n",
        "lec_17.tex": "\\lecture{17}{Wed 04 Oct 2023 15:24}{AVL}\n\\section{AVL Tree}\n\nWhat if we want guaranteed \\( O(\\log(n)) \\) operations on everything?\n\\begin{definition}\n\tAn AVL is a BST that is \\textit{always} balanced. It stores a balance factor that is the difference between the left node's and right node's height.\n\\end{definition}\n\nIf the balance factor has magnitude greater than or equals to 2, then it is learning too far left (positive) or too far right (negative). We can maintain the tree's balance with rotations.\n\n\\begin{note}\n\tGetting the height of a BST is \\( O(n) \\). However, we can store the height of a node in the node itself. This makes getting the height of a node \\( O(1) \\).\n\\end{note}\n\n\\subsection{Operations}\n\\subsubsection{Update}\nWhen we add or remove data, the heights and balance factors of each ancestor will change. Therefore, we must update these values.\n\n\\begin{algorithm}[H]\n\t\\KwIn{curr, the node to update}\n\tcurr.height = max(kids.height) + 1\\;\n\tcurr.bf = curr.left.height - curr.right.height; \\tcp{Children must be updated first!}\n\t\\caption{Update} \n\\end{algorithm}\n\\subsubsection{Add}\nHow do we add data?\n\\begin{enumerate}\n\t\\item We add the data to the leaf position, as you would in a BST.\n\t\\item If the tree is no longer balanced, we then rotate the tree to balance it.\n\\end{enumerate}\nNote that when a node is added, at most \\( \\log (n) \\) nodes (its ancestors) have a new balance factor.\n\n\n\\begin{algorithm}[H]\n\t\\uIf{curr = null} {\n\t\tadd new node\\;\n\t}\n\t\\uElseIf{curr.data < data}{\n\t\trecurse left\\;\n\t}\n\t\\uElseIf{curr.data > data}{\n\t\trecurse right\\;\n\t}\n\t\\ElseIf{curr.data = data}{\n\t\t\\tcp{Duplicate, do nothing}\n\t}\n\t\\ForEach{node : curr \\( \\to \\) root}{\n\t\tupdate(node)\\;\n\t\t\\If{node.bf is bad}{\n\t\t\trotate\n\t\t}\n\t}\n\t\\caption{Add}\n\\end{algorithm}\n\n\\begin{note}\nYou should update after recursion so the height values for the children are correct.\n\\end{note}\n\n\\subsection{Rotations}\nThere are four types of rotations: left, right, left-right, right-left.\n\\subsubsection{Left Rotation}\nThis type of rotation is used when:\n\\begin{itemize}\n\t\\item The node is leaning right (balance factor is -2)\n\t\\item The right child is also leaning right (balance factor is -1)\n\\end{itemize}\n\n\\begin{algorithm}[H]\n\t\\caption{Left Rotation}\n\t\\KwIn{A, the root of the tree}\n\t\\KwOut{the new root of the tree}\n\tB = A.right\\;\n\tA.right = B.left\\;\n\tB.left = A\\;\n\tupdate(A)\\;\n\tupdate(B)\\;\n\t\\Return{B}\n\\end{algorithm}\n\n\\begin{note}\nThis is an O(1) operation.\n\\end{note}\n\n\\subsubsection{Right Rotation}\nThis type of rotation is used when:\n\\begin{itemize}\n\t\\item The node is leaning left (balance factor is 2)\n\t\t\\item The left child is also leaning left (balance factor is 1)\n\\end{itemize}\n\n\\begin{algorithm}[H]\n\t\\caption{Right Rotation}\n\t\\KwIn{A, the root of the tree}\n\t\\KwOut{the new root of the tree}\n\tB = A.left\\;\n\tA.left = B.right\\;\n\tB.right = A\\;\n\tupdate(A)\\;\n\tupdate(B)\\;\n\t\\Return{B}\n\\end{algorithm}\n",
        "lec_18.tex": "\\lecture{18}{Fri 06 Oct 2023 15:22}{AVL Continued}\n\n\\subsubsection{Right Left Rotation}\nThis type of rotation is used when:\n\\begin{itemize}\n\t\\item The node is leaning right (balance factor is -2)\n\t\\item The right child is leaning left (balance factor is 1)\n\\end{itemize}\nWe rotate the right child to the right. Then, we rotate the root to the left. Even though this is a combination of two rotations, it is still considered a single operation.\n\n\\subsubsection{Left Right Rotation}\nThis type of rotation is used when:\n\\begin{itemize}\n\t\\item The node is leaning left (balance factor is 2)\n\t\\item The left child is leaning right (balance factor is -1)\n\\end{itemize}\nThis is just the mirror of the previous operation: we rotate the left child to the left, and the root to the right.\n\n\\subsection{Runtime}\nHow long do operations on an AVL tree take?\n\\begin{itemize}\n\t\\item When you add data into an AVL tree, you do at most one rotation, so the runtime is \\( O(\\log (n)) \\).\n\t\\item When you remove from an AVL tree, you do at most \\( \\log (n) \\) rotations. Even still, the runtime is still \\( O(\\log (n)) \\).\n\t\\item Remember that each rotation is \\( O(1) \\).\n\\end{itemize}\n\n\\begin{note}\n\tAn AVL can be at most \\( 1.44\\log (n) =\\log (n)\\) tall. This is derived from \\( \\frac{1}{\\log_2(\\phi )} \\).\n\\end{note}\n\n\\exercise{1}\nWhat does the following AVL tree look like after these operations?\n\\begin{itemize}\n\t\\item add(56, 75, 61, 88, 93, 77)\n\t\\item remove(61)\n\\end{itemize}\n",
        "lec_19.tex": "\\lecture{19}{Wed 11 Oct 2023 15:31}{2-4 Trees}\n\n\\section{2-4 Trees}\n\nAnother \\( O(\\log n) \\) data structure.\n\\begin{definition}\n\tA \\textbf{perfect} binary search tree is a full binary search tree where all leaves are at the same depth.\n\\end{definition}\n\nWe maintain the perfect shape for any operation on a 2-4 tree. However, this means that they can only have 1, 3, 7, 15, 31 nodes, etc. Our solution is to add multiple data items to each node.\n\n\\begin{property}\n\tEvery node has 1-3 data items, and 2-4 children. Every internal node always has one more child than the number of data items.\n\\end{property}\n\n\\begin{note}\n\tNote that 2-4 trees are not binary trees.\n\\end{note}\n\n\\begin{property}\n\tLet \\( d_{1}, d_{2}, d_{3} \\) be the data in the node, and \\( t_{1}, t_{2}, t_{3}, t_{4} \\) be the data of the children.\n\t\\begin{itemize}\n\t\t\\item The data in each node must be sorted (\\( d_{1} < d_{2} < d_{3} \\)).\n\t\t\\item The data of each children must be sorted between the node's data: \\( t_{1} < d_{1} \\), \\( d_{1} < t_{2} < d_{2} \\), \\( d_{2} < t_{3} < d_{3} \\), \\( d_{3} < t_{4} \\).\n\t\\end{itemize}\n\\end{property}\n\n\\subsection{Operations}\n\n\\subsubsection{Contains}\nVery similar to a binary search tree.\n\n\\begin{algorithm}[H]\n\t\\caption{Contains}\n\t\\uIf{data < \\( d_{1} \\)}{\n\t\texplore \\( t_{1} \\)\\;\n\t}\\uElseIf{data < \\( d_{2} \\)}{\n\t\texplore \\( t_{2} \\)\\;\n\t}\\uElseIf{data < \\( d_{3} \\)}{\n\t\texplore \\( t_3 \\)\\;\n\t}\\Else{\n\t\texplore \\( t_{4} \\)\\;\n\t}\n\\end{algorithm}\n\n\\subsubsection{Add}\nA little bit harder. We must maintain the property that all leaves have the same height. We use the same logic as the contains algorithm, and add it to the node. \\par\nIf there are already three data items in the leaf (overflow), we solve it with a process called promotion. We push one data item into the parent (either \\( d_{2},d_{3} \\)), and split the rest of the data into two leaves around the promoted data. \\par\nIf the parent is now too full, repeat the process again. If the root is too full, then the promoted data becomes the new root, and the root splits into two children.\n\n\\subsubsection{Remove}\nThere are several cases to consider:\n\\begin{description}\n\t\\item[Case 1:] Removing from a leaf with 2 or 3 data items. This case is very easy, as we just remove the data from the leaf.\n\t\\item[Case 2:] Removing from an internal node. This case is simple as well, as we maintain the structure of the tree, replacing the data with its predecessor or successor. Because the predecessor or successor is always in a leaf, we can use Case 1, 3 or 4 to remove the predecessor or successor.\n\\end{description}\n",
        "lec_20.tex": "\\lecture{20}{Fri 13 Oct 2023 15:32}{2-4 Trees Continued}\n\nRemove operation continued:\n\n\\begin{description}\n\t\\item[Case 3:] We want to remove from a leaf with only one data item, but the siblings have spare data. This operation is called a \\textbf{transfer} or \\textbf{rotation}. We take the spare data directly to the left or right, promote it into the parent, and pull the parent into the removed leaf. Note that if you transfer an internal node, you must move the subtrees as well!\n\t\\item[Case 4:] We want to remove from a leaf with only one data item, and there is no spare data in the left or right siblings. Then, we pull down from the parent (left or right) and \\textbf{fuse} two nodes together. \\par\n\t\tHowever, you might empty the parent, which requires you pull from the grandparent. However, the grandparent might be empty after that operation as well! Therefore, we stop when the parent has 2 or 3 data items, we use a transfer, or the root becomes empty.\n\\end{description}\n\nStill confused: here is a handly flowchart:\n\\begin{figure}[ht]\n    \\centering\n    \\incfig{2-4-tree-remove-flowchart}\n    \\caption{2-4 Tree Remove Flowchart}\n    \\label{fig:2-4-tree-remove-flowchart}\n\\end{figure}\n\n\\subsection{Runtime}\nThe runtime of a 2-4 tree is as follows:\n\\begin{itemize}\n\t\\item Adding a node may require up to \\( \\log n \\) promotions, which is \\( O(\\log n) \\).\n\t\\item Removing a node may require up to \\( \\log n \\) fusions, which is \\( O(\\log n) \\).\n\\end{itemize}\n\n\\begin{note}\n\tNote that even though an AVL and a 2-4 tree have the same runtime complexity, the depth of a 2-4 tree is less than or equal to that of an AVL tree. Therefore, it is used when going down the tree is costly - for example, when the data is stored on disk. For example, databases use a generation called a B-tree (where there are many items per node).\n\\end{note}\n\n",
        "lec_21.tex": "\\lecture{21}{Mon 16 Oct 2023 15:31}{Sorting}\n\n\\section{Sorting}\nWhat is an algorithm?\n\n\\begin{itemize}\n\t\\item A sequential list of instructions to accomplish some goal.\n\\end{itemize}\n\nFor each sorting algorithm, we will look at:\n\\begin{itemize}\n\t\\item Time complexity (best, average, worst)\n\t\\item Stability: equal valued items maintain their relative order. This is important because it allows us to sort by multiple criteria.\n\t\\item Adaptivity: an algorithm is faster if data is (partially) sorted.\n\t\\item In place: uses O(1) extra memory to sort the data.\n\\end{itemize}\n\nThere are also two types of sorting algorithms:\n\\begin{itemize}\n\t\\item Iterative: uses loops to iterate over the data. It is easier to implement and is usually in-place, but only sorts one item at a time.\n\t\\item Divide and Conquer: (usually) uses recursion that splits the data into smaller pieces, sorts them, and then merges them. It is usually faster, but requires more memory.\n\t\\item Non-Comparitive: uses special properties of the data to sort it. It is usually faster, but is only applicable to certain data.\n\\end{itemize}\n\n\\subsection{Bubble Sort}\n\nFor bubble sort, we iterate through the array from beginning to end, comparing pairs of adjacent items, which are swapped if they are out of order. We repeat this process, stopping at an earlier ending value until no swaps are made.\n\n\\begin{algorithm}\n\t\\caption{Bubble Sort}\n\t\\KwIn{A, the data to be sorted}\n\t\\For{i from \\( 0 \\to n-1 \\)}{\n\t\t\\For{j from \\( 0 \\to n-1-i \\)}{\n\t\t\t\\If{A[j] > A[j+1]}{\n\t\t\t\tswap(A[j], A[j+1])\\;\n\t\t\t}\n\t\t}\n\t}\n\\end{algorithm}\n\n\\subsubsection{Optimizations}\n\\begin{enumerate}\n\t\\item If we make no changes, we know that the array is sorted. This means that we can stop the algorithm early.\n\t\\item Track the last index where a swap was made, and only iterate up to that index. We can do this because we know that all elements after that index are sorted.\n\\end{enumerate}\n\n\\begin{algorithm}\n\t\\caption{Optimized Bubble Sort}\n\t\\KwIn{A, the data to be sorted}\n\tend = n - 2\\;\n\t\\While{end > -1}{\n\t\tlastSwap = -1\\;\n\t\t\\For{i : \\( 0 \\to end \\)}{\n\t\t\t\\If{A[i] > A[i+1]}{\n\t\t\t\tswap(A[i], A[i+1])\\;\n\t\t\t\tlastSwap = i\\;\n\t\t\t}\n\t\t}\n\t\tend = lastSwap - 1\\;\n\t}\n\\end{algorithm}\n\n\\subsubsection{Runtime}\nThe best case for bubble sort occurs when the data is already in sorted order. We still have to check that the data is sorted, so the best case is \\( O(n) \\).\n\nThe worst case for bubble sort occurs when the data is in reverse sorted order. We then do \\( n + n - 1 + n - 2 + \\ldots + 1 = \\frac{n}{2}(n+1) = \\frac{n^2 + n}{2}\\) operations, which is \\( O(n^2) \\).\n\nThe average case is, unfortunately, very close to the worst case. Therefore, it is \\( O(n^2) \\). Why is this?\n\nIf we randomly order the data, then on average half of the pairs are out of order. Every swap in our algorithm fixes one pair. There are on average \\( \\frac{n(n-1)}{4} \\) pairs that are out of order, therefore our runtime is \\( O(n^2) \\) on average.\n\n\\subsubsection{Conclusion}\nBubble sort is stable, adaptive, and in-place.\n\n\\subsection{Insertion Sort}\nWe iterate over the elements, swapping the current element backwards until they are at the proper position in the first part of the array.\n\n\\subsubsection{Runtime}\nThe best case for insertion sort occurs when the data is already in sorted order. We iterate over all data, and we do one comparison for each item, which results in a runtime of \\( O(n) \\).\n\nThe worst case for insertion sort is when the data is in reverse sorted order. Similar to bubble sort, the worst case is \\( O(n^2) \\).\n\nThe average case, also like bubble sort, is \\( O(n^2) \\).\n\n\\subsubsection{Conclusion}\nInsertion sort is stable, adaptive, and in-place.\n\n\\subsection{Selection Sort}\nWe find the largest item in our data, and swap it to the end of the array. We then repeat this process, swapping the next largest item to the second to last position, and so on until the data is sorted.\n\n\\subsubsection{Runtime}\nAll cases for selection sort is \\( O(n^2) \\). We don't do anything depending on the data, so our runtime is quadratic even if our data is sorted!\n",
        "lec_22.tex": "\\lecture{22}{Fri 20 Oct 2023 15:33}{Sorting Continued}\n\nContinuing on with selection sort:\n\n\\begin{algorithm}[H]\n\t\\caption{Selection Sort}\n\t\\KwIn{A, the array to sort}\n\t\\For{$i = A.length -1$ \\KwTo \\( 2 \\)}{\n\t\tSwap max(A[1..i]) with A[i]\\;\n\t}\n\\end{algorithm}\n\n\\subsubsection{Conclusion}\n\nSelection sort is in-place. However, it is not adaptive and not stable. Even still, selection sort uses the fewest swaps, which makes it applicable for applications where writing to memory is costly.\n\n\\subsection{Cocktail Shaker Sort}\nBubble sort is (kind of) adaptive. The Cocktail Shaker sort builds upon this, becoming adaptive for arrays like [2, 3, 4, 5, 6, 7, 8, 1].\n\nIn one iteration of the Cocktail Shaker sort, we run bubble sort from left-to-right, then once again from right-to-left. Note that as a result, both sides of the array will be sorted.\n\n\\subsubsection{Optimizations}\nWe can track the last swap in both directions. Then, we can stop the algorithm once we reach the last swap in both directions.\n\n\\subsubsection{Runtime}\nThe best case for this sort (sorted data) is \\( O(n) \\).\n\nThe average case for this sort is \\( O(n^2) \\).\n\nThe worst case for this sort (reverse sorted data) is \\( O(n^2) \\).\n\n\\subsubsection{Conclusion}\nJust like Bubble sort, it is in-place, stable, and adaptive.\n\n\\begin{note}\n\tWhy do we study bad sorting algorithms? They are stable, adaptive, and in-place. Fast sorts often sacrifice one of these properties, and are often harder to implement. Complex sorts use these sorts as building blocks, and they aren't even the worst (Bogosort, Stalinsort, etc)!\n\\end{note}\n\n\\begin{note}\n\tTheoretical computer scientists have found that the best possible worst case sorting algorithm sorts in \\( O(n\\log (n)) \\).\n\\end{note}\n\n\\subsection{Heap Sort}\nWe can achieve an \\( O(n\\log (n)) \\) sort by adding everything to a heap in \\( O(n) \\), and removing data one at a time in \\(  O(log(n)) \\).\n\n\\subsubsection{Conclusion}\nHeap sort is not in-place: while you can define and apply the heap logic on any backing array, more often than not you will be inserting your data into an external heap, which is \\( O(n) \\) extra memory.\n\nBecause heap sort is \\( O(n\\log (n)) \\) for every case, it is not adaptive.\n\n\\begin{note}\n\tWhat other data structures can we sort with? Are they good? Well, we can use an AVL/BST, add all the data in \\( O(n\\log (n)) \\)/\\( O(n^2) \\), and get the inorder traversal in \\( O(n) \\). A similar implementation can be created with skiplists.\n\\end{note}\n",
        "lec_23.tex": "\\lecture{23}{Mon 23 Oct 2023 15:33}{Merge Sort}\n\n\\subsection{Merge Sort}\n\nMerge sort is an example of a divide-and-conquer sort. It breaks a large problem into smaller problems, solves them, and combines the solutions into one larger solution.\n\nIt takes an array of data, splits them into two, sorts them (using merge sort yet again), and merges the two sorted lists back together.\n\n\\begin{algorithm}\n\t\\caption{Merge Sort}\n\t\\If{array.length == 1}{\n\t\treturn\\;\n\t}\n\tleft = arr[0:arr.length/2]\\;\n\tright = arr[arr.length/2:arr.length]\\;\n\tmergeSort(left)\\;\n\tmerseSort(right)\\;\n\tmerge(left, right, arr)\\;\n\\end{algorithm}\n\nHow do we merge the two sorted sublists? We track \\( i \\) and \\( j \\), which both start at 0. We compare the two elements at left[\\( i \\)] and right[\\( j \\)], and add the smaller one to the array. We then increment the index of the smaller element. \n\n\\begin{note}\n\tIf there  are two equal values, we take the element from the left sublist to maintain stability.\n\\end{note}\n\n\\subsubsection{Runtime}\nThe operation list of merge sort is \\( 2\\log (n) \\) tall, and there are \\( n \\) operations per level. Therefore, the runtime is (always) \\( O(n\\log (n)) \\).\n\n\\begin{note}\n\tNote that one such \"worst case\" (most comparisons) of merge sort for an array of size 8 is \\([5, 1, 7, 3, 2, 8, 6, 4]\\). This is because we have to compare every element in the first half with every element in the second half, and so on for each merge. Even still, this is \\( O(n\\log (n)) \\).\n\\end{note}\n\n\\subsubsection{Conclusion}\nMerge sort is stable, but not adaptive (same runtime for all case). We also need to create a second array to split and merge the arrays, so it is not in-place.\n\n\\subsection{Quicksort}\nQuicksort is also a divide-and-conquer algorithm. However, unlike merge sort, it is in place. It also can be done non-recursively, which is more efficient.\n\nThe overall plan of Quicksort is as follows:\n\\begin{enumerate}\n\t\\item Pick one data as the pivot.\n\t\\item Partition the data into two sublists: one with elements less than the pivot, and one with elements greater than the pivot.\n\t\\item We repeat this plan in each of the sublists, until the data is sorted.\n\\end{enumerate}\n\n",
        "lec_24.tex": "\\lecture{24}{Wed 25 Oct 2023 15:30}{QuickSort}\n\nHow do we partition the list? After picking the pivot,\n\\begin{enumerate}\n\t\\item We swap the pivot with arr[0],\n\t\\item Then we maintain two pointers \\( i \\) and \\( j \\) and walk them towards the center until both see two elements that are in the wrong spot.\n\t\\item We swap arr[i] and arr[j]\n\t\\item We then stop once the two pointers cross each other, and swap the pivot with \\( j \\).\n\\end{enumerate}\n\n\\subsubsection{Runtime}\nThe average runtime is \\( O(n \\log n) \\), and the worst case is \\( O(n^2) \\). Note that the best pivot is the median. Below are some sample pivots:\n\n\\begin{itemize}\n\t\\item arr[0] as the pivot runs in \\( O(n^2) \\) if the data is already sorted.\n\t\\item Median as the pivot runs in \\( O(n^2) \\) when we have a malicious user.\n\t\\item Random pivot runs in \\( O(n \\log n) \\) on average. But you could pick the worst pivot every time, which would have worst case \\( O(n^2) \\).\n\\end{itemize}\n\n\\begin{remark}\n\tIn this class, QuickSort refers to QuickSort with a random pivot.\n\\end{remark}\n\n\\begin{note}\n\tThere is an algorithm called Median-of-medians (grad algo), which gives you a guaranteed ``good enough'' pivot and runs fast enough such that the worst case is \\( O(n\\log (n)) \\). However, due to constant time factors, it is not used in practice.\n\\end{note}\n\n\\subsubsection{Conclusion}\nQuickSort is in-place, but not adaptive or stable. Note that QuickSort with recursion is technically not in-place due to the \\( \\log (n) \\) to \\( n \\) memory complexity of the call stack.\n\n\\subsubsection{QuickSelect}\nGiven an array of \\( n \\) elements, find the \\( k \\)-th smallest element in the array. Note that if \\( k=1 \\) or \\( k=n \\), we can find the smallest or largest element in \\( O(n) \\) timewith linear search. Also, if the array was already sorted, we can find the \\( k \\)-th smallest element in \\( O(1) \\) time.\n\nQuickSelect is important because it beats the obvious plan of first sorting the array and getting the \\( k \\)-th element, instead running in \\( O(n) \\) time on average. Instead of recursing to both sides as in QuickSort, we recurse only to the side with the \\( k \\)-th smallest element. \n\n\\begin{enumerate}\n\t\\item Same as steps of QuickSort\n\t\\item If \\( j=k+1 \\), we return the pivot. If \\( j > k-1 \\), we find the \\( k \\)-th smallest element in the left subarray. If \\( j < k+1 \\), we find the \\( k-j-1 \\)-th smallest element in the right subarray.\n\\end{enumerate}\n\nNote that because each time we pick an array, we do half as much work, such that we have a runtime of \\[\n\tn + \\frac{n}{2} + \\frac{n}{4} + \\ldots = 2n = O(n)\n.\\] on average. In the worst case, we have a runtime of \\( O(n^2) \\).\n",
        "lec_25.tex": "\\lecture{25}{Fri 27 Oct 2023 15:31}{RadixSort}\n\n\\subsection{RadixSort}\n\nRadixSort has an average case runtime of \\( O(n) \\)! But the best case we can do with sorting is \\( n\\log (n) \\), so what's the catch?\n\n\\begin{note}\n\tThe bound of \\( n\\log (n) \\) only applies to comparison-based sorting algorithms.\n\\end{note}\n\nBecause RadixSort cannot do comparisons, it can only sort string like things, including integers. Suppose you have a ton of integers. The idea of RadixSort is to sort by ones digit, then tens digit, then hundreds digit, etc. This order guarantees stability of the sort.\n\nHowever, we don't use the other sorts to sort by digits. Instead, we create buckets/queues and place items in their proper buckets.\n\n\\begin{algorithm}[H]\n\t\\caption{RadixSort}\n\t\\For{i = 0 to max number of digits}{\n\t\t\\For{each item}{\n\t\t\tenqueue item into bucket corresponding to digit i\n\t\t}\n\t\t\\For{each bucket}{\n\t\t\tdequeue all items into original list\n\t\t}\n\t}\n\\end{algorithm}\n\n\\begin{note}\n\tNote for negative digits, you can use 19 buckets to store all possible digits. \n\\end{note}\n\nHow are we going to manipulate the digits of a number? We could convert this string to a char array, but that would be expensive. Instead, we can use the mod operator and integer division to get the digits.\n\n\\begin{property}\n\tFor digit number \\( i \\), we can divide the number by \\( 10^{i-1} \\) and then mod by 10. This will give us the digit we want.\n\\end{property}\n\nWhat base should we choose for RadixSort? Note that if we choose a super large base, such as 2000, then we need to use 2000 buckets! The optimal base is usually \\( 2^8 \\), but varies depending on use case.\n\n\\subsubsection{Runtime}\n\nWe have to make \\( n \\) operations for each digit, and we have to do this for each digit. So the runtime is \\( O(nk) \\), where \\( k \\) is the number of digits in the largest number. \n\n\\subsubsection{Conclusion}\n\nRadixSort is stable, but it is not adaptive and in-place.\n",
        "lec_26.tex": "\\lecture{26}{Mon 30 Oct 2023 15:31}{Pattern Matching}\n\n\\section{Pattern Matching}\n\nWhat is our motivation for this problem? Imagine we want to find a pattern in a text. What is the most efficient way of doing so?\n\n\\begin{eg}\n\tGoogle searches the entire internet for a pattern: your name. We can also search very long DNA for specific codons (the pattern).\n\\end{eg}\n\n\\begin{notation}\n\tLet the length of the text be \\( n \\). Let the length of the pattern be \\( m \\).\n\\end{notation}\n\n\\begin{definition}\n\tLet the \\textbf{alphabet} be the set of valid characters for the text.\n\\end{definition}\n\nThere are two types of pattern matching, which differ on what to do when we find the pattern. If we stop, that means we are searching for a single occurence. If we continue, that means we are searching for all occurences.\n\n\\subsection{Brute Force}\n\nBrute force is not just for pattern matching. Instead, we try every possible solution. For pattern matching specifically, we have \\( O(n) \\) (\\( O(n-m) \\)) choices for the start of the pattern. We check each starting spot for the pattern.\n\n\n\\begin{algorithm}[H]\n\t\\caption{Brute Force Pattern Matching}\n\ti = 0\\;\n\t\\While{i < len(text) - len(pattern)}{\n\t\tj = 0\\;\n\t\t\\While{j < len(pattern) and text[i+j] = pattern[j]}{\n\t\t\tj++\\;\n\t\t}\n\t\t\\If{j = len(pattern)}{\n\t\t\treturn True\\;\n\t\t}{\n\t\t\tj = 0\\;\n\t\t\ti++\\;\n\t\t}\n\t}\n\\end{algorithm}\n\n\\subsubsection{Runtime}\nThe best case for our brute force pattern matching is \\( O(n) \\). The worst case is \\( O(nm) \\), because we need to check every \\( n-m \\) starting positions at most \\( m-1 \\) times. For English, the average case is \\( O(n) \\). \n\nNote that we don't usually care about the average case because english text is not random. Even though brute force is fine, is there a better way to search for patterns?\n\n\\begin{note}\n\tJava String.contains() uses brute force.\n\\end{note}\n\n\\subsection{Boyer-Moore}\n\nThe idea for this algorithm is to use the wrong character to shift the pattern by a certain amount with a lookup table. Our lookup table stores the last appearance of each character in the pattern. If a character has not appeared, this index is -1.\n\n\\begin{algorithm}[H]\n\t\\caption{Boyer-Moore Pattern Matching}\n\t\\KwIn{text, pattern}\n\t\\KwOut{True if pattern in text, False otherwise}\n\t\\For{i < len(pattern)}{\n\t\t\tlookup[pattern[i]] = i\\;\n\t}\n\ti = 0\\;\n\t\\While{i < len(text) - len(pattern)}{\n\t\tj = len(pattern) - 1\\;\n\t\t\\While{j >= 0 and text[i+j] = pattern[j]}{\n\t\t\tj -= 1\\;\n\t\t}\n\t\t\\If{j = -1}{\n\t\t\treturn True\\;\n\t\t}{\n\t\t\ti += max(1, j - lookup[text[i+j]])\\;\n\t\t}\n\t}\n\treturn False\\;\n\\end{algorithm}\n",
        "lec_27.tex": "\\lecture{27}{Wed 01 Nov 2023 15:32}{Pattern Matching Continued}\n\n\\subsubsection{Runtime}\n\nThe worst case runtime is \\( O(mn) \\). The best case for finding a single occurence is \\( O(m) \\). The best case for failing to find an occurence or finding all occurences is \\( O(\\frac{n}{m}) \\). However, be careful that building the lookup table takes \\( O(m) \\) time, so technically our best case runtime for these two cases is \\( O(\\frac{n}{m} +m) \\).\n\n\\begin{note}\n\tWe should use Boyer-Moore whenever the text has characters not in the pattern. This is more likely as the alphabet grows, so it is better for larger alphabets. \n\\end{note}\n\nTraditional Boyer-Moore includes a good suffix rule, which improves big O runtime, but doesn't really speed up runtime in real life scenarios. It is also quite similar to KMP, but we will not cover it.\n\n\\subsection{Knuth-Morris-Pratt}\n\nWhat is the longest word that you can think of which has no repeated letters? Demographic.\n\nNote that when we try to find pattern\t``demographic'' in a text, we can shift, no matter what, the d all the way to under the c. However, if there are repeated patterns, we can shift the word to align with another pattern to save work. That is the core idea of KMP.\n",
        "lec_28.tex": "\\lecture{28}{Fri 03 Nov 2023 15:31}{KMP Continued}\n\nDepending on which letter we failed on when checking the pattern, we can shift by a predetermined amount. Note that we keep track of \\( j - \\text{shift} - 1 \\) (\\( j \\) is index of pattern) and insert this value into the failure table into \\( j-1 \\), as this is the number of characters in our pattern that are already matched for the next iteration of the algorithm.\n\nHow do we get the failure table? We need to know how much of the beginning matches the end, i.e. is there a matching prefix/suffix? More formally, the failure table at \\( j \\) is the length of the longest string which is both a prefix of the pattern and a proper suffix of \\( p[0\\ldots 1] \\).\n\n\\begin{eg}\n\tLet the pattern be ``axabaxax''. The failure table is as follows:\n\t\\begin{center}\n\t\t\\begin{tabular}{|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Prefix} & \\textbf{Longest Prefix/Suffix} \\\\\n\t\t\t\\hline\n\t\t\ta & 0 \\\\\n\t\t\tax & 0 \\\\\n\t\t\taxa & 1 \\\\\n\t\t\taxab & 0 \\\\\n\t\t\taxaba & 1 \\\\\n\t\t\taxabax & 2 \\\\\n\t\t\taxabaxa & 3 \\\\\n\t\t\taxabaxax & 2 \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\\end{eg}\n\n\\begin{eg}\n\tLet the pattern be ``eminem''. The failure table is as follows:\n\t\\begin{center}\n\t\t\\begin{tabular}{|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Prefix} & \\textbf{Longest Prefix/Suffix} \\\\\n\t\t\t\\hline\n\t\t\te & 0 \\\\\n\t\t\tem & 0 \\\\\n\t\t\temi & 1 \\\\\n\t\t\temin & 0 \\\\\n\t\t\temine & 1 \\\\\n\t\t\teminem & 2 \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\\end{eg}\n\n\\begin{eg}\n\tLet the pattern be ``ababaab''. The failure table is as follows:\n\t\\begin{center}\n\t\t\\begin{tabular}{|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Prefix} & \\textbf{Longest Prefix/Suffix} \\\\\n\t\t\t\\hline\n\t\t\ta & 0 \\\\\n\t\t\tab & 0 \\\\\n\t\t\taba & 1 \\\\\n\t\t\tabab & 2 \\\\\n\t\t\tababa & 3 \\\\\n\t\t\tababaa & 1 \\\\\n\t\t\tababaab & 2 \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\\end{eg}\n\n\\begin{eg}\n\tLet the pattern be ``salsas''. The failure table is as follows:\n\t\\begin{center}\n\t\t\\begin{tabular}{|c|c|}\n\t\t\t\\hline\n\t\t\t\\textbf{Prefix} & \\textbf{Longest Prefix/Suffix} \\\\\n\t\t\t\\hline\n\t\t\ts & 0 \\\\\n\t\t\tsa & 0 \\\\\n\t\t\tsal & 0 \\\\\n\t\t\tsals & 1 \\\\\n\t\t\tsalsa & 2 \\\\\n\t\t\tsalsas & 1 \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\\end{eg}\n\nHow do we run KMP?\n\\begin{enumerate}\n\t\\item We iterate L to R through the pattern\n\t\\item If we fail at an index \\( j \\) (in the pattern), suppose \\( FT[j-1]=k \\).\n\t\\item If \\( k > 0  \\), we can save some matched characters, so we shift until \\( k \\) characters of the patterns still overlap with previous matched characters (i.e. shift by \\( j - k + 1 \\)). Note that we don't need to recompare these still overlapping characters!\n\t\\item If \\( k=0 \\), then there is no useful repetition. Therefore, we should align the pattern with the mismatched character.\n\t\\item If \\( j=0 \\), then we cannot access the failure table. Therefore, we should just shift by one.\n\t\\item If we have a complete match, use the last entry in the failure table (i.e. \\( FT[m-1] \\) where \\( m \\) is the length of the pattern).\n\\end{enumerate}\n\n\\begin{algorithm}[H]\n\t\\caption{Failure Table Generation}\n\t\\KwIn{Our pattern}\n\ti = 0 \\tcp*{prefix}\n\tj = 1 \\tcp*{suffix, also current ft entry index}\n\tft = int[len(pattern)]\\;\n\tft[0] = 0\\;\n\t\\While{j < len(pattern)}{\n\t\t\\uIf{pattern[i] == pattern[j]}{\n\t\t\tft[j] = i + 1\\;\n\t\t\ti++; j++\\;\n\t\t}\n\t\t\\Else{\n\t\t\t\\uIf{i == 0}{\n\t\t\t\tft[j] = 0\\;\n\t\t\t\tj++\\;\n\t\t\t}\n\t\t\t\\Else{\n\t\t\t\ti = ft[i-1] \\tcp*{progress before, try smaller prefix suffix pair}\n\t\t\t}\n\t\t}\n\t}\n\\end{algorithm}\n\n",
        "lec_29.tex": "\\lecture{29}{Mon 06 Nov 2023 15:44}{KMP Continued Continued}\n\nHere is the pseudocode for KMP:\n\n\\begin{algorithm}[H]\n\t\\caption{KMP}\n\t\\KwIn{Pattern $P$ of length $m$, text $T$ of length $n$}\n\t\\KwOut{All indices $i$ such that $T[i..i+m-1] = P$}\n\tft = makeFT(\\( P \\))\\;\n\ti = 0\\;\n\tj = 0\\;\n\t\\While{\\( i \\le n - m \\)}{\n\t\t\\uIf{T[i] == P[j]}{\n\t\t\t\\uIf{j == m - 1}{\n\t\t\t\tj = ft[j]\\;\n\t\t\t\ti++\\;\n\t\t\t}\n\t\t\t\\Else{\n\t\t\t\ti++\\;\n\t\t\t\tj++\\;\n\t\t\t}\n\t\t}\n\t\t\\Else{\n\t\t\t\\uIf{j == 0}{\n\t\t\t\ti++\\;\n\t\t\t}\n\t\t\t\\Else{\n\t\t\t\tj = ft[j] - 1\\;\n\t\t\t}\n\t\t}\n\t}\n\\end{algorithm}\n\n\\subsubsection{Runtime}\n\nThe worst case time is \\( O(m+n) \\). Note that the trivial best case is when you are trying to find one occurrence and it is at the start of the case, which is \\( O(m) \\).\n\n\\begin{note}\n\tNote that KMP can get stuck sometimes.\n\\end{note}\n\n\\begin{eg}\n\tLet the text be aaab..., and the pattern be aaaa. Then, we would check the pattern 4 times here, which is less efficient that Boyer-Moore. In other words, KMP does not recognize that b is not in the pattern.\n\\end{eg}\n\n\\begin{note}\n\tBoyer Moore has a better best case with \\( O(\\frac{n}{m} + m) \\), but a worse worst case with \\( O(mn) \\). Of the two, KMP is preferred for smaller alphabets, and Boyer-Moore is preferred for large alphabets.\n\\end{note}\n",
        "lec_30.tex": "\\lecture{30}{Wed 08 Nov 2023 15:33}{Robin Karp}\n\n\\subsection{Robin Karp}\n\nInstead of comparing character by character, we can use a rolling hash function to compare substrings. For each alignment, compare \\( hash(pattern) \\) to \\( hash(text[i\\ldots i+m-1]) \\). If the hashes are different, we know the two strings are different. However, if they are the same, then you have to check character by character. Can we optimize finding the hash of the substring \\( O(1) \\)?\n\n\\subsubsection{Rolling Hash}\nThe first hash we calculate is \\( O(m) \\), but updating the hash is \\( O(1) \\). Each hash is calculated from the previous hash in \\( O(1) \\) time. A potential hash is to add all the algorithms together. Then, we can subtract the first one and add the next one to roll the hash. However, this leads to many collisions.\n\nThe actual hash uses a base \\( B \\), which is prime. Then, we treat the string as a base \\( B \\) number. Then, abcd would be represented as \\[\n\ta \\times B^3 + b \\times B^2 + c \\times B + d\n.\\] Note that we can roll our hash by subtracting \\( a \\times B^3 \\), multiplying what remains by \\( B \\), and then adding the new character \\( e \\). \n\n\\subsubsection{Runtime}\nThe best case runtime is when we find one occurence at the beginning, which is \\( O(m) \\). For all occurences, the hash never matches, so we have a runtime of \\( O(n+m) \\). In the worst case, the hash always matches the text, so we have to compare character by character, which is \\( O(nm) \\). \n\n\\subsection{Galil Rule}\nThe Galil rule is a combination of KMP and Boyer Moore. Essentially, when we have a full match in Boyer Moore, we can shift by the period (\\( m-ft[m-1] \\)) as we do in KMP. This optimizes the worst case in which the text consists of many patterns.\n",
        "lec_31.tex": "\\lecture{31}{Fri 10 Nov 2023 15:42}{Graphs}\n\n\\section{Graphs}\n\nFor motivation, think about landlocked and doubly landelocked countries. How would we find the number of doubly landlocked countries? We could use a graph where countries are nodes, and edges are placed between two things that are touching.\n\n\\begin{definition}\n\tA \\textbf{vertex} is a node in a graph. \n\\end{definition}\n\n\\begin{definition}\n\tAn \\textbf{edge} connects two vertices in a graph.\n\\end{definition}\n\n\\begin{note}\n\tThe way you draw the graph doesn't matter!\n\\end{note}\n\n\\begin{definition}\n\tTwo vertices are \\textbf{adjacent} if they share an edge.\n\\end{definition}\n\n\\begin{definition}\n\tA vertex is \\textbf{incident} to an edge if it is an endpoint of that edge.\n\\end{definition}\n\n\\begin{definition}\n\tThe \\textbf{degree} of a vertex is the number of vertices it is adjacent to.\n\\end{definition}\n\n\\begin{definition}\n\tA vertex is \\textbf{isolated} if its degree is 0.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{simple} graph is a graph without self-loops and multiple edges.\n\\end{definition}\n\n\\begin{definition}\n\tThe number of vertices in \\( G \\), denoted the \\textbf{order} of \\( G \\), is \\( |V| \\).\n\\end{definition}\n\n\\begin{definition}\n\tThe number of edges in \\( G \\), denoted the \\textbf{size} of \\( G \\), is \\( |E| \\).\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{directed} graph is a graph where edges have a direction.\n\\end{definition}\n\n\\begin{eg}\n\tCurrency exchange rates can be modeled with a directed graph.\n\\end{eg}\n\n\\begin{eg}\n\tFlights can be modeled with a directed graph.\n\\end{eg}\n\n\\begin{eg}\n\tPathfinding between locations in a map.\n\\end{eg}\n\nHow do we traverse a graph?\n\n\\begin{definition}\n\tA \\textbf{walk} is a traversal across a graph through a series of edges.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{path} is a walk in which no vertex or edge is repeated.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{trail} is a walk in which no edge is repeated.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{cycle} is a path in which the first and last vertices are the same.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{circuit} is a trail in which the first and last vertices are the same.\n\\end{definition}\n\n\\begin{definition}\n\tTwo vertices are \\textbf{connected} if there is a path from one to another.\n\\end{definition}\n\n\\begin{definition}\n\tA graph is \\textbf{connected} if every pair of vertices is connected.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{tree} is an acyclic connected graph.\n\\end{definition}\n\n\\begin{note}\n\tAll trees have \\( |V|-1 \\) edges.\n\\end{note}\n\n\\begin{definition}\n\tA \\textbf{clique} is a graph with the maximum number of edges.\n\\end{definition}\n\n\\begin{note}\n\tAll cliques have \\( \\frac{n(n-1)}{2} \\approx O(|V|^2)\\) edges.\n\\end{note}\n\n\\begin{definition}\n\tA \\textbf{sparse} graph has \\( \\approx |V| \\) edges (tree).\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{dense} graph has \\( \\approx |V|^2 \\) edges (clique).\n\\end{definition}\n",
        "lec_32.tex": "\\lecture{32}{Mon 13 Nov 2023 15:32}{DFS}\n\n\\subsection{Graph Representations}\n\nHow do we store a graph in a computer? There are to main ideas: adjacency matrices and adjacency lists. The rows are the ``out'' vertices and the columns are the ``in'' vertices, and the matrix itself can store the label or weight or some other value of the edge exists.\n\n\\begin{note}\n\tAn undirected graph will have a symmetric adjacency matrix.\n\\end{note}\n\nThe major appeal of using a matrix is having O(1) operations to remove, add, or set edges. However, there is a lot of space used for nothing - we will always use \\( O(|V|^2) \\) memory. Also, adding or removing new vertices to the graph is expensive, also time \\( O(|V|^2) \\) as we need to copy our data to a new array. Also, getting the list of adjacent vertices is time \\( O(|V|) \\).\n\nAn adjacency list is a map from vertices to a list of edges incident to the vertex. With this representation, we can add/remove a vertex in time \\( O(1) \\). The worst case for removing an edge is \\( O(\\deg(V)) \\approx O(|V|) \\). And, we can get edges incident to this vertex in time \\( O(1) \\), and the vertices at the end of the edges in time \\( O(\\deg(V)) \\approx O(|V|) \\)\n\n\\begin{note}\n\tAn adjacency matrix should be used for dense graphs, and an adjacency list should be used for sparse graphs.\n\\end{note}\n\nWe can also use an edge list, which is a lot worse. This is because we need to look through every single edge in our list to find specific information, which is \\( O(|E|) \\).\n\nStarting at a vertex in the graph, which vertices are reachable?\n\nThe naive solution is to wander through the grarph randomly, and note when a new vertex is seen. This is very slow because it repeats vertices. We can optimize this by marking nodes as we visit them. However, we can get trapped and miss some nodes.\n\nThe actual plan is, as we traverse through the graph, we will store the vertices seen but not traversed into a to-do list. When we have nothing to do, we will check the to-do list to jump to the next vertex. If we insert new items into the to-do list at the front, we have DFS. If we insert new items to the back of the to-do list, we have BFS.\n\n\\subsection{Depth-First Search}\n\n\\begin{algorithm}\n\t\\caption{DFS}\n\t\\KwIn {Graph \\(G = (V, E)\\), start vertex \\(v\\)}\n\t\\For{\\( w \\) adj to \\( v \\)}{\n\t\t\\If{\\( w \\) is not marked}{\n\t\t\tmark \\( w \\) as seen\\;\n\t\t\tDFS\\( (G, w) \\)\\;\n\t\t}\n\t}\n\\end{algorithm}\n\n\\begin{note}\n\tIn this class, the tiebreaker for which node to visit first is alphabetical.\n\\end{note}\n\nDFS can be used for finding connected components, cycles, spanning trees, as well as finding ``a'' path between vertices.\n\n\\subsubsection{Runtime}\n\nDFS has a time complexity of \\( O(|V|+|E|) \\).\n"
    },
    "cs-2051": {
        "lec_01.tex": "\\lecture{1}{Wed 24 Aug 2023 12:54}{Logical Equivalencies}\n",
        "lec_02.tex": "\\lecture{2}{Wed 04 Oct 2023 12:58}{Quantifiers}\n",
        "lec_03.tex": "\\lecture{3}{Wed 04 Oct 2023 12:58}{Nested Quantifiers}\n",
        "lec_04.tex": "\\lecture{4}{Wed 04 Oct 2023 12:58}{Rules of Inference}\n",
        "lec_05.tex": "\\lecture{5}{Wed 04 Oct 2023 12:58}{Proof by Contradiction}\n",
        "lec_06.tex": "\\lecture{6}{Wed 04 Oct 2023 12:59}{Proof by Cases, Colorings, and Invariants}\n",
        "lec_07.tex": "\\lecture{7}{Wed 04 Oct 2023 12:59}{Advanced Proof Techniques}\n",
        "lec_08.tex": "\\lecture{8}{Wed 04 Oct 2023 12:59}{Intro to Sets}\n",
        "lec_09.tex": "\\lecture{9}{Wed 04 Oct 2023 12:59}{Set Proofs and Relations}\n",
        "lec_10.tex": "\\lecture{10}{Wed 04 Oct 2023 13:00}{Exam 1 Review}\n",
        "lec_11.tex": "\\lecture{11}{Tue 03 Oct 2023 14:01}{Cardinality; Countable Sets}\n\nWe used sets to talk about \\textit{relations}. Depending what relations we were looking at, we could determine whether they were \\textit{reflextive, symmetric, antisymmetric, or transitive.}\n\n\\begin{definition}\n\tAn \\textbf{equivalence relation} is one that is reflexive, symmetric, and transitive.\n\\end{definition}\n\n\\begin{definition}\n\tAn \\textbf{ordering relation} is one that is reflexive, antisymmetric, and transitive.\n\\end{definition}\n\n\\section{Functions}\n\nBelow are some definitions that we use for functions.\n\n\\begin{definition}\n\tA \\textbf{function} is a relation such that \\( \\forall a \\exists !b (f(a) = b) \\).\n\\end{definition}\n\n\\begin{definition}\n\tFor a particular function \\( f \\colon A \\to B  \\), we call \\( A \\) the \\textbf{domain} and \\( B \\) the \\textbf{codomain}.\n\\end{definition}\n\n\\begin{definition}\n\tThe \\textbf{range} of \\( f \\) is \\( \\{b \\in  B ~|~ \\exists  a \\in  A (f(a) = b)\\}   \\)\n\\end{definition}\n\n\\begin{definition}\n\tWe say \\( f \\) is \\textbf{one-to-one} if \\( \\forall x,y \\in A(x \\neq  y \\implies f(x) \\neq f(y)) \\). Another word for this is \\textbf{injective}. We can use the contrapositive to prove a function \\( f \\) is injective.\n\\end{definition}\n\n\\begin{definition}\n\tWe say \\( f \\) is \\textbf{onto} if \\( \\forall b \\in B (\\exists a \\in  A(f(a) = b)) \\).\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{bijection} is a function \\( f \\) that is one-to-one and onto.\n\\end{definition}\n\n\\begin{definition}\n\tGiven a bijection \\( f \\), we define the \\textbf{inverse function} \\( f^{-1} \\)\n\\begin{align*}\n\t& f^{-1} \\colon B \\to A \\\\\n\t& f^{-1}(b) = a \\iff f(a) = b\n\\end{align*}\n\\begin{property}\n\t\\( f^{-1} \\) is a bijection.\n\\end{property}\n\\end{definition}\n\n\\begin{eg}\n\tConsider:\n\t\\begin{align*}\n\t\tf \\colon & \\mathbb{R} \\to \\mathbb{R}^+ \\\\\n\t\t\t\t\t\t & x \\to  x^2\n\t.\\end{align*}\n\tThis function by itself is not one-to-one. But note that we can make restrictions in our domain and codomain to make this function a bijection.\n\\end{eg}\n\\begin{eg}\n\tConsider:\n\t\\[\n\t\t\\sin \\colon \\mathbb{R} \\to \\mathbb{R}\n\t.\\] \n\tThis function is not one-to-one and onto (not a bijection). Therefore, to find the inverse of the function, we restrict the domain and codomain.\t\\[\n\t\t\\sin \\colon \\left[-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right] \\to [-1,1]\n\t.\\] \n\\end{eg}\n\n\\begin{definition}\n\tLet \\( S \\) be a set. We say \\( S \\) has \\textbf{cardinality} \\( n \\) if \\( S \\) has exactly \\( n \\) elements (there is a bijection from \\( S \\) to the set \\( [n]=\\{1, 2, 3, \\ldots , n\\}   \\)). We write \\( |S|=n \\).\n\\end{definition}\n\\begin{remark}\n\tCardinality is \\textit{well defined} because you have no bijection between \\( [n] \\) and \\( [m] \\) for \\( n \\neq  m \\).\n\\end{remark}\n\\begin{remark}\n\tIf \\( S \\) is infinite, then we say \\( |S| = \\infty \\). In other words, there does not exist a biection from \\( S \\) to any \\( [n] \\).\n\\end{remark}\n\n\\begin{definition}\n\tLet \\( A \\) and \\( B \\) be sets. We say \\( |A| = |B| \\) if there is a bijection from \\( A \\) to \\( B \\).\n\\end{definition}\n\n\\begin{eg}\n\tLet \\( \\sim \\) be a relation on \\( \\mathcal{P}(U) \\). \\( A \\sim B \\) if there is a bijection from \\( A \\) to \\( B \\). Are all infinite sets the same?\n\\end{eg}\n\n\\section{Countability}\nWhat does it mean for a set to be countable?\n\n\\begin{definition}\n\tA set \\( S \\) is \\textbf{countable} if there is a one-to-one mapping (\\( \\exists f \\colon S \\to \\mathbb{N} \\) that is a bijection) from \\( S \\) to \\( \\mathbb{N} \\).\n\\end{definition}\n\\begin{itemize}\n\t\\item Every finite set is countable.\n\t\\item \\( \\mathbb{N} \\) is countable (you can map \\( \\mathbb{N} \\) to itself).\n\t\\item \\( 2 \\mathbb{N} = \\{2a ~|~ a \\in  \\mathbb{N}\\}   \\) is countable.\n\t\t\\begin{align*}\n\t\t\tf \\colon & 2 \\mathbb{N} \\to  \\mathbb{N} \\\\\n\t\t\t\t\t\t\t & 2a \\to a\n\t\t.\\end{align*}\n\t\t\\begin{remark}\n\t\t\tThere is a one-to-one mapping between sets \\( 2\\mathbb{N} \\) and \\( \\mathbb{N} \\). That means they have the same cardinality, even if this goes against our intuition.\n\t\t\\end{remark}\n\\end{itemize}\n\n",
        "lec_12.tex": "\\lecture{12}{Thu 05 Oct 2023 13:35}{Properties of Countable Sets; Cantor's Diagonalization Argument}\n\nContinuing on from last time, we also have that \\( \\mathbb{Z} \\) is countable as well.\n\n\\begin{theorem}\n\t\\( \\mathbb{Z} \\) is countable.\n\\end{theorem}\n\\begin{proof}\n\tConstruct \\( f \\colon \\mathbb{Z} \\to  \\mathbb{N} \\). Define:\n\t\\[\n\t\tf(n) = \\begin{cases}\n\t\t\t0 & n = 0 \\\\\n\t\t\t2n & n > 0 \\\\\n\t\t\t-2n + 1 & n < 0\n\t\t\\end{cases}\n\t.\\] We wish to prove that \\( \\forall x,y (x \\neq  y \\implies f(x) \\neq  f(y)) \\). Proceeding by contraposition, given \\( f(n)=f(m) \\), we have the following cases:\n\t\\begin{description}\n\t\t\\item[Case 1:] \\( n = 0 \\). Then \\( f(n) = 0 = f(m) \\implies m = 0\\).\n\t\t\\item[Case 2:] \\( n > 0, m > 0 \\). Then \\( f(n) = 2n = f(m) = 2m \\implies 2n = 2m \\implies n = m \\)\n\t\t\\item[Case 3:] \\( n > 0, m < 0 \\). Then \\( f(n) \\) is even and \\( f(m) \\) is odd \\contra. Vacuously true.\n\t\\end{description}\n\tThe other cases are analogous!\n\\end{proof}\n\n\\begin{property}\n\tLet \\( A,B \\) be countable sets. There are the following properties of countable sets:\n\t\\begin{itemize}\n\t\t\\item \\( \\{a\\} \\cup A  \\) is countable.\n\t\t\t\\begin{proof}\n\t\t\t\tWe wish to find \\( g \\colon \\{a\\}  \\cup  A \\to \\mathbb{N}  \\). Construct \\[\n\t\t\t\t\tg(x) = \\begin{cases}\n\t\t\t\t\t\t0 & x = a \\\\\n\t\t\t\t\t\tf(x) + 1 & x \\in A\n\t\t\t\t\t\\end{cases}\n\t\t\t\t.\\] \t\t\n\t\t\t\\end{proof}\n\t\t\\item \\( F \\cup A \\) is also countable for any finite set \\( F \\).\n\t\t\\item \\( A \\cup  B \\) is also countable.\n\t\t\\item If \\( f \\colon S \\to A \\) is one-to-one and \\( A \\) is countable, then \\( S \\) is countable.\n\t\t\\item \\( A \\times B \\) is countable.\n\t\t\\item \\( S \\subseteq A \\) is countable.\n\t\t\t\\begin{proof}\n\t\t\t\tGiven \\( f \\colon A \\to  \\mathbb{N} \\) is one-to-one, its restriction to \\( S \\) is also one-to-one.\n\t\t\t\\end{proof}\n\t\t\\item \\( \\mathbb{Q} \\) is countable.\n\t\t\t\\begin{proof}\n\t\t\t\tThis is because \\( \\mathbb{Q} \\subseteq \\mathbb{Z} \\times \\mathbb{Z} \\). \n\t\t\t\\end{proof}\n\t\\end{itemize}\n\\end{property}\n\n\\subsection{Cantor's Diagonalization Argument}\nWhat about the real numbers?\n\\begin{theorem}\n\t\\( \\mathbb{R} \\) is not countable (Cantor).\n\\end{theorem}\n\\begin{proof}\n\tProve instead that \\( (0,1) \\) (a subset of \\( \\mathbb{R} \\)) is \\textbf{not} countable. We will argue by contradiction. We assume that \\( (0,1) \\) is countable. Hence, there exists \\[\n\t\tf \\colon (0,1) \\to \\mathbb{N} \n\t.\\] shown below.\n\n\t\\begin{table}[H]\n\t\t\\caption{Our one-to-one mapping (pages of our book)}\\label{tab:}\n\t\t\\begin{center}\n\t\t\t\\begin{tabular}[c]{|l|l|}\n\t\t\t\t\\hline\n\t\t\t\t\\multicolumn{1}{|c|}{\\textbf{\\( \\mathbb{N} \\)}} & \n\t\t\t\t\\multicolumn{1}{c|}{\\textbf{\\( (0,1) \\)}} \\\\\n\t\t\t\t\\hline\n\t\t\t\t0 & \\( 0.d_{11} d_{12} d_{13} d_{14} \\ldots  \\) \\\\\n\t\t\t\t1 & \\( 0.d_{21} d_{22} d_{23} d_{24} \\ldots  \\) \\\\\n\t\t\t\t2 & \\( 0.d_{31} d_{32} d_{33} d_{34} \\ldots  \\) \\\\\n\t\t\t\t3 & \\( 0.d_{41} d_{42} d_{43} d_{44} \\ldots  \\) \\\\\n\t\t\t\t\\vdots & \\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\\end{center}\n\t\\end{table}\n\n\tLet's define \\( b \\in \\mathbb{R} \\) as follows: \\[\n\t\tb = 0.b_1b_2b_3b_4 \\ldots\n\t.\\] where \\[\n\t\tb_i = \\begin{cases}\n\t\t\t3 & d_{ii} \\neq 3 \\\\\n\t\t\t0 & d_{ii} = 3\n\t\t\\end{cases}\n\t.\\] Then, we claim that \\( f(b) \\) is not well defined! In other words, there should exist \\( k \\in \\mathbb{N}\\) in our book such that \\( f(b) = k \\). However, we have constructed \\( b \\) such that \\( b_k \\neq d_{kk} \\): there is no \\( k \\in \\mathbb{N} \\) that exists in our book \\contra (\\( b \\) will always have one bit that is different from all entries in our mapping)!\n\\end{proof}\n\n\\exercise{1}\nProve that \\( |S| \\neq |\\mathcal{P}(S)| \\).\n",
        "lec_13.tex": "\\lecture{13}{Thu 12 Oct 2023 14:15}{Runtime Complexity}\n\n\\section{Analyzing Runtimes}\n\nHow many comparisons should we make to choose the best out of \\( n \\) restuarants?\n\n\\subsection{Big O Notation}\nBig O Notation does not care about constants.\n\n\\begin{definition}\n\tGiven \\( f,g \\colon \\mathbb{R} \\to \\mathbb{R} \\), we say \\( f \\) is big-O of \\( g \\) (write \\( f=O(g) \\)) if \\[ \\exists C>0, k \\in \\mathbb{R}(|f(x)| \\le  C \\cdot |g(x)| \\quad \\forall  x \\ge  k) \\] where \\( C \\) and \\( k \\) are arbitrary constants (witnesses).\n\\end{definition}\n\n\\begin{eg}\n\tGiven \\( f,g \\colon \\mathbb{N} \\to \\mathbb{R} \\), is \\( f=O(g) \\):\n\t\\begin{itemize}\n\t\t\\item \\( f(n) = 2n+1 \\), \\( g(n) = n - 10 \\)? Yes.\n\t\t\\item \\( f(n) = 2n+1 \\), \\( g(n) = n + \\sqrt{n}  \\)? Yes.\n\t\t\t\\begin{proof}\n\t\t\t\tWe have:\n\t\t\t\t\\begin{align*}\n\t\t\t\t\t2n & \\le 2n && \\forall  n \\in \\mathbb{N} \\\\\n\t\t\t\t\t1 & \\le 2\\sqrt{n} && \\forall n \\in  \\mathbb{N}, n > 0\n\t\t\t\t.\\end{align*}\n\t\t\t\tAdding both sides, we have \\[\n\t\t\t\t\tf(n) \\le  2g(n) \\quad \\forall n\\ge 1\n\t\t\t\t.\\] Therefore, \\( f = O(g) \\) for \\( C = 2, k = 1 \\).\n\t\t\t\\end{proof}\n\t\t\\item \\( f(n) = n\\log (n) \\), \\( g(n) = n + \\sqrt{n}  \\)? No.\n\t\t\t\\begin{proof}\n\t\t\t\tWe proceed by contradiction. Assume \\( f = O(g) \\). Then, there exists \\( C>0 \\) and \\( k \\in \\mathbb{N} \\) such that \\[\n\t\t\t\t\tn\\log (n) \\le  C(n + \\sqrt{n} ) \\quad \\forall n \\ge k\n\t\t\t\t.\\] Simplifying, we have: \n\t\t\t\t\\begin{align*}\n\t\t\t\t\t\\log (n) & \\le  C \\left(\\frac{n+ \\sqrt{n}}{n}\\right) \\\\\n\t\t\t\t\t\t\t\t\t\t& = C\\left(\\frac{n}{n} + \\frac{\\sqrt{n}}{n}\\right) \\\\\n\t\t\t\t\t\t\t\t\t\t& = C\\left(1 + \\frac{1}{\\sqrt{n} }\\right) \\\\\n\t\t\t\t\t\t\t\t\t\t& \\le 2C \\qquad \\forall n \\ge  1\n\t\t\t\t.\\end{align*}\n\t\t\t\tThis inequality yields a contradiction, as \\( \\log (n) \\) grows to infinity as \\( n \\to \\infty \\) and therefore cannot be bounded by a constant \\contra.\n\t\t\t\\end{proof}\n\t\\end{itemize}\n\\end{eg}\n\n\\begin{notation}\n\tLet \\( \\mathcal{F} = \\{f \\colon \\mathbb{N}\\to \\mathbb{R}\\}   \\). We say \\( f \\sim g \\) if \\( f = O(g) \\) and \\( g = O(f) \\).\n\t\\begin{note}\n\t\tThis relation between functions \\( f\\sim g \\) is reflextive, symmetric, and transitive.\n\t\\end{note}\n\\end{notation}\n\nNote that proving the big-O of a function with witnesses is very time-consuming. There are several properties of big-O that we can use to simplify proofs. \n\\begin{property}\n\tLet \\( f_i,g_i \\colon \\mathbb{N} \\to  \\mathbb{R}\\)\n\t\\begin{itemize}\n\t\t\\item If \\( f = O(G) \\), then \\( f + \\alpha = O(g) \\) for \\( \\alpha \\in \\mathbb{R} \\).\n\t\t\t\\begin{note}\n\t\t\t\tAs long as \\( g \\not\\to 0 \\) as \\( x \\to \\infty \\).\n\t\t\t\\end{note}\n\t\t\\item If \\( f_{1}=O(g)\\), \\( f_{2} = O(g) \\), then \\( f_{1} + f_{2} = O(g) \\)\n\t\t\\item If \\(\tp(x) = a_n x^n + a_{n-1}x^{n-1} + \\ldots + a_1 x + a_0 \\), then \\( p(x) = O(x^n) \\).\n\t\t\\item If \\( f_{1}=O(g_{1}), f_{2}=O(g_{2}) \\), then \\( f_{1}f_{2} = O(g_{1}g_{2}) \\).\n\t\t\\item \\( 1 \\le  \\log (n) \\le  n^{\\alpha} \\le n^{\\beta} \\le n^{\\beta}\\log (n) \\le 2^n \\) for \\( \\beta > \\alpha > 0 \\)\n\t\\end{itemize}\n\\end{property}\n\n\\begin{eg}\n\tGoing back to the restaurants, we have two forming strategies:\n\t\\begin{enumerate}\n\t\t\\item Compare one restuarant at a time.\n\t\t\\item Pair the restaurants. Keep the best of each pair. Repeat.\n\t\\end{enumerate}\n\tLet \\( T(n) =  \\) the number of meals needed to find the best restaurant out of a list of length \\( n \\). Then, for the first strategy, we have: \\[\n\t\tT(n) = T(n - 1) + 2\n\t\t.\\] For the second strategy, we have: \\[\n\t\t\tT(n) = T\\left(\\frac{n}{2}\\right) + n\n\t\t.\\] \n\n\tWe can solve the first strategy's recurrence relation with substitution:\n\t\\begin{align*}\n\t\tT(n) & = T(n- 1) + 2 \\\\\n\t\t\t\t\t& = T(n-2) + 2 + 2 \\\\\n\t\t\t\t\t& = T(n-3) + 2 + 2 + 2 \\\\\n\t\t\t\t\t& (\\ldots) \\\\\n\t\t\t\t\t& = T(n - (n - 1)) + 2(n - 1) \\\\\n\t\t\t\t\t& = 2(n - 1) \\tag{\\( T(1) = 0 \\)}\n\t.\\end{align*}\n\n\tAnd for the second:\n\t\\begin{align*}\n\t\tT(n) &= T\\left(\\frac{n}{2}\\right) + n \\\\\n\t\t\t\t\t&= T\\left(\\frac{n}{4}\\right) + \\frac{n}{2} + n \\\\\n\t\t\t\t\t&= T\\left(\\frac{n}{8}\\right) + \\frac{n}{4} + \\frac{n}{2} + n \\\\\n\t\t\t\t\t&(\\ldots ) \\\\\n\t\t\t\t\t&=n\\left(1 + \\frac{1}{2} + \\frac{1}{4} + \\ldots + \\frac{1}{2^{k-1}}\\right) \\\\\n\t.\\end{align*}\n\\end{eg}\n",
        "lec_14.tex": "\\lecture{14}{Tue 17 Oct 2023 14:17}{Solving Recurrence Relations}\n\n\\section{Induction}\n\n\\begin{eg}\n\tTake from last lecture the following recurrence relation: \\[\n\t\tT(n) = T(n - 1) + 2, \\quad T(1) = 0\n\t.\\] \n\tWe can \"guess\" the solution by looking at the relation. We guess \\( T(n) = 2 \\cdot n \\). Then, we have: \\[\n\t\tT(n) = 2(n - 1) + 2 = 2n - 2 + 2 = 2n, \\quad \\text{but } T(1) \\neq  2\n\t.\\] \n\tSo instead, we guess \\( T(n) = 2 \\cdot (n-1) \\), which satisfies our base case.\n\\end{eg}\n\nWe use induction to check if a statement \\( P(n) \\) is true for all \\( n \\in \\mathbb{N} \\). We check that \n\\begin{enumerate}\n\t\\item \\( P(0) \\) is true. This is called the base case of induction.\n\t\\item Assume \\( P(k) \\) is true for some \\( k \\in \\mathbb{N} \\). This is called the inductive hypothesis.\n\t\\item Prove that \\( P(k) \\implies P(k+1) \\). This is called the step of induction.\n\\end{enumerate}\n\n\\begin{eg}\n\tWe will prove that for all \\( n \\ge 1, n \\in \\mathbb{N} \\), \\[\n\t\t1 + 2 + \\ldots + n = \\frac{n(n+1)}{2}\n\t.\\] Let \\( P(n) \\coloneq 1 + 2 + \\ldots  + n = \\frac{n(n+1)}{2}\\). Formally, we wish to prove \\( \\forall n P(n) \\), \\( n \\ge 1, n \\in \\mathbb{N} \\). We will proceed with induction on \\( n \\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n = 1 \\). Then, we have \\[\n\t\t\tP(1) \\coloneq 1 = \\frac{1(1+1)}{2} = 1\n\t\t.\\]\n\t\t\\item[Step:] We assume that \\( P(k) \\) is true, where \\( k \\ge 1, k \\in \\mathbb{N} \\). We wish to show that \\( P(k+1) \\) is true. From our inductive hypothesis, we have \\[\n\t\t\t1 + 2 + \\ldots + k = \\frac{k(k+1)}{2}\n\t\t.\\] Adding \\( k + 1 \\) to both sides, we have \\[\n\t\t\t1 + 2 + \\ldots + k + (k+1) = \\frac{k(k+1)}{2} + k + 1 = \\frac{(k+1)(k+2)}{2}\n\t\t.\\] The last equality confirms that \\( P(k+1) \\) is true.\n\t\\end{description}\n\tIt follows by induction that \\( P(n) \\) is true for any \\( n \\ge 1, n \\in \\mathbb{N} \\).\n\\end{eg}\n\n\\begin{eg}\n\tLet \\( H_n = 1 + \\frac{1}{2} + \\frac{1}{3} + \\ldots  + \\frac{1}{n} \\). We wish to prove that \\( H_{2^n} \\ge  1 + \\frac{n}{2} \\) for \\( n \\in \\mathbb{N} \\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n = 0 \\). Then, we have \\[\n\t\t\t\tH_{2^0} = H_1 = 1 \\ge 1 + \\frac{0}{2} = 1 \n\t\t\t\\]\n\t\t\\item[Step:] Let \\( P(n) \\coloneq H_{2^n} \\ge 1 + \\frac{n}{2} \\). We assume \\( P(k) \\) is true for some \\( k \\in \\mathbb{N} \\). We will prove that \\( P(k) \\implies P(k+1) \\). From our inductive hypothesis, we have \\[\n\t\t\t1 + \\frac{1}{2} + \\frac{1}{3} + \\ldots  + \\frac{1}{2^k} \\ge 1 + \\frac{k}{2}\n\t\t.\\] We add to both sides \\(\\frac{1}{2^k+1} + \\frac{1}{2^k+2} + \\ldots + \\frac{1}{2^{k+1}}\\) such that \\[\n\t\t\tH_{2^k+1} \\ge  1 + \\frac{k}{2} + \\frac{1}{2^k+1} + \\frac{1}{2^k+2} + \\ldots + \\frac{1}{2^{k+1}}\n\t\t.\\] It is enough to show that \\( \\frac{1}{2^k+1} + \\frac{1}{2^k+2} + \\ldots + \\frac{1}{2^{k+1}} \\ge \\frac{1}{2} \\). Then, we have \\[\n\t\t\t\\frac{1}{2^k+1} + \\frac{1}{2^k+2} + \\ldots + \\frac{1}{2^{k+1}} \\ge \\frac{1}{2^{k+1}} + \\frac{1}{2^{k+1}} + \\ldots + \\frac{1}{2^{k+1}} = 2^k \\cdot  \\frac{1}{2\\cdot 2^k} = \\frac{1}{2} \n\t\t.\\] Combining these inequalities, we have \\[\n\t\t\tH_{2^{k+1}} \\ge 1 + \\frac{k}{2} + \\frac{1}{2^k+1} + \\ldots + \\frac{1}{2^{k+1}} \\ge 1 + \\frac{k}{2} + \\frac{1}{2} = 1 + \\frac{k+1}{2} \n\t\t\\] Therefore, \\( P(k+1) \\) is true.\n\t\\end{description}\n\\end{eg}\n",
        "lec_15.tex": "\\lecture{15}{Thu 19 Oct 2023 14:02}{Induction Continued}\n\n\\begin{eg}\n\tGiven \\( \\alpha  \\in \\mathbb{R} \\), \\( \\alpha  >  0\\), \\( \\alpha  \\neq  1 \\). Show that \\[\n\t\t1 + \\alpha  + \\alpha ^2 + \\ldots + \\alpha ^n = \\frac{1-\\alpha ^{n+1}}{1 - \\alpha }\n\t.\\] \n\\end{eg}\n\n\\begin{proof}\n\tWe will proceed with induction on \\( n \\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n = 0 \\). Then, we have \\[\n\t\t\t1 = \\alpha ^0 = \\frac{1-\\alpha ^{0+1}}{1-\\alpha } = \\frac{1-\\alpha }{1-\\alpha } = 1\n\t\t.\\] \n\t\t\\item[Step:] Assume that \\[\n\t\t\t1 + \\alpha  + \\alpha ^2 + \\ldots  + \\alpha ^k = \\frac{1-\\alpha ^{k+1}}{1-\\alpha } \\qquad \\text{for some } k \\ge 0\n\t\t.\\] From the assumption (inductive hypothesis), we have \n\t\t\\begin{align*}\n\t\t\t1 + \\alpha  + \\alpha ^2 + \\ldots  + a^{k+1} &= (1 + \\alpha  + \\alpha^2 + \\ldots  + \\alpha ^k) + \\alpha^{k+1} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= \\frac{1-\\alpha ^{k+1}}{1-\\alpha } + \\alpha ^{k+1} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= \\frac{1-\\alpha ^{k+1} + (1-\\alpha )\\alpha ^{k+1}}{1-\\alpha } \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= \\frac{1-\\alpha ^{k+1} + \\alpha ^{k+1} - \\alpha ^{k+2}}{1-\\alpha } \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= \\frac{1-\\alpha ^{(k+1) + 1}}{1-\\alpha }\n\t\t.\\end{align*}\n\t\twhich was what we wanted.\n\t\\end{description}\n\\end{proof}\n\n\\begin{eg}\n\tShow that for every \\( n \\in \\mathbb{N} \\), \\( n \\ge 1 \\), 21 divides \\( 4^{n+1} + 5^{2n-1} \\).\n\\end{eg}\n\n\\begin{proof}\n\tWe will proceed with induction on \\( n \\). Let \\( P(n) \\) be the statement that \\( 21 \\mid 4^{n+1} + 5^{2n-1} \\). We wish to prove that \\( P(n) \\) is true for every \\( n \\in \\mathbb{N} \\), \\( n \\ge 1 \\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n = 1 \\). Then, we have \\[\n\t\t\t\t4^1+5^{2 \\cdot 1 - 1} = 4^2 + 5 = 9 = 21\n\t\t.\\] \n\t\t\\item[Step:] We assume \\( P(k) \\) is true for \\( k \\ge 1 \\). We wish to show that \\( P(k+1) \\) is true as well. In other words, we wish to show that 21 divides \\[\n\t\t\t4^{(k+1)+1} + 5^{2(k+1)-1}\n\t\t.\\] We have:\n\t\t\\begin{align*}\n\t\t\t4^{(k+1)+1} + 5^{2(k+1)-1} &= 4 \\cdot 4^{k+1} + 5^2 \\cdot 5^{2k-1} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= 4 \\cdot 4^{k+1} + 25 \\cdot 5^{2k-1} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= 4 \\cdot 4^{k+1} + (21 + 4) \\cdot 5^{2k-1} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= 4 \\cdot 4^{k+1} + 21 \\cdot 5^{2k-1} + 4 \\cdot 5^{2k-1} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= 4 \\cdot \\left( 4^{k+1} + 5^{2k-1}\\right) + 21 \\cdot 5^{2k-1}\n\t\t.\\end{align*}\n\t\tThen, from our assumption, we have \n\t\t\\begin{align*}\n\t\t\t4^{(k+1)+1} + 5^{2(k+1)-1}  &= 4 \\cdot 21\\cdot q + 21 \\cdot 5^{2k-1}  \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= 21 \\cdot (4q + 5^{2k-1})\n\t\t.\\end{align*}\n\t\tBy definition, this means that 21 divides \\( 4^{(k+1)+1} + 5^{2(k+1)-1} \\), or \\( P(k+1) \\) is true, which was what we wanted.\n\t\\end{description}\n\\end{proof}\n\n\\begin{eg}\n\tGiven a complete set of triominoes, we want to tile an \\( n\\times n \\) board. \n\t\\begin{observe}\n\t\tThe board cannot be tiled if \\( n \\) is not divisible by 3, because \\( n^2 \\) must be divisible by 3 for the board to be tiled.\n\t\\end{observe}\n\tWe want to show that we can tile any \\( n \\times n \\) board with top left corner removed if \\( n \\) is a power of 2.\n\\end{eg}\n\n\\begin{proof}\n\tWe will proceed with induction on \\( n \\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n = 1 \\). It is clear that we can fit a triomino in the 3 squares of the board.\t\n\t\t\\item[Step:] \\( n > 1 \\). We assume it is possible to tile some \\( 2^k \\times 2^k \\) board with top left corner removed. We wish to show that it is possible to tile the \\( 2^{k+1} \\times 2^{k+1}   \\) board with top left corner removed as well. Here \\( k \\in \\mathbb{N} \\), \\( k \\ge 1 \\). By partitioning the board into 4 smaller squares, we can apply our assumption to tile the entire board (see figure below).\n\t\\end{description}\n\\end{proof}\n\n\\begin{figure}[H]\n    \\centering\n    \\incfig{triomino-tiling}\n    \\caption{Triomino Tiling}\n    \\label{fig:triomino-tiling}\n\\end{figure}\n\n\\begin{eg}\n\tWe have a group of  \\( N \\) people. One person is a celebrity if everybody knows that person \\textbf{and} that person knows no one.\n\\end{eg}\n\n\\exercise{1}\nIs \\( 4^n - 1 \\) a multiple of 3?\n",
        "lec_16.tex": "\\lecture{16}{Tue 24 Oct 2023 14:08}{Strong Induction}\n\nContinuing the example from last time, we want to see for any group if there exists a celebrity. To do so, we can ask any person \\( A \\) whether or not they know person \\( B \\). How many questions will we have to ask?\n\nLet \\( T(n) \\) be the minimum number of questions we must ask to determine if there is a celebrity. Note that if \\( A \\) knows \\( B \\), \\( A \\) cannot be the celebrity. Also note that if \\( A \\) doesn't know \\( B \\), then \\( B \\) cannot be the celebrity. Then, our recurrence relation is defined by \\[\n\tT(n) = 1 + T(n-1)\n.\\] We wish to show that we can find whether if there is a celebrity (or not) in at most \\( 3(n-1) \\) questions.\n\n\\begin{note}\n\tThe base case is \\( T(2) = 2 \\). This is because if \\( A \\) knows \\( B \\), you still need to check whether or not \\( B \\) knows \\( A \\).\n\\end{note}\n\n\\begin{proof}\n\tWe will proceed with induction. \n\t\\begin{description}\n\t\t\\item[Base case:] \\( n=2 \\). We will ask both people, which is sufficient to determine if there is a celebrity. We have \\( 2 \\le  3 = 3(2-1) \\), as desired.\n\t\t\\item[Step case:] Assume that for any group of \\( k \\) people, we can determine if there is a celebrity by asking at most \\( 3(k-1) \\) questions. We wish to show that we can determine the existence of a celebrity in a group of \\( k+1 \\) people in at most \\( 3(k+1-1) =3k \\) questions. Pick two people \\( A \\) and \\( B \\), and ask if \\( A \\) knows \\( B \\). Then, there are two cases:\n\t\t\t\\begin{description}\n\t\t\t\t\\item[Case 1:] \\( A  \\) does not know \\( B \\). Then, we know that \\( B \\) cannot be a celebrity. By the inductive hypothesis applied to the original group without \\( B \\), we can find whether or not there exists a (candidate) celebrity \\( C \\) in \\( 3(k-1) \\) steps. To confirm that \\( C \\) is indeed a celebrity in the group with \\( B \\), we must check that \\( B \\) knows \\( C \\), and \\( C \\) does not know \\( B \\). This yields \\[\n\t\t\t\t\t\t1 + 3(k-1) + 2 = 3(k+1-1) = 3k\n\t\t\t\t\t.\\] total questions, as desired.\n\t\t\t\t\\item[Case 2:] \\( A \\) knows \\( B \\). Left as an exercise to the reader!\n\t\t\t\\end{description}\n\t\\end{description}\n\tAs we have verified both the base and step case of induction, it follows that we can determine the existence of a celebrity in a group of \\( n \\) people in at most \\( 3(n-1) \\) moves.\n\\end{proof}\n\n\\subsection{Strong Induction}\nInstead of assuming that the previous case is true, we assume that all previous cases are true. In other words, we check that \\( P(0) \\) is true, and we check that \\( P(0) \\land P(1) \\land P(2) \\land \\ldots \\land P(k) \\implies P(k+1) \\) is true for all \\( k \\in \\mathbb{N} \\). If both are true, we can then similarly conclude that \\( P(k) \\) is true for all \\( k\\in \\mathbb{N} \\).\n\n\\begin{eg}\n\tShow that for all natural numbers \\( n\\ge 2 \\) have a decomposition into prime factors. \n\\end{eg}\n\n\\begin{proof}\n\tWe will proceed with strong induction on \\( n \\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n=2 \\) is prime, as desired.\n\t\t\\item[Step case:] Assume every natural number of up \\( k \\) can be decomposed into prime factors. We wish to show that \\( k+1 \\) can be decomposed into prime factors as well.\n\t\t\t\\begin{description}\n\t\t\t\t\\item[Case 1:] \\( k+1 \\) is prime. Then the decomposition is trivial.\n\t\t\t\t\\item[Case 2:] Otherwise, \\( k+1 \\) is composite. Then, there exists \\( a, b \\in \\mathbb{N} \\) such that \\( k+1 = ab \\), \\( 2\\le a,b\\le k \\). By the inductive hypothesis, \\( a \\) and \\( b \\) can be decomposed into prime factors. Then, \\( k+1 \\) can be decomposed into prime factors as well, as desired.\n\t\t\t\\end{description}\n\t\t\tIn both cases, \\( k+1 \\) can be decomposed into prime factors.\n\t\\end{description}\n\tAs we have verified both the base and step case of induction, it follows that for all \\( n\\in \\mathbb{N} \\), \\( n\\ge 2 \\), \\( n \\) can be decomposed into prime factors.\n\\end{proof}\n\n\\begin{note}\n\tFor strong induction, we must carefully choose our base case(s), as shown below.\n\\end{note}\n\n\\begin{eg}\n\t(Making change/coin change) There are 4 pesos bills and 5 pesos bills in an unknown country. What is the minimum value \\( \\zeta \\) such that one can make change for all \\( n \\ge \\zeta \\)?\n\\end{eg}\n\n\\begin{proof}\n\tWe wish to show that \\( \\zeta=12 \\). We will proceed with strong induction on \\( n \\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n=12=4+4+4 \\), \\( n=13=4+4+5 \\), \\( n=14=4+5+5 \\), \\( n=15=5+5+5 \\).\n\t\t\\item[Step case:] Assume one can make change for all values \\( [12, k] \\) for some natural number \\( k \\). We wish to show that we can make change for \\( k+1 \\). To make change for \\( k+1 \\), we can use the change for \\( k-3 \\) and add one 4 peso bill.\n\t\\end{description}\n\\end{proof}\n\n\\subsection{Induction and Recursion}\n\n\\begin{eg}\n\tLet \\( \\{a_n\\}_{n \\in \\mathbb{N}} \\) be a sequence of numbers such that \\( a_{0}=1, a_{1}=3, a_{2}=9, a_n=a_{n-1} + a_{n-2} + a_{n-3}\\) for all \\( n \\ge 3 \\). Prove that \\( a_n \\le 3^n \\).\n\\end{eg}\n\n\\begin{proof}\n\tWe will proceed with strong induction on \\( n \\).\n\t\\begin{description}\n\t\t\\item[Base case:] For \\( n=0 \\), we have \\( a_0=1=3^0 \\), as desired. For \\( n=1 \\), we have \\( a_1=3=3^1 \\), as desired. And for \\( n=2, a_2=9=3^2 \\), as desired.\n\t\t\\item[Step case:] Assume \\( a_k \\le 3^k \\) for all values up to \\( k \\in \\mathbb{N} \\). We wish to show that \\( a_{k+1}\\le 3^{k+1}  \\). We know that \n\t\t\t\\begin{align*}\n\t\t\t\ta_{k+1} &= a_k+a_{k-1}+a_{k-2} \\\\\n\t\t\t\t\t\t\t\t&\\le 3^k + 3^{k-1} + 3^{k-2} \\tag {Follows from inductive hypothesis} \\\\\n\t\t\t\t\t\t\t\t&< 3^k+3^k+3^k \\\\\n\t\t\t\t\t\t\t\t&= 3^{k+1}\n\t\t\t.\\end{align*}\n\t\t\tas desired.\n\t\\end{description}\n\tWe have verified the base and step of induction. It follows that \\( a_n \\le 3^n \\) for all \\( n \\in \\mathbb{N} \\).\n\\end{proof}\n",
        "lec_17.tex": "\\lecture{17}{Thu 26 Oct 2023 14:03}{Last of Induction}\n\n\\begin{eg}\n\tThe Fibonacci Sequence is defined as follows:\n\t\\begin{align*}\n\t\t&f_{0}=f_{1}=1 \\\\\n\t\t&f_n=f_{n-1}+f_{n-2} \\qquad \\forall n\\ge 2\n\t.\\end{align*}\n\tWe can do two things to compute the value of \\( f_n \\). If \\( n=0,1 \\), then it will output \\( 1 \\). Otherwise, we return \\( f_{n-1}+f_{n-2} \\). Note that we can also accomplish the following (more efficiently) with memoization.\n\n\tThen, let \\( T(n) \\) be the number of operations to get \\( f_n \\). Note that \\( T(n) \\ge  c+ T(n-1) + T(n-2) \\), which implies that the number of steps to calculate the \\( n \\)-th Fibonacci number is greater than the \\( n \\)-th Fibonacci number!\n\n\tShow that \\( f_n \\ge \\alpha ^{n-2}  \\), where \\( \\alpha =\\frac{1+\\sqrt{5} }{2} \\).\n\\end{eg}\n\n\\begin{proof}\n\tLet us proceed by induction. Note that for \\( n=0 \\), we have \\( f_{0}=1>\\alpha ^{0-1} \\) and for \\( n=1 \\), \\( f=1>\\alpha ^{1-2}  \\). Then, it suffices to verify the step case. Assume \\( f_k \\ge \\alpha ^{k-2}  \\) for some \\( k \\in \\mathbb{N} \\ge 0 \\). Then, we have:\n\t\\begin{align*}\n\t\tf_{k+1}&=f_k+f_{k-1} \\tag{Given} \\\\\n\t\t\t\t\t &\\ge \\alpha ^{k-2}+\\alpha ^{(k-1)-2}   \\tag{By I.H.} \\\\\n\t\t\t\t\t &=\\alpha ^{k-2} + \\alpha ^{k-3} \\\\\n\t\t\t\t\t &=\\alpha ^{k-3}(\\alpha +1) \\\\\n\t\t\t\t\t\t&=\\alpha ^{k-3}(\\alpha ^{2}) \\tag{\\(\\alpha ^{2}=\\alpha +1\\)} \\\\\n\t\t\t\t\t\t&=\\alpha ^{k-1} \\\\\n\t\t\t\t\t\t&=\\alpha ^{(k+1)-2} \n\t.\\end{align*}\n\tas desired. Therefore, \\( f_n \\ge \\alpha ^{n-2}  \\) for all \\( n \\in \\mathbb{N} \\).\n\\end{proof}\n\n\\begin{eg}\n\tConsider the following matrix \\( M = \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix}  \\). Note that \\( M\\begin{pmatrix} x \\\\ y \\end{pmatrix} =\\begin{pmatrix} x + y \\\\ x \\end{pmatrix}  \\).\n\n\tShow that for \\( f_{0}=0\\), \\(f_{1}=1 \\), and \\( f_n=f_{n-1}+f_{n-2} \\): \\[\n\t\tM^n = \\begin{pmatrix} f_{n+1} & f_n \\\\ f_n & f_{n-1} \\end{pmatrix} \n\t.\\] \n\\end{eg}\n\n\\begin{proof}\n\tWe have for \\( n=1 \\), \\( M^1 = \\begin{pmatrix} 1& 1 \\\\ 1& 0 \\end{pmatrix}  \\), and for \\( n=2 \\), \\( M^2=\\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}  \\). Assume that for some \\( k \\), \\[\n\t\tM^k=\\begin{pmatrix} f_{k+1} & f_k \\\\ f_k & f_{k-1} \\end{pmatrix} \n\t.\\] Then, we have \n\t\\begin{align*}\n\t\tM^{k+1} = M^k \\cdot M &= \\begin{pmatrix}f_{k+1} & f_k \\\\ f_k & f_{k-1} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t&= \\begin{pmatrix} f_{k+1}+f_k & f_{k+1} \\\\ f_k+f_{k-1} & f_k \\end{pmatrix} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t&= \\begin{pmatrix} f_{k+2} & f_{k+1} \\\\ f_{k+1} & f_k \\end{pmatrix} \n\t.\\end{align*}\n\tas desired.\n\\end{proof}\n\n\\begin{remark}\n\tCheck \\( f_{n-1}\\cdot f_{n+1}=f_n^2+(-1)^n \\)\n\\end{remark}\n\nThis is because\n\\begin{align*}\n\tf_{n-1}\\cdot f_{n+1} - f_n^2 &= \\det \\begin{pmatrix} f_{n+1} & f_n \\\\ f_n & f_{n-1} \\end{pmatrix} \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t &= \\det \\left( \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix}^n  \\right) \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= \\det \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix} ^n \\\\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t&= (-1)^n\n.\\end{align*}\n\n\\begin{eg}\n\tCount the number of bit-sequences of length \\( n \\) such that there are no two consecutive 1s.\n\\end{eg}\n\nLet \\( a_n \\) be the number of bit-sequences. Note that if we place a 0, we can fill the rest with \\( a_{n-1} \\) sequences. Note that if we place a 1, then the next bit must be a 0, such that we can place the rest with \\( a_{n-2} \\). In other words, the number of bit-sequences is given by \\[\n\ta_n = a_{n-1} + a_{n-2}\n.\\] Note that we have 2 bit-sequences of length 1 and 3 bit-sequences of length 2. We quickly realize that \\( a_n \\) is the \\( n+2 \\)-nd Fibonacci number.\n\n\\begin{eg}\n\tLet \\( s_n \\) be the number of bit-sequences such that there are no two 1s at distance 2.\n\\end{eg}\n\nWe have \\( s_3=6 \\). Note that if we place a 0, we can fill the rest with \\( s_{n-1} \\) sequences. Note that if we place a 1, then we can place two 0s such that we can fill the rest with \\( s_{n-3} \\) sequences. Or, we can place two 1s and then two 0s such that we can fill the rest with \\( s_{n-4} \\) sequences. In other words, our recurrence relation is \\[\n\ts_n = s_{n-1} + s_{n-3} + s_{n-4}\n.\\] \n\n\\begin{note}\n\tIf we define the empty string to be a bit-sequence, then \\( s_0=1 \\). This is ok as long as we don't use the empty string in our operation.\n\\end{note}\n",
        "lec_18.tex": "\\lecture{18}{Tue 31 Oct 2023 14:00}{Modular Arithmetic}\n\n\\section{Modular Arithmetic}\n\nConsider the relation \\( \\equiv_m \\) defined by the following: \\[\n\ta \\equiv_m b \\iff (a - b) \\text{ is a multiple of } m\n.\\] \n\n\\begin{note}\n\tThis is an equivalence relation because it is reflexive, symmetric, and transitive.\n\\end{note}\n\nWe can establish this relation because we know the following theorem (that defines a remainder):\n\n\\begin{theorem}\n\tLet \\( b,m \\in \\mathbb{Z},m>0 \\). There exists a unique pair of integers \\( q,r \\) such that \\( b = qm + r \\) and \\( 0 \\le r < m \\).\n\\end{theorem}\n\n\\begin{eg}\n\t13 = 2(5) + 3, such that \\( q=2,r=3 \\), so \\( 13 \\equiv_5 3 \\). \n\\end{eg}\n\n\\begin{definition}\n\t\\( \\mathbb{Z}_m = \\{\\overline{0}   ,\\overline{1} ,\\overline{2} ,\\ldots ,\\overline{m-1} \\}  \\) is the set of remainders when \\( m \\) is divided by a number.\n\\end{definition}\n\n\\begin{eg}\n\tLet \\( m=4 \\). \\( \\mathbb{Z}_4 = \\begin{Bmatrix} \\overline{0}&=&\\{0,\\pm 4,\\pm 8,\\pm 12,\\ldots \\} \\\\ \\overline{1}&=&\\{\\ldots ,-7,-3, 1,5,9,\\ldots \\} \\\\ \\overline{2}&=&\\{\\pm 2, \\pm 6, \\pm 10, \\ldots \\} \\\\ \\overline{3}&=&\\{\\ldots ,-5,-1,3,7,11,\\ldots \\}           \\end{Bmatrix}   \\) \n\\end{eg}\n\n\\begin{notation}\n\tWe write \\( b \\pmod m \\) to denote the remainder of \\( b \\) when divided by \\( m \\).\n\\end{notation}\n\n\\begin{notation}\n\tWe write \\( a \\equiv b \\pmod m \\) to denote that \\( a \\equiv_m b \\).\n\\end{notation}\n\n\\subsection{Operations in \\( \\mathbb{Z}_m \\)}\n\nHow do we perform operations?\n\n\\begin{definition}\n\tWe define \\textbf{addition} to be \\( \\overline{a}+\\overline{b} \\coloneq \\overline{a+b}     \\).\n\\end{definition}\n\n\\begin{eg}\n\t\\( \\overline{10}+\\overline{8}=\\overline{10+8}=\\overline{18}=\\overline{2}    \\) when \\( m=4 \\).\n\\end{eg}\n\n\\begin{definition}\n\tWe define \\textbf{multiplication} to be \\( \\overline{a}\\cdot \\overline{b}=\\overline{ab}    \\).\n\\end{definition}\n\n\\begin{eg}\n\t\\( \\overline{2}+\\overline{13}=\\overline{5} \\pmod {10}   \\) when \\( m=10 \\).\n\\end{eg}\n\n\\begin{note}\n\tIntegers are not closed under division, so we must be careful. \n\\end{note}\n\nAs we can see, the zero property and identity property in modular multiplication does not hold. Just like in linear algebra, we must define the inverse of a number in order to perform division.\n\n\\begin{definition}\n\tLet \\( m>1 \\). We say \\( a\\in \\mathbb{Z} \\) is a \\textbf{divisor of of zero} if \\( \\exists b \\in \\mathbb{Z} \\), \\( b \\not\\equiv 0 \\pmod m \\) such that \\( a \\cdot b \\equiv 0 \\pmod m \\). In other words, \\( \\exists \\overline{b}\\in \\mathbb{Z}_m,\\overline{b}\\neq \\overline{0}\\land \\overline{a}\\neq \\overline{0}  \\land \\overline{a}\\overline{b}=\\overline{0}       \\)\n\\end{definition}\n\n\\begin{definition}\n\tLet \\( a \\in \\mathbb{Z} \\). We say \\( a \\) is \\textbf{invertible} if \\( \\exists b \\in \\mathbb{Z} \\) such that \\( a \\cdot b \\equiv 1 \\pmod m \\).\n\\end{definition}\n\n\\begin{note}\n\tThese two properties are mutually exclusive.\n\\end{note}\n\n\\begin{definition}\n\tLet \\( a,b \\in \\mathbb{Z} \\). We define their greatest common divisor (\\( \\gcd \\)) of \\( a \\) and \\( b \\) as \\[\n\t\t\\gcd(a,b) = \\max\\{d \\in \\mathbb{Z} : d|a \\land d|b\\}  \n\t.\\] \n\\end{definition}\n\n\\begin{prop}\n\tLet \\( a>b>0 \\) be integers. Then, it holds that \\[\n\t\t\\gcd (a,b) = \\gcd (a-b,b)\n\t.\\]\n\\end{prop}\n\n\\begin{proof}\n\tLet \\( d=\\gcd(a,b) \\), \\( \\hat{d}=\\gcd(a,a-b) \\). Note that if \\( x|y \\) and \\( x|z \\), then \\( x|\\alpha y+\\beta z \\) for \\( x,y,z,\\alpha ,\\beta \\in \\mathbb{Z} \\). Then, we know that \\[\n\t\td|a \\land d|b \\text{ by definition of } \\gcd(a,b)\n\t.\\] Set \\( \\alpha =1,\\beta =-1 \\) to get \\( d|a-b \\) fron our note. Then, \\( d \\) is a common divisor of both \\( a \\) and \\( a-b \\). However, \\( \\hat{d}\\ge d \\) because \\( \\hat{d} = \\gcd(a,a-b) \\) (\\( \\hat{d} \\) is the \\emph{greatest} common divisor). We also know that \\[\n\t\t\\hat{d}|a \\land \\hat{d}|a-b \\text{ by definition of } \\gcd(a,a-b)\n\t.\\] Set \\( \\alpha =1,\\beta =-1 \\). then, we have \\[\n\t\t\\hat{d}|\\alpha a+\\beta (a-b) = a + (b-a) = b\n\t.\\] Now, we know that \\( \\hat{d} \\) is a common divisor of both \\( a \\) and \\( b \\). Then, \\( d \\ge \\hat{d} \\) because \\( d=\\gcd(a,b) \\). Because \\( \\ge   \\) is antisymmetric, we conclude that \\( d=\\hat{d} \\).\n\\end{proof}\n\n\\begin{algorithm}[H]\n\t\\caption{Euclidean Algorithm}\n\t\\KwIn{ $a \\ge b\\ge 0$ }\n\t\\KwOut{ $\\gcd(a,b)$ }\n\t\\If{b = 0}{\n\t\treturn \\( a \\)\\;\n\t}\n\treturn $\\gcd(b,a \\mod b)$\\;\n\\end{algorithm}\n\n\\begin{lemma}\n\tLet \\( d=\\gcd(a,m) > 1 \\). Then \\( a \\) is a divisor of zero \\( \\pmod m \\).\n\\end{lemma}\n\n\\begin{proof}\n\tLet \\( b=\\frac{m}{d} \\). Then, \\( a\\cdot b=a\\cdot \\frac{m}{d}=\\frac{a}{d}\\cdot m \\equiv 0 \\pmod m \\). The last equality follows from the fact that \\( d|a \\).\n\\end{proof}\n\n\\begin{eg}\n\tLet \\( a=2 \\), \\( m=10 \\). Then, \\( b=\\frac{10}{2}=5 \\), and \\( 2 \\cdot 5 \\equiv 0 \\pmod {10} \\).\n\\end{eg}\n\n\\begin{theorem}\n\tLet \\( a\\ge b \\), \\( a,b \\in \\mathbb{N} \\). Then, \\[\n\t\t\\gcd(a,b) = \\min \\{d > 0 : \\exists \\alpha ,\\beta ~ d = \\alpha a + \\beta b\\}  \n\t.\\] \n\\end{theorem}\n\nWe will prove this next lecture!\n\n\\begin{lemma}\n\tLet \\( 1=\\gcd(a,m) \\). Then, \\( \\exists ! b \\in \\mathbb{Z}_m \\) such that \\( \\overline{a}  \\cdot \\overline{b} = \\overline{1} \\). In other words, \\( a \\) is invertible \\( \\pmod m \\).\n\\end{lemma}\n\n\\begin{proof}\n\tFrom the theorem, we have \\( \\exists \\alpha ,\\beta : 1 = \\alpha a + \\beta m \\). If we take this expression \\( \\pmod m \\), we have \\( \\overline{1}=\\overline{\\alpha }\\cdot \\overline{a}+\\cancelto{0}{\\overline{\\beta }\\cdot \\overline{m}}      \\). So, \\( \\alpha  \\) is the inverse of \\( a \\) \\( \\pmod m \\)\n\\end{proof}\n\n\\begin{note}\n\tFrom these two lemmas, we can classify every integer \\( a \\) as invertible, or a divisor of 0.\n\\end{note}\n\n",
        "lec_19.tex": "\\lecture{19}{Thu 02 Nov 2023 14:10}{Modular Arithmetic Continued}\n\nHow do we solve \\( \\overline{a}\\overline{x}=\\overline{b}    \\) in \\( \\mathbb{Z}_m \\)?\n\nIt suffices to solve the cases where \\( \\overline{b}=\\overline{0}   \\) and \\( \\overline{b}=\\overline{1}   \\). We first check \\( \\gcd(a,m)=d \\).\n\n\\begin{eg}\n\tFind \\( \\gcd(30,12) \\).\n\\end{eg}\n\nBy the Euclidean algorithm, we have\n\\begin{align*}\n\t& \\gcd(30, 18) \\tag{\\( 30=18\\cdot 1 + 12 \\)} \\\\\n\t&= \\gcd(18, 12) \\tag{\\( 18=12\\cdot 1 + 6 \\)} \\\\\n\t&= \\gcd(12, 6) \\tag{\\( 12=6\\cdot 2 + 0 \\)} \\\\\n\t&= \\gcd(6,0) = 6\n.\\end{align*}\n\nThen, let us go back to the spot where we have a remainder of 6. Then, we get a linear combination \\[\n\t6 = 18 \\cdot 1 + 12 \\cdot (-1)\n.\\] We also get from one step above that: \\[\n\t6 = 18 \\cdot 1 + (30 \\cdot 1 + 18 \\cdot -2) \\cdot (-1) = 30 \\cdot (-1) + 18 \\cdot 3\n.\\] In other words, we have now found a method for finding the coefficients of the linear combination that represents the gcd in terms of the two numbers we started with. \n\n\\begin{definition}\t\n\tLet \\( d=\\gcd(a,b) \\). Then, there exists numbers \\( s,t \\in \\mathbb{Z} \\) such that \\( d=as+bt \\). These are known as \\textbf{Bezout coefficients}.\n\\end{definition}\n\nWe can use the Extended Euclidean Algorithm to find one such coefficient.\n\n\\begin{algorithm}\n\t\\caption{Extended Euclidean Algorithm}\n\t\\KwIn{\\( (a,b), a \\ge b \\)}\n\t\\KwOut{\\( (d,s,t) \\) such that \\( d=\\gcd(a,b) \\), \\( d=as+bt \\)}\n\t\\( r_0=a \\)\\;\n\t\\( r_1=b \\)\\;\n\t\\( s_0=1 \\)\\;\n\t\\( s_1=0 \\)\\;\n\t\\( t_0=0 \\)\\;\n\t\\( t_1=1 \\)\\;\n\t\\While{\\( r_k\\neq 0 \\)}{\n\t\t\\( r_{k+1}=r_{k-1} \\pmod {r_k} \\)\\;\n\t\t\\( s_{k+1}=s_{k-1} - (r_{k-1} \\text{ div } r_k)\\cdot s_k \\)\\;\n\t\t\\( t_{k+1}=t_{k-1} - (r_{k-1} \\text{ div } r_k) \\cdot t_k \\)\\;\n\t}\n\treturn \\( (r_{k-1}, s_{k-1}, t_{k-1}) \\)\\;\n\\end{algorithm}\n\n\\begin{remark}\n\tIf \\( d=1 \\), then \\( s \\) is the inverse of \\( a \\pmod b \\).\n\\end{remark}\n\nWe can also solve systems of linear equations in \\( \\mathbb{Z}_m \\). Let's say we wish to find a number such that we can satisfy \\[\n\t\\begin{pmatrix}\n\t\tx &\\equiv a_1 \\pmod {m_1} \\\\\n\t\tx &\\equiv a_2 \\pmod {m_2} \\\\\n\t\t\t& \\hdots \\\\\n\t\tx &\\equiv a_k \\pmod {m_k} \\\\\n\t\\end{pmatrix}\n.\\] \n\n\\begin{theorem}\n\t(Chinese Remainder Theorem) Let \\( m_{1},m_{2},\\ldots m_k \\in \\mathbb{N}\\) be numbers such that \\( \\gcd(m_i,m_j)=1 \\) for all \\( 1 \\le  i < j \\le  k \\). Then, the system has a unique solution in \\( \\mathbb{Z}_M \\) where \\( M=m_{1}m_{2}\\ldots m_k \\).\n\\end{theorem}\n\n\\begin{proof}\n\tFirst, we will prove the solution's existence. Set \\( M_i =\\frac{M}{m_i}\\). Because all \\( m \\)'s don't share a factor, \\( \\gcd(m_i, M_i) = 1\\). Let \\( y_i \\) be the inverse of \\( M_i \\pmod {m_i}\\). Consider: \\[\n\t\tx = M_1 \\cdot y_{1} \\cdot  a_{1} + M_{2} \\cdot y_{2}\\cdot a_{2}+ \\ldots + M_{k} \\cdot y_{k} \\cdot a_{k}\n\t.\\] Then, we check: \n\t\\begin{align*}\n\t\tx &\\equiv M_{1}\\cdot y_{1}\\cdot a_{1}+ \\cancelto{0}{M_{2}\\cdot y_{2}\\cdot a_{2}} + \\ldots  + \\cancelto{0}{M_k \\cdot  y_k \\cdot a_k} \\pmod {m_1} \\\\\n\t\t\t& \\equiv \\cancelto{1}{M_{1}\\cdot y_{1}}\\cdot a_{1} \\equiv a_{1} \\pmod {m_1}\n.\\end{align*}\n  We can reprove this for each \\( 2\\le i\\le k \\).\n\\end{proof}\n\n\\begin{eg}\n\tSolve \\[\n\t\t\\begin{matrix}\n\t\t\tx & \\equiv 3 \\pmod 5 \\\\\n\t\t\tx & \\equiv 2 \\pmod 7\n\t\t\\end{matrix}\n\t.\\] \n\\end{eg}\n\nWe first check that \\( \\gcd(5,7)=1 \\), which it is.\nThen, we continue as follows:\n\\begin{align*}\n\tM&=5\\cdot 7=35 \\\\\n\tM_1 &= \\frac{35}{5} = 7 \\\\\n\tM_2 &= \\frac{35}{7} = 5 \\\\\n\ty_{1} &= 3 \\tag{Inverse of 7 mod 5} \\\\\n\ty_{2} &= 3 \\tag{Inverse of 5 mod 7} \\\\\n.\\end{align*}\nTherefore, the solution is as follows: \n\\begin{align*} \n\tx &= M_{1} \\cdot  y_{1} \\cdot  a_{1} + M_{2} \\cdot  y_{2} \\cdot  a_{2} \\\\\n\t\t&= 7 \\cdot  3 \\cdot 3 + 5 \\cdot 3 \\cdot  2 = 93\\\\\n\t\t& \\equiv 23 \\pmod {35}\n.\\end{align*}\n\n\\begin{note}\n\tThis solution (\\( x \\equiv 93 \\pmod {35} \\)) is the only solution, which we will prove in our homework.\n\\end{note}\n",
        "lec_20.tex": "\\lecture{20}{Tue 07 Nov 2023 14:12}{What the Modular Arithmetic}\n\nFix \\( m \\in \\mathbb{N} \\), \\( m>1 \\).\n\nLet \\( a \\in \\mathbb{N} \\). Then, \\[\n\t\\{a^n\\}_{n\\ge 0} \\text{ in } \\mathbb{Z}_m=\\{\\overline{0},\\overline{1},\\overline{2},\\ldots ,\\overline{m-1}    \\}  \n.\\] \n\nNote that we can easily calculate \\( a^k \\) with \\[ a^k \\pmod m= \\left( a^{k-1} \\pmod m \\right) \\left( a \\pmod m \\right) \\pmod m .\\] as well as finding repetition. What do we mean by this?\n\n\\begin{eg}\n\tLet \\( m=6 \\), and \\( a=11 \\). Then, \\( a^n \\) has periodicity \\( 2 \\) when taken mod m.\n\\end{eg}\n\n\\begin{eg}\n\tWhat is \\( 4^{2023} \\pmod 7 \\)? Note that \\( 2023 \\equiv 1 \\pmod 3 \\) (from the periodicity), so \\( 4^{2023}\\equiv 4^1 \\equiv 4 \\pmod 7  \\).\n\\end{eg}\n\n\\begin{theorem}\n\tLet \\( a \\in \\mathbb{N} \\). \\( a \\) is divisible by 3 if and only if the sum of its digits is divisible by 3.\n\\end{theorem}\n\n\\begin{proof}\n\tAll of \\( \\{10^n\\}_{n\\ge 0}  \\) in \\( \\mathbb{Z}_3 \\) are in the class of \\( \\overline{1}  \\)! Therefore, \\( 10^k \\equiv 1 \\pmod 3 \\). Then, \\( a \\) can be written as \\[\n\t\ta = (a_k a_{k-1} a_{k-2}\\ldots a_{1}a_{0})_{10} \\qquad a_i \\in \\{0,1,2,\\ldots 9\\}  \n\t.\\] Then, we know that\n\t\\begin{align*}\n\t\ta &= 10^k a_k + \\ldots  + 10^2 a_2 + 10 a_1 + a_0 \\\\\n\t\t\t&\\equiv \\cancelto{1}{\\overline{10^k}}\\cdot \\overline{a_k} + \\ldots  + \\cancelto{1}{\\overline{10^2}}\\cdot \\overline{a_2}  + \\cancelto{1}{\\overline{10} }\\cdot \\overline{a_1} + a_{0} \\pmod 3 \\\\\n\t\t\t&= a_k + \\ldots  + a_{1} + a_{0} \\pmod 3\n\t.\\end{align*}\n\\end{proof}\n\n\\begin{theorem}\n\tLet \\( a \\in \\mathbb{N} \\). \\( a \\) is divisible by 4 if and only if the last two digits of \\( a \\) are divisible by 4.\n\\end{theorem}\n\n\\begin{proof}\n\tWe know that \\( \\{10^n\\} _{n\\ge 0}  \\) in \\( \\mathbb{Z}_{4} \\) are \\( \\overline{1}  \\) for \\( n=0 \\), \\( \\overline{2}  \\) for \\( n=1 \\), and \\( \\overline{0}  \\) for \\( n\\ge 2   \\). Therefore, we only look at \\( n=0 \\) and \\( n=1 \\) (tens and ones place).\n\\end{proof} \n\n\\begin{lemma}\n\tIf \\( 1 \\le i < j \\le p-1 \\). Then, \\( ai \\not\\equiv aj \\pmod p \\).\n\\end{lemma}\n\n\\begin{proof}\n\t\\( ai-aj = a(i-j) \\not\\equiv 0 \\pmod p \\). \n\\end{proof}\n\n\\begin{theorem}\n\t(Fermat) Let \\( p \\) be prime. Let \\( a \\in \\mathbb{N} \\) such that \\( \\gcd(a,p)=1\\). Then, \\( a^{p-1}\\equiv 1 \\pmod p \\).\n\\end{theorem}\n\n\\begin{proof}\n\tFix \\( p \\), prime number. Then we know for \\( a^n \\) in \\( \\mathbb{Z}_p \\), \\( a\\in \\mathbb{N} \\),  \\( \\gcd(a,p)=1 \\). Therefore, all of \\[\n\t\t1, a, a^2, a^3, \\ldots , a^{n}\n\t.\\] are invertible. Then all of \\[\n\t\t\\overline{1}, \\overline{2}, \\overline{3}, \\overline{p-1} \n\t.\\] and \\[\n\t\t\\overline{1a}, \\overline{2a}, \\overline{3a}, \\overline{(p-1)a} \n\t.\\] are invertible. Note that the previous two sets are the same sets. Then, we have\n\t\\begin{align*}\n\t\t\\overline{1}\\cdot \\overline{2}\\cdot \\overline{3}\\cdot \\ldots \\cdot \\overline{(p-1)} &\\equiv \\overline{1a}\\cdot \\overline{2a}\\cdot \\overline{3a}\\cdot \\overline{(p-1)a}     \\pmod p \\\\\n\t\t(p-1)! &\\equiv (p-1)! \\cdot a^{p-1}  \\pmod p \\\\\n\t\t\t\t\t\t1 &\\equiv a^{p-1} \\pmod p\n\t.\\end{align*}\n\\end{proof}\n\n\\begin{eg}\n\tWhat is \\( 4^{2023} \\pmod 7  \\)?\n\\end{eg}\n\nNote that 2023 is prime, and therefore \\( 4^{2022}\\equiv 1 \\pmod 7   \\). Then, \\( 4\\cdot 1 \\equiv 4 \\pmod 7 \\).\n\n\\begin{eg}\n\tWhat is \\( 4^{2023} \\pmod {131} \\)?\n\\end{eg}\n\nNote that \\( 2023 \\pmod {130} = 73 \\). Then, by Fermat's theorem, \\( 4^{2023} \\equiv 4^{73} \\pmod {131}   \\).\n\nGoing even deeper, What about counting the number of invertible elements?\n\\[\n\t\\left| \\{1 \\le  x \\le m \\colon \\gcd(x,m)=1\\}   \\right| = \\varphi(m)\n.\\] is the number of invertible elements in \\( \\mathbb{Z}_m \\). Then, \\( \\varphi(p)=p-1 \\). Note that \\( \\varphi(p^2)=p(p-1) \\).\n\n\\begin{theorem}\n\t\\( \\varphi(ab)=\\varphi(a) \\cdot  \\varphi(b) \\) as long as \\( \\gcd(a,b)=1 \\).\n\\end{theorem}\n\n\\begin{theorem}\n\t(Euler) Let \\( m \\in \\mathbb{N} \\). Let \\( p_{1},p_{2},\\ldots ,p_k \\) be the distinct prime factors of \\( m \\). Then, \\[\n\t\t\\varphi(m) = m \\cdot \\left( 1-\\frac{1}{p_{1}} \\right) \\left( 1- \\frac{1}{p_{2}} \\right)\\ldots  \\left( 1-\\frac{1}{p_k} \\right) \n\t.\\] \n\\end{theorem}\n\n\\begin{theorem}\n\t(Euler) Let \\( m \\in \\mathbb{N} \\), \\( m > 1 \\). Let \\( a \\in \\mathbb{N} \\) such that \\( \\gcd(a,m)=1 \\). Then \\[\n\t\ta^{\\varphi(m)} \\equiv 1 \\pmod m\n\t.\\] \n\\end{theorem}\n\n\\subsection{Cryptography}\n\nFrom Euler's theorem, we have \\( a^{\\varphi(m)+1}\\equiv a \\pmod m  \\).\n\nWe label our alphabet such that \\( A \\to 00 \\), \\( B \\to 01 \\), \\( Z \\to  25 \\), etc. Then, you can write every message as a sequence of digits. Let this message be \\( m \\). We release to the world public key \\( (N,e) \\).\n\nThe encryption key is \\( m^e \\pmod N \\). Then, the decryption key is then \\( m^{t\\cdot \\varphi(N)+1}\\equiv m \\pmod N  \\). the decryption strategy is that if we find \\( d \\in \\mathbb{N} \\) such that \\( ed \\equiv 1 \\pmod {\\varphi(n)} \\), then we have \\[\n\t\\left( m^e \\right) ^d = m^{e\\cdot d} = m^{t\\cdot \\varphi(n)+1} \\equiv m \\pmod n\n.\\] such that we can then decrypt the message.\n\n\\begin{note}\n\tFor the public key, we need \\( \\gcd(\\varphi(N), e)=1 \\).\n\\end{note}\n\n\\begin{remark}\n\tWe usually set \\( N=p\\cdot q \\), both prime. Then, \\( \\varphi(N) = (p-1)(q-1) \\). This is better than \\( N \\) prime because our enemies need to factorize \\( N \\) to find our \\( p \\) and \\( q \\). By the time they read our message, it is already too late.\n\\end{remark}\n\n\\begin{note}\n\tIf a message is longer than \\( N \\), we need to break it into pieces because when don't want some number congruent to the message, but the message itself.\n\\end{note}\n",
        "lec_21.tex": "\\lecture{21}{Thu 09 Nov 2023 14:11}{Counting}\n\n\\section{Counting}\n\nWe can use the product rule for counting (when counting happens sequentially).\n\n\\begin{eg}\n\tLet a license plate be defined by 3 letters followed by 3 digits. How many possible license plates are there?\n\\end{eg}\n\nWe can choose 26 for the first letter, 26 for the second, and 26 for the third. Then, we can choose 10 for the first digit, 10 for the next, and 10 for the last. Therefore, our answer is \\[\n\t26\\cdot 26\\cdot 26\\cdot 10\\cdot 10\\cdot 10=(260)^3\n.\\] \n\n\\begin{eg}\n\tLet \\( |A|=n \\), \\( |B|=m \\). How many functions are there such that \\( f:A\\to B \\)?\n\\end{eg}\n\nEvery element in \\( A \\) can be mapped to an element in \\( B \\), such that there are \\( m \\) choices for all \\( n \\) elements. Then, the total number of functions is \\( m^n\\).\n\n\\begin{eg}\n\tLet \\( |A|=n \\), \\( |B|=m \\). How many functions \\( f:A\\to B \\) are there such that \\( f \\) is onto?\n\\end{eg}\n\nNote that \\( a_{1} \\) must be mapped to an element in \\( B \\). There are \\( m \\) choices to do so. Then, \\( a_{2} \\) must be mapped to an element in \\( B \\), not equal to \\( f(a_{1}) \\). There are \\( m-1 \\) choices to do so. Then, \\( a_{3} \\) must be mapped to an element in \\( B \\), not equal to \\( f(a_{1}) \\) and \\( f(a_{2}) \\). There are \\( m-2 \\) choices to do so. It follows that the total number of functions is \\[\n\t\\begin{cases}\n\t\tm(m-1)(m-2)\\cdots(m-n+1)=(m)_n & m \\ge n\\\\\n\t\t0 & m < n\n\t\\end{cases}\n.\\] \n\n\\begin{notation}\n\t\\( (m)_n = \\frac{m!}{(m-n)!} = m(m-1)(m-2)\\ldots (m-n+1)\\) \n\\end{notation}\n\nWe can use the sum rule to add together cases in counting:\n\n\\begin{eg}\n\tCount the number of passwords consisting of letters and digits, with length 8-10.\n\\end{eg}\n\nWe can count passwords of length 8, 9, and 10 separately. \\[\n\t36^8+36^9+36^{10}\n.\\] \n\nWe can also use the subtraction rule! We can count the number of things that do not satisfy our condition, then subtract that number from the total number of things.\n\n\\begin{eg}\n\tCount the number of passwords with at least one digit.\n\\end{eg}\nNote that if we naively fix the position of the digit, make it a digit, and fill the rest of the letters (with 36 options), then we will be over-counting passwords!\n\nInstead, we count the total number of passwords with no constraints, and subtract the number of passwords with no digits, yielding solution \\[\n\t36^8+36^9+36^{10} - (26^8+26^9+26^{10})\n.\\] \n\n\\begin{notation}\n\tWe will define factorial (!) as \\[\n\t\tn! \\coloneq \\begin{cases}\n\t\t\t1 & n=0\\\\\n\t\t\tn(n-1)! & n>0\n\t\t\\end{cases}\n\t.\\]\n\\end{notation}\n\nThere is also a division rule, where we count every element exactly \\( N \\) times.\n\n\\begin{definition}\n\t\\textbf{Permutations} count the number of something where order matters! It is written as \\[\n\t\tP(n,k)=\\frac{n!}{(n-k)!}= \\text{\\# of ordered }k\\text{-tuples from } n \\text{ elements}\n\t.\\] \n\\end{definition}\n\nWe derive this formula from the division rule: if we have a permutation of length \\( n \\) of \\( n \\) elements, we are counting the number of permutations of length \\( k \\) of \\( n \\) elements \\( n-k \\) times!\n\n\\begin{definition}\n\t\\textbf{Combinations} count the number of something where order \\textit{order doesn't matter}. It is written as \\[\n\t\t\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\t.\\] \n\\end{definition}\n\nNote that permutations also counts this, but there are \\( k! \\) ways to order a combination. Therefore, we divide the number of permutations by \\( k! \\) to yield this amount.\n\nWe can also use recursion for counting sets. Let \\( 1,2,3,\\ldots ,n \\) be the elements we can choose from. We can add \\( k \\) to the number of sets of size \\( n-1 \\) without \\( k \\), of which there are \\( \\binom{n-1}{k-1} \\) of them. We can add \\( k \\) to the number of sets of size \\( n-1  \\) with \\( k \\), of which there are \\( \\binom{n-1}{k} \\) of them. Therefore, we have Pascal's identity: \\[\n\t\\binom{n}{k}=\\binom{n-1}{k-1}+\\binom{n-1}{k}\n.\\]\n",
        "lec_22.tex": "\\lecture{22}{Tue 14 Nov 2023 14:05}{Counting Continued}\n\n\\begin{eg}\n\tHow many strings of 10 bits are there such that there are exactly 4 1's?\n\\end{eg}\n\nWe can choose 4 places of 10 to place 1's, and the rest are 0's. So the answer is $\\binom{10}{4}$. \n\n\\begin{eg}\n\tHow many strings of 10 bits are there such that there are at most 4 1's?\n\\end{eg}\n\nWe can use the sum rule! \\[\n\t\\binom{10}{0} + \\binom{10}{1} + \\binom{10}{2} + \\binom{10}{3} + \\binom{10}{4} = 386\n.\\] \n\n\\begin{eg}\n\tShow that if \\( |S| = n \\), then \\( |\\mathcal{P}(S)| = 2^n \\).\n\\end{eg}\n\nA subset can be formed by choosing whether or not to include every element. Therefore, the number of subsets, the cardinality of the powerset, is \\( 2^{|S|} = 2^n \\).\n\n\\begin{theorem}\n\t\\[\n\t\t(a+b)^n = \\sum_{k=0}^n \\binom{n}{k} a^k b^{n-k}\n\t.\\] \n\\end{theorem}\n\n\\begin{proof}\n\tFor each term \\( (a+b)(a+b)\\ldots (a+b) \\), we can choose to either multiply the \\( a \\) or the \\( b \\). If there are \\( k \\) number of \\( a \\)'s, then there are \\( n-k \\) number of \\( b \\)'s. Therefore, the number of terms with \\( k \\) number of \\( a \\)'s is \\( \\binom{n}{k} \\). Therefore, the coefficient of \\( a^k b^{n-k} \\) is \\( \\binom{n}{k} \\). Summing over all \\( k \\) gives the result.\n\\end{proof}\n\n\\exercise{1}\nProve the binomial theorem with induction!\n"
    },
    "math-3012": {
        "lec_01.tex": "\\lecture{1}{Wed 04 Oct 2023 13:07}{Counting and Formulas}\n\n\\section{Intro to Combinatorics}\n\nWhat is combinatorics? It is related to discrete math (finite structures) - things we can count.\n\n\\subsection{Counting}\n\\begin{eg}\n\tCount the number of binary strings with length 10.\n\\end{eg}\n\nFor this problem, we can choose the characters in the string from \\( \\{0, 1\\}   \\). We make this choice 10 times. Therefore, there are \\( 2^{10}  \\) number of binary strings of length 10.\n\n\\begin{eg}\n\tCount the number of binary strings with length \\( n \\), such that there are no two consecutive ones. \n\\end{eg}\n\nThis problem is a little less straight forward. Let \\( F(n) \\) be the number of binary strings of length \\( n \\). To form a string of \\( n \\), we can:\n\\begin{itemize}\n\t\\item Choose 1 as our starting digit. Then, we must choose 0 as the next digit. Then, there are \\( F(n-2) \\) ways to choose the rest of the digits.\n\t\\item Choose 0 as our starting digit. Then, there are \\( F(n-1) \\) ways to choose the rest of the digits.\n\\end{itemize}\nThis problem has a recursive solution: \\( F(n) = F(n-1) + F(n-2) \\). We will learn more on how to find general formulas for these relations later.\n\n\\begin{note}\n\tNote that these are actually the fibonnaci numbers. There exists a general formula given by \\[\n\t\tF(n) = \\frac{1}{\\sqrt{5} }\\left( \\frac{1+\\sqrt{5} }{2} \\right)^{n+2} - \\frac{1}{\\sqrt{5} }\\left( \\frac{1-\\sqrt{5} }{2} \\right)^{n+2} \n\t.\\] \n\\end{note}\n\n\\begin{remark}\nThe right term approaches 0 as \\( n \\to \\infty \\). Therefore, this function's growth is exponential (\\( 1.6^n \\)). Sometimes, knowing how fast a function grows is more informative that knowing its specific equation.\n\\end{remark}\n\n\\subsection{Approximate Counting}\nSometimes, we cannot easily find a formula like this one to count things. And even if we do, it might not be very informative. \n\n\\begin{definition}\n\tA \\textbf{partition} of \\( n \\) is an expression of \\( n \\) as a sum of positive integers (where the order of the summands does not matter).\n\\end{definition}\n\n\\begin{eg}\n\tLet \\( P_n \\) be the number of partitions of a positive integer. How do you calculate \\( P_n \\)?\n\\end{eg}\n\nWell, we can calculate it by hand for smaller cases. We have:\n\n\\begin{align*}\n\tP_1 &= 1 \\\\\n\tP_2 &= 2 \\\\\n\tP_3 &= 3 \\\\\n\tP_4 &= 5 \\\\\n\tP_5 &= 7 \\\\\n.\\end{align*}\n\n\\begin{note}\nWe must be careful! It is easy to assume that \\( P_n = 8 \\) from our calculations. However, this is not the case.\n\\end{note}\n\nThere actually doesn't exist any known equation for this sequence. However, there exists a really handy estimation formula:\\[\n\tP_n \\approx \\frac{1}{4n\\sqrt{3} }e^{\\pi \\sqrt{\\frac{2n}{3} } }\n.\\] \nThis is what it means to approximately count. We don't know the exact value of \\( P_n \\) for all \\( n \\), but we are interested in how fast it grows, and a rough estimate of its actual value.\n\n\\subsection{Preface to Graphs}\nGraphs are commonly used to model real world problems.\n\\begin{definition}\n\tA \\textbf{graph} is a network of vertices with pairwise edges between them.\n\\end{definition}\n\n\\begin{definition}\n\tA graph is \\textbf{planar} if it can be drawn without edge-crossings.\n\\end{definition}\n\n\\exercise{1}\nIs the pentagonal graph planar?\n",
        "lec_02.tex": "\\lecture{2}{Wed 04 Oct 2023 13:07}{Intro to Graphs}\n\nContinuing on the idea of graphs: Graphs can be represented with a vertex set and an edge set.\n\n\\begin{figure}[ht]\n    \\centering\n    \\incfig{example-graph}\n    \\caption{Example Graph}\n    \\label{fig:example-graph}\n\\end{figure}\n\nHere, the vertex set is \\( V=\\{a, b, c, d, e, f, g\\}   \\), and the edge set is \\( E = \\{ab, ad, bd, fg, fe\\}   \\). In this graph, there are 7 vertices and 5 edges. However, real life applications have lots more vertices and lots more edges. \\par\n\nIn this class, we will only consider simple graphs:\n\n\\begin{definition}\n\tA \\textbf{simple} graph is a graph in which:\n\t\\begin{itemize}\n\t\t\\item A vertex cannot have an edge to itself.\n\t\t\\item Two vertices cannot have more than one edge between them.\n\t\\end{itemize}\n\\end{definition}\n\n\\begin{definition}\n\tIf there is an edge between vertices \\( u \\) and \\( v \\), we say that \\( u \\) and \\( v \\) are \\textbf{adjacent} or \\textbf{neighbors}.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{complete graph} \\( K_n \\) is a graph with \\( n \\) vertices, all of which are adjacent to each other.\n\\end{definition}\n\nWe know that the planar graph \\( K_5 \\) is not planar. But why is this? Well, it is implied by the four color theorem.\n\n\\subsubsection{Four Color Theorem}\nThe four color theorem states that:\n\\begin{theorem}\n\tIf \\( G \\) is a planar graph, then it is possible to color the vertices of \\( G \\) using at most 4 colors such that adjacent vertices are colored differently.\n\\end{theorem}\n\n\\begin{figure}[ht]\n    \\centering\n    \\incfig{coloring-of-a-graph}\n    \\caption{Coloring of a graph}\n    \\label{fig:coloring-of-a-graph}\n\\end{figure}\n\n\\begin{note}\n\tWe cannot color this graph with only 3 colors, because it contains the complete graph \\( K_4 \\).\n\\end{note}\n\nThe four color theorem was conjectured in 1852 by Gunthrie. It was experimentally observed when counting a map of the counties in England. Gunthrie observed that 4 colors were enough. \\par\n\nThe four color theorem was proven in 1879 by Kempe, then in 1880 by Tait. However, errors were found in their proofs in 1890 and 1891 respectively. Finally, it was solved in 1976 by Appel and Haken.\n\n\\begin{note}\n\tThis was the first example of a significant mathematical problem in which a solution was found by a computer.\n\\end{note}\n\nWhy was the four color theorem so hard to prove? Because it says something about \\textit{every} planar graph.\n\n\\subsubsection{Ramsey's Numbers}\n\nHere are some definitions that we will need for Ramsey's theorem:\n\n\\begin{definition}\n\tA \\textbf{clique} in a graph is a set of vertices that are all adjacent to each other.\n\\end{definition}\n\n\\begin{definition}\n\tAn \\textbf{independent set} in a graph is a set of vertices that are not adjacent to each other.\n\\end{definition}\n\n\\begin{theorem}\n\tFor any \\( k \\), there exists \\( N \\in  \\mathbb{N} \\) such that every graph with at least \\( N \\) vertices has a clique or an independent set of size \\( k \\) (or both).\n\\end{theorem}\n\nThis \\( N \\) is known as the \\( k \\)th Ramsey number and is denoted \\( R(k) \\). It is the smallest number \\( N \\) that satisfies the theorem. Let us compute some values of \\( R(k) \\):\n\\begin{itemize}\n\t\\item \\( R(2)=2 \\).\n\t\t\\begin{proof}\n\t\t\tWe only need the two vertices, which are either part of the same clique, or part of the same independent set.\n\t\t\\end{proof}\n\t\\item \\( R(3)=6 \\). To prove this, we need to show that 5 does not work, but 6 does.\n\t\t\\begin{proof}\n\t\t\t\\( R(3)>5 \\) because there exists a graph of size 5 in which there is no clique and no independent set of size 3. This is the \"pentagonal\" graph. \\par\n\t\t\t\\( R(3) \\le 6 \\). Let \\( G \\) be a graph such that \\( |V(G)| \\ge 6 \\). Pick a vertex \\( v \\in  V(G) \\). \\( v \\) must be adjacent to or not adjacent to at least 5 other vertices. \\par\n\t\t\tLet \\( A \\) be the set of vertices \\( v \\) is adjacent to, and \\( B \\) the set of vertices \\( v \\) is not adjacent to. Note that because \\( |A| + |B| \\ge 5 \\), at least one of \\( |A| \\) or \\( |B| \\) is greater than or equal to 3. \\par\n\t\t\tAssume \\( |A| \\ge 3 \\). Then, if \\( A \\) is an independent set, we have found an independent set of at least size \\( 3 \\). Otherwise, if \\( A \\) is not an independent set, then at least two vertices in \\( A \\) must be adjacent. Therefore, those two vertices and \\( v \\) form a clique of size at least 3. The case where \\( |B| \\ge 3 \\) can be proven similarly.\n\t\t\\end{proof}\n\t\\item \\( R(4) = 18 \\).\n\t\t\\begin{note}\n\t\t\tThere was a study in Budapest which found that in a group of 18 teenagers, there was either a group of 4 that were all friends, or a group of 4 that were not friends. This was not a discovery in psychology!\n\t\t\\end{note}\n\t\\item \\( R(5) = ~? \\). The 5th Ramsey number is an open problem. All we know is that \\( 43 \\le  R(5) \\le 48 \\). This illustrates an example of a \\textit{small number} problem that computers cannot solve.\n\\end{itemize}\n",
        "lec_03.tex": "\\lecture{3}{Wed 04 Oct 2023 13:08}{Intro to Sets}\n\n\\section{Intro to Sets}\n\nWhat exactly are sets?\n\n\\begin{definition}\n\tA \\textbf{set} is a collection of unordered, unique, elements.\n\\end{definition}\n\n\\begin{notation}\n\tWe say that \\( x \\in  X \\) when \\( x \\) is an element/member of the set \\( X \\).\n\\end{notation}\n\n\\begin{definition}\n\tThe \\textbf{Principle of Extensionality} states that if two sets have the same elements, then they are equal.\n\\end{definition}\n\n\\begin{note}\n\tOrder does not matter! Only whether or not the element is in the set.\n\\end{note}\n\n\\begin{eg}\n\t\\( \\{a, b, c\\} = \\{a, c, b\\} =\\{a, b, a, b, c\\}     \\)\n\\end{eg}\n\nWhat are some well known infinite sets?\n\\begin{itemize}\n\t\\item \\( \\mathbb{N} \\) is the set of all natural numbers (including 0 in this class).\n\t\\item \\( \\mathbb{Z} \\) is the set of all integers.\n\t\\item \\( \\mathbb{Q} \\) is the set of all rational numbers.\n\t\\item \\( \\mathbb{R} \\) is the set of all real numbers.\n\\end{itemize}\n\n\\begin{notation}\n\tIf we say that \\( n \\in \\mathbb{N} \\), that means that \\( n \\) is a natural number.\n\\end{notation}\n\n\\begin{definition}\n\tA set with no elements is called the empty set, denoted by \\( \\O \\).\n\\end{definition}\n\n\\begin{note}\n\t\\( \\{\\O\\} \\neq \\O  \\)! The set \\( \\{\\O\\}   \\) has one element: the empty set!\n\\end{note}\n\n\\begin{notation}\n\tWe can write use set builder notation to write \\( \\{0, 2, 4, 6, \\ldots \\}   \\) as \\( \\{n ~|~ n \\in \\mathbb{N}, n \\text{ is even}\\} \\).\n\\end{notation}\n\n\\begin{definition}\n\tWe say a set \\( A \\) is a subset of a set \\( B \\) (\\( A \\subseteq B\\)) if every element in \\( A \\) belongs to \\( B \\).\n\\end{definition}\n\n\\begin{eg}\n\t\\( \\mathbb{N} \\subseteq \\mathbb{Z} \\), \\( \\{a, c\\} \\subseteq \\{a, b, c\\}    \\).\n\\end{eg}\n\n\\begin{property}\n\tThe empty set \\( \\O \\) is a subset of every set.\n\\end{property}\n\n\\begin{note}\n\tThe elements of a set's elements are not their own elements! Be careful when there are sets within sets.\n\\end{note}\n\n\\subsection{Set Operations}\nLet \\( A,B \\) be sets. There are 5 key operations on sets:\n\\begin{itemize}\n\t\\item The \\textbf{union} of a set \\( A \\cup B \\) is defined by \\( \\{x ~|~ x \\in A \\lor x \\in B\\}   \\).\n\t\\item The \\textbf{intersection} of a set \\( A \\cap B \\) is defined by \\( \\{x ~|~ x \\in A \\land x \\in B \\}   \\).\n\t\\item The \\textbf{difference} of a set \\( A \\setminus B \\) is defined by \\( \\{x ~|~ x \\in A \\land x \\not\\in B \\}   \\).\n\t\\item The \\textbf{symmetric diffference} of a set \\( A \\triangle B \\) is defined by \\( (A \\setminus B) \\cup (B \\setminus A) \\).\n\t\\item The \\textbf{cartesian product} \\( A \\times B \\) is defined by \\( \\{(a,b) ~|~ a \\in A \\land b \\in B\\}  \\)\n\t\t\\begin{note}\n\t\t\t\\( (a,b) \\) is the ordered pair with the first element \\( a \\) and second element \\( b \\). \\( (a,b) \\neq  (b,a) \\) unless \\( a = b \\).\n\t\t\\end{note}\n\\end{itemize}\n\n\\begin{eg}\n\t\\[ \\{1,2\\} \\times \\{a,b\\} \\times \\{x,y\\} = \\{ (1, a, x), (1, a, y), (2, a, x), (2, a, y), (1, b, x), (1, b, y), (2, b, x), (2, b, y) \\} .\\]\n\\end{eg}\n\nMore generally, given set \\( A_{1}, A_{2}, A_{3}, \\ldots, A_k \\), their cartesian product is the set of all ordered tuples \\( (a_{1}, a_{2}, a_{3}, \\ldots , a_k) \\) where \\( a_{1} \\in A_1, a_{2} \\in A_2, \\ldots , a_k \\in A_k \\).\n\n\\begin{notation}\n\t\\( A^k = \\underbrace{A \\times A \\times  A \\times \\ldots \\times A}_{k \\text{ times}}\\)\n\\end{notation}\n\n\\exercise{1}\n\\( \\{0,1\\}^3 = \\{?\\}   \\), \\( \\O \\times A = ~?\\)\n",
        "lec_04.tex": "\\lecture{4}{Wed 04 Oct 2023 13:08}{Rules of Counting; Permutations}\n",
        "lec_05.tex": "\\lecture{5}{Wed 04 Oct 2023 13:08}{Combinations; Formulaic vs Combinatoric Proofs}\n",
        "lec_06.tex": "\\lecture{6}{Wed 04 Oct 2023 13:09}{Binomial Theorem}\n",
        "lec_07.tex": "\\lecture{7}{Wed 04 Oct 2023 13:09}{Multichoose; Lattice Paths}\n",
        "lec_08.tex": "\\lecture{8}{Wed 04 Oct 2023 13:10}{Catalan Numbers}\n",
        "lec_09.tex": "\\lecture{9}{Wed 04 Oct 2023 13:10}{Multinomial Theorem}\n",
        "lec_10.tex": "\\lecture{10}{Wed 04 Oct 2023 13:10}{Recursion and Induction}\n",
        "lec_11.tex": "\\lecture{11}{Wed 04 Oct 2023 13:10}{Mathematical Induction Continued}\n",
        "lec_12.tex": "\\lecture{12}{Wed 04 Oct 2023 13:10}{Strong/Complete Induction}\n",
        "lec_13.tex": "\\lecture{13}{Wed 04 Oct 2023 13:10}{Bounds on Binomial Coefficients}\n",
        "lec_14.tex": "\\lecture{14}{Wed 04 Oct 2023 13:12}{Bounds Continued}\n",
        "lec_15.tex": "\\lecture{15}{Wed 04 Oct 2023 13:13}{Stirling's Approximation; Graph Theory}\n\n\\section{Intro to Graph Theory}\n",
        "lec_16.tex": "\\lecture{16}{Wed 04 Oct 2023 13:13}{Handshake Theorem; Order of Summation}\n",
        "lec_17.tex": "\\lecture{17}{Wed 04 Oct 2023 13:13}{Isomorphic Graphs; Walks and Paths}\n",
        "lec_18.tex": "\\lecture{18}{Wed 04 Oct 2023 13:58}{Intro to Trees}\n\nWe will continue on the idea of graphs from last lecture.\n\\begin{definition}\n\tA graph \\( G \\) is \\textbf{connected} if \\( V(G) \\neq \\varnothing \\) and for all \\( u,v\\in V(G) \\), there is a \\( uv \\)-walk in \\( G \\).\n\\end{definition}\n\n\\begin{figure}[ht]\n    \\centering\n\t\t\\incfig[0.8]{a-disconnected-graph}\n    \\caption{Examples of Graphs}\n    \\label{fig:a-disconnected-graph}\n\\end{figure}\n\nIn general, any graph can be partitioned into (connected) components (connected induced subgraphs with no edges between them).\n\n\\begin{note}\n\tA graph is connected if and only if it has one component.\n\\end{note}\n\n\\begin{definition}\n\tA \\textbf{cycle} in a graph \\( G \\) is a walk \\( (x_{0}, x_{1},\\ldots, x_{L}) \\) such that:\n\t\\begin{itemize}\n\t\t\\item \\( x_{0} = x_L \\)\n\t\t\\item \\( x_{0}, x_{1}, \\ldots, x_{L - 1} \\) are distinct\n\t\t\\item \\( L \\geq 3 \\)\n\t\\end{itemize}\n\\end{definition}\n\n\\begin{definition}\n\tA cycle of length 3 is called a \\textbf{triangle}.\n\\end{definition}\n\n\\begin{definition}\n\tA graph is \\textbf{acyclic} if it has no cycles.\n\\end{definition}\n\n\\begin{definition}\n\tA connected acyclic graph is called a \\textbf{tree}.\n\\end{definition}\n\n\\begin{definition}\n\tAcyclic graphs are also called \\textbf{forests}.\n\\end{definition}\n\\begin{remark}\n\tBecause all components in an acyclic graph are acyclic, and connected acyclic graphs are trees!\n\\end{remark}\n\n\\begin{definition}\n\tA \\textbf{leaf} in a tree is a vertex of degree 1.\n\\end{definition}\n\\begin{remark}\n\tLeaves are useful because deleting leaves from a tree results in a smaller tree.\n\\end{remark}\n\n\\begin{problem}\n\tLet \\( T \\) be a tree, \\( v \\in  V(T) \\) a leaf. Then \\( T-v \\coloneqq \\) the graph obtained from \\( T \\) by removing \\( v \\) and its incident edge is also a tree. Why?\n\\end{problem}\n\n\\begin{prop}\n\tIf \\( T \\) is a tree with \\( n \\ge 2 \\) vertices, then it has \\( \\ge 2 \\) leaves.\n\\end{prop}\n\\begin{proof}\n\tConsider a longest path! Let \\( T \\) be a tree with \\( n\\ge 2 \\) vertices. Let \\( (x_{0}, x_{1}, \\ldots , x_L) \\) be \\underbar{a} path in \\( T \\) of max length.\n\t\\begin{note}\n\t\t\\( 1 \\le L \\le n - 1 \\), because we only have \\( n \\) vertices available to us, and a tree with at least two vertices has at least 1 edge.\n\t\\end{note}\n\tWe claim that \\( x_L \\) is a leaf in \\( T \\). We will prove this by contradiction: suppose \\( x_L \\) is not a leaf. Then, \\( \\deg_T(x_L) \\ge 2 \\), so \\( x_L \\) has to have a neighbor \\( y \\) that is different from \\( x_{L-1} \\).\n\t\\begin{note}\n\t\t\\( y \\) is also different from \\( x_{0}, x_{1}, \\ldots , x_{L - 2} \\) because there are no cycles in \\( T \\).\n\t\\end{note}\n\tTherefore, \\( (x_{0}, x_{1}, \\ldots, x_L, y) \\) is a path in \\( T \\) of length \\( L + 1 > L \\) which is impossible \\contra. It follows that \\( x_L \\) is a leaf, as claimed. \\par\n\tBy a similar argument, \\( x_{0} \\) is also a leaf.\n\\end{proof}\n\n\\begin{theorem}\n\tIf \\( T \\) is a tree with \\( n \\) vertices, then it has exactly \\( n-1 \\) edges.\n\\end{theorem}\n\\begin{proof}\n\tProof by induction on \\( n \\). \\par\n\t\\begin{description}\n\t\t\\item[Base:]  n = 1. Then, \\( T \\) has 1 vertex and 0 edges. \\( 1 - 1 = 0 \\), so the theorem holds.\n\t\t\\item[Step:] We wish to prove for some \\( n \\ge 1 \\), every tree with \\( n \\) vertices has \\( n - 1 \\) edges. Let \\( T \\) be a tree with \\( n + 1 \\) vertices. We want to show that \\( T \\) has \\( n \\) edges.\n\t\t\t\\begin{note}\n\t\t\t\t\\( T \\) has \\( n + 1 \\ge 1 + 1 = 2 \\) vertices, so \\( T \\) has a leaf, denoted \\( v \\in  V(T) \\).\n\t\t\t\\end{note}\n\t\t\tLet \\( T' \\coloneqq T - v \\). Then \\( T' \\) is a tree with \\( n \\) vertices. \\( |E(T')| = |E(T)| - 1 \\). And by our inductive hypothesis, \\( T' \\) has \\( n - 1 \\) edges. Therefore, \\( |E(T)| = n \\), as desired.\n\t\\end{description}\n\\end{proof}\n\n\\begin{property}\n\tEvery connected graph \\( G \\) has a spanning tree (a spanning subgraph that is a tree).\n\\end{property}\n\\exercise{1}\nHow many spanning trees does a complete graph \\( K_n \\) have?\n",
        "lec_19.tex": "\\lecture{19}{Fri 06 Oct 2023 14:01}{Spanning Trees; Eulerian and Hamiltonian Graphs}\n\nWhy does every connected graph have a spanning tree?\n\\begin{proof}\n\tIf \\( G \\) is a tree, then \\( G \\) itself is the spanning tree. If \\( G \\) is not a tree, then it contains one or more cycles. It can be shown that deleting edges from any cycle removes the cycle, but maintains connectivity of the graph.\n\\end{proof}\n\nThere is another proof using extremal configurations:\n\\begin{proof}\n\tLet \\( T \\) be a connected spanning subgraph of \\( G \\) with the fewest edges.\n\t\\begin{note}\n\t\t\\( G \\) itself is a connected spanning subgraph of \\( G \\). Therefore, there must exist \\( T \\) with the fewest edges.\n\t\\end{note}\n\t\\begin{remark}\n\t\tThis argument assumes \\( G \\) is finite (even though the fact holds true for infinitely connected graphs as well).\n\t\\end{remark}\n\tWe claim that \\( T \\) is a tree (as desired). We will proceed with proof by contradiction. Suppose that \\( T \\) is not a tree. Then, \\( T \\) has a cycle \\( (x_{0}, x_{1}, \\ldots , x_l = x_{0}) = C\\). Let \\( T' \\) be the graph obtained from \\( T \\) by removing one of the edges of the cycle. The graph \\( T' \\) is connected as well. \\par\n\tFor any \\( u,v \\in V(G) \\), then, since \\( T \\) is connected, there is a \\( uv \\)-walk in \\( T \\). Then, by replacing the removed edge in this walk by the other edges of \\( C \\), there still remains a \\( uv \\)-walk in \\( T' \\). However, this is impossible as \\( |E(T')| < |E(T)| \\) \\contra. We conclude that \\( T \\) is a spanning tree.\n\\end{proof}\n\n\\exercise{1}\nLet \\( F \\) be a spanning forest in \\( G \\) with the most edges. Show that \\( F \\) is a tree.\n\n\\begin{corollary}\n\tA connected graph with \\( n \\) vertices has at least \\( n-1 \\) edges.\n\\end{corollary}\n\n\\subsection{Eulerian and Hamiltonian Graphs}\n\nLet \\( G \\) be a connected graph.\n\\begin{definition}\n\tA closed walk in a graph \\( G \\) is a walk that starts and ends at the same vertex (e.g. a cycle).\n\\end{definition}\n\n\\begin{definition}\n\tAn \\textbf{Euler circuit} in \\( G \\) is a closed walk that uses every edge exactly once.\n\\end{definition}\n\n\\begin{definition}\n\t\\( G \\) is \\textbf{Eulerian} if it has an Euler circuit.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{Hamiltonian cycle} in \\( G \\) is a cycle that uses every vertex exactly once.\n\\end{definition}\n\n\\begin{definition}\n\t\\( G \\) is \\textbf{Hamiltonian} if it has a Hamiltonian cycle.\n\\end{definition}\n\n\\begin{figure}[ht]\n    \\centering\n    \\incfig{example-graphs}\n    \\caption{Eulerian and Hamiltonian Graph}\n    \\label{fig:example-graphs}\n\\end{figure}\n\nThis graph is Eulerian because it conains an Euler circuit: \\( (1, 2, 4, 5, 2, 3, 5, 6, 3, 1, 6, 4, 1) \\). This graph is also Hamiltonian because it contains a Hamiltonian Cycle: \\( (1, 4, 5, 6, 3, 2, 1) \\).\n\n\\begin{definition}\n\tA graph is \\textbf{even} if every vertex has even degree.\n\\end{definition}\n\n\\begin{observe}\n\tFor a graph to be Eulerian, it must be even. This is because an Euler circuit enters and leaves each vertex the same number of times.\n\\end{observe}\n\n\\begin{definition}\n\tA \\textbf{trail} is a walk that uses each edge at most once.\n\\end{definition}\n\n\\begin{lemma}\n\tIf \\( G \\) is an even graph, and \\( v \\in  V(G) \\) is a vertex of degree greater than 0, then there is a closed trail of positive length in \\( G \\) starting and ending at \\( v \\).\n\\end{lemma}\n\n\\begin{proof}\n\tLet \\( T = (v=x_{0}, x_{1}, x_{2}, \\ldots , x_L)\\) be a trail starting at \\( v \\) of maximum length.\n\t\\begin{note}\n\t\t\\( L \\ge 1 \\) because \\( \\deg_G(v)>0 \\).\n\t\\end{note}\n\tWe want to argue that \\( T \\) is closed (\\( x_L=v \\)). Suppose that this is not the case. Then, the trail \\( T \\) enters \\( x_L \\) \\( k \\) times and leaves it \\( k-1 \\) times, for some \\( k\\ge 1 \\). In total, \\( T \\) uses \\( k + (k-1)=2k-1 \\) edges incident to \\( x_L \\). But \\( \\deg_G(x_L) \\) must be even, so there is an unused edge, say \\( x_Ly \\) incident to \\( x_L \\) \\contra. This is impossible because \\( v=x_{0}, x_{1}, \\ldots , x_L, y \\) would be a longer trail starting at \\( v \\).\n\\end{proof}\n\n\\begin{theorem}\n\tA connected even graph is Eulerian (Euler).\n\\end{theorem}\n\n\\begin{proof}\n\tNext time!\n\\end{proof}\n\n\\begin{note}\n\tMathematicians like these theorems: ``obvious necessary condition is sufficient.''\n\\end{note}\n",
        "lec_20.tex": "\\lecture{20}{Wed 11 Oct 2023 14:03}{Euler's Theorem}\n\nContinuing with the proof of Euler's Theorem:\n\n\\begin{proof}\n\tLet \\( G \\) be a conneced even graph. Let \\( T = (x_{0}, x_{1}, x_{2}, \\ldots , x_L = x_{0})\\) be a closed trail in \\( G \\) of maximum length. We want to show that \\( T \\) is an Euler circuit. Assume, for the sake of contradiction, that \\( T \\) is not an Euler circuit. Then some edges are not used in \\( T \\). Let \\( U \\) be the set of all unused edges. Note that\n\t\\[\n\t\tU = E(G) \\setminus \\{x_{0}x_{1}, x_{1}x_{2}, \\ldots , x_{L-1}x_L\\} \\neq \\varnothing\n\t.\\] \n\tLet \\( X \\coloneq \\{x_{0}, x_{1}, x_{2}, \\ldots , x_{L-1}\\}   \\) be the vertices used in \\( T \\) and \\( Y \\coloneqq V(G) \\setminus X \\) be the unused vertices. Note that every edge used by \\( T \\) has both endpoints in \\( X \\). We claim that there is an edge in \\( U \\) incident to a vertex in \\( X \\).\n\t\\begin{description}\n\t\t\\item[Case 1:]\\( Y = \\varnothing \\). In this case, every vertex is in \\( X \\). Therefore, every edge in \\( U \\) is incident to two vertices in \\( X \\).\n\t\t\\item[Case 2:] \\( Y \\neq \\varnothing \\). In this case, because \\( G \\) is connected, there must be at least one edge that connects a vertex in \\( X \\) to a vertex in \\( Y \\). This edge is incident to a vertex in \\( X \\), and is in \\( U \\) (as all edges not in \\( U \\) are only incident to vertices in \\( X \\)).\n\t\\end{description}\n\tIn both cases, we can find an edge in \\( U \\) incident to two vertices in \\( X \\), which was what we wanted. \\par\n\tSo, let \\( x_i \\in X \\) be a vertex incident to an edge in \\( U \\). Let \\( G' \\) be the spanning subgraph of \\( G \\) with edge set \\( U \\) (we only keep the unused edges). Note that \\( \\deg_{G'}(x_i) > 0 \\) because \\( x_i \\) is incident to an edge in \\( U \\). Also note that \\( G' \\) is an even graph (exercise). \\par\n\tThen, by the lemma, there exists a closed trail \\[\n\t\t(x_i=z_{0}, z_{1}, z_{2}, \\ldots , z_k = x_i)\n\t\\]  in \\( G' \\) starting and ending at \\( x_i \\) of length \\( k > 0 \\). Note that this closed trail only uses edges in \\( U \\). But then there would exist closed trail \\[\n\t(x_{0}, x_{1}, \\ldots , \\underbrace{x_i = z_{0}, z_{1}, \\ldots , z_k = x_i}_{\\text{only added unused edges}}, x_{i+1}, \\ldots , x_{L-1}, x_L = x_{0})\n\t.\\] in \\( G \\) of length \\( L + k > L \\), which is impossible \\contra. Therefore, our assumpion is false, and \\( T \\) is an Euler circuit.\n\\end{proof}\n\nNote that this proof actually gives you an easy way of finding an Euler circuit in a connected, even graph. \\par\n\nUnfortunately, there is no similar, simple way to tell if a graph has a Hamilton cycle.\n\n\\begin{definition}\n\tThe \\textbf{minimum degree} of \\( G \\), denoted by \\( \\delta(G) \\), is the minimum of the degrees of the vertices of \\( G \\).\n\\end{definition}\n\n\n\\begin{property}\n\tLet \\( G \\) be a graph with \\( n \\ge 3 \\) vertices.\n\t\\begin{itemize}\n\t\t\\item If \\( \\delta(G) \\ge n - 1 \\), then \\( G \\) has a Hamilton cycle (it is a complete graph).\n\t\t\\item If \\( n \\) is even, then we can find \\( G \\) with \\( \\delta(G) = \\frac{n}{2} - 1 \\) and no Hamiltonian cycle.\n\t\t\\item If \\( n \\) is odd, then we can find \\( G \\) with \\( \\delta(G) = \\frac{n-1}{2} \\) and no Hamiltonian cycle (exercise).\n\t\\end{itemize}\n\\end{property}\n\n\\begin{theorem}\n\tIf \\( G \\) is a graph with \\( n \\ge 3 \\) vertices and \\( \\delta(G) \\ge \\frac{n}{2} \\), then \\( G \\) is Hamiltonian (Dirac).\n\\end{theorem}\n\\begin{proof}\n\tTheorem 5.18 in the book (clever use of longest paths).\n\\end{proof}\n\n\\exercise{1}\nWrite a computer program that finds euler circuits in connected even graphs.\n\\exercise{2}\nIf \\( G \\) is an even graph, then for any set \\( X \\subseteq V(G) \\), the number of edges joining \\( X \\) to \\( V(G)\\setminus X \\) is even.\n",
        "lec_21.tex": "\\lecture{21}{Fri 13 Oct 2023 14:03}{Graph Coloring}\n\n\\subsection{Graph Coloring}\n\nAnother property of graphs.\n\n\\begin{definition}\n\tA proper \\( k \\)\\textbf{-coloring} of a graph \\( G \\) is an assignment of labels (\"colors\") from an \\( k \\)-element set to the vertices of \\( G \\) such that adjacent vertices are assigned different labels.\n\\end{definition}\n\n\\begin{definition}\n\tThe \\textbf{chromatic number} of \\( G \\), denoted \\( \\chi(G) \\), is the minimum \\( k \\) such that \\( G \\) has a proper \\( k \\)-coloring.\n\\end{definition}\n\n\\begin{note}\n\tIf \\( G \\) has \\( n \\) vertices, then \\( \\chi(G) \\le n \\).\n\\end{note}\n\n\\begin{notation}\n\t\\( C_n \\) denotes the \\( n \\)-cycle.\n\\end{notation}\n\n\\begin{eg}\n\t\\( \\chi(C_n) = 2\\) if \\( n \\) is even, and \\( \\chi(C_n) = 3 \\) if \\( n \\) is odd.\n\\end{eg}\n\n\\begin{eg}\n\t\\( \\chi(\\text{tree}) = 2 \\) if there are at least two vertices.\n\\end{eg}\n\n\\begin{note}\n\tIn a proper coloring, vertices of the same color form an independent set. In other words, \\( \\chi(G) \\) is the minimum \\( k \\) such that it is possible to partition \\( V(G) \\) into \\( k \\) independent sets.\n\\end{note}\n\nWhy is coloring useful?\n\\begin{itemize}\n\t\\item It's fun. \n\t\\item Practical uses, e.g. scheduling problems and register allocation (in compilers), radio bandwidth allocation, etc.\n\t\\item It is a useful auxiliary tool for other problems, e.g. an algorithm that may process one independent set in a graph at a time.\n\t\\item It can capture in a single number some complex structural information about a graph.\n\\end{itemize}\n\n\\begin{eg}\n\tYou are trying to assign a set of jobs \\( J_{1}, J_2, \\ldots , J_n \\) into time slots, where some jobs conflict with each other and can't be assigned to the same time slot (if they use the same equipment). Define a graph \\( G \\colon V(G) = \\{J_{1}, J_{2},\\ldots , J_n \\}   \\) where edges are inserted between every conflicting job. Then, we know that every valid time slot assignment is a proper coloring of \\( G \\). Note that \\( \\chi (G) \\) is the minimum number of time slots required to complete all jobs.\n\\end{eg}\n\n\\begin{definition}\n\t\\( G \\) is \\textbf{bipartite} if \\( \\chi(G) \\le 2 \\).\n\\end{definition}\n\n\\begin{theorem}\n\t\\( G \\) is bipartite if and only if \\( G \\) has no odd cycles.\n\\end{theorem}\n\nIf \\( \\chi(G) \\ge 3 \\) is because there are cycles of odd length, then what makes \\( \\chi (G) \\) large?\n\n\\begin{definition}\n\t\\( \\omega(G) \\), the \\textbf{clique number} of \\( G \\), is the maxmimum size of a clique in \\( G \\).\n\\end{definition}\n\n\\begin{property}\n\t\\( \\chi(G) \\ge \\omega (G) \\)\n\\end{property}\n\n\\begin{note}\n\tWe can also have \\( \\chi(G) > \\omega (G) \\): \\( \\chi (C_5) = 3\\), but \\( \\omega (C_5) = 2\\) (works for any cycle of odd length).\n\\end{note}\n\n\\begin{definition}\n\tA graph \\( G \\) is \\textbf{triangle-free} if there are no cliques of size 3 (which look like triangles) in \\( G \\).\n\\end{definition}\n\n\\begin{theorem}\n\tFor any \\( k \\in \\mathbb{N} \\), there is a graph \\( G \\) such that \\( \\chi (G) \\ge k \\) and \\( \\omega (G) = 2 \\).\n\\end{theorem}\n\nThere are many proofs for this theorem. We will use the Blanche Descartes construction.\n\n\\begin{note}\n\tBlanche Descartes is the pen name of 4 undergrads at Cambridge in 1935. One of them was W. T. Tutte, who went on to become a founder of modern discrete math. He was also a codebreaker in World War 2.\n\\end{note}\n\n\\begin{proof}\n\tOur plan is to start with a triangle-free graph \\( G \\) with \\( \\chi(G) = k \\), and construct a triangle-free graph \\( BD(G) \\) with \\( \\chi (BG(G))= k+1 \\). One way we could do this is by adding a vertex adjacent to every vertex in \\( G \\). However, a problem occurs: adding this vertex creates lots of triangles. \\par\n\tInstead, we can connect all vertices in \\( G \\) to separate other vertices, where all of those vertices need to have the same color. How do we do this? We use many copies of \\( G \\). \\par\n\tLet \\( n \\coloneq |V(G)| \\), and \\( k \\coloneq \\chi (G) \\). Define \\( N \\coloneq k\\cdot (n - 1) + 1 \\). Define \\( r \\coloneq \\binom{N}{n} \\). Take a set of vertices \\( X \\) of size \\( N \\). List all \\( n \\)-element subsets of \\( X \\) as \\( S_{1}, S_{2}, \\ldots , S_r \\). Let \\( G_{1}, G_{2}, \\ldots , G_r \\) be copies of \\( G \\), disjoint from each other and from \\( X \\). Note that \\( |V(G_i)| = n = |S_i| \\) such that we can connect vertices in \\( G_i \\) to \\( S_i \\) by \\( n \\) disjoint edges. The resulting graph is \\( BD(G) \\).\n\\end{proof}\n\n\\exercise{1}\nWhat is \\( BD(K_2) \\)?\n",
        "lec_22.tex": "\\lecture{22}{Mon 16 Oct 2023 13:58}{Graph Coloring Continued}\n\nBlanche Descartes construction continued:\n\n\\begin{eg}\n\t\\( BD(K_2) = C_9\\)\n\\end{eg}\n\n\\begin{eg}\n\t\\( BD(C_9) \\)? Note that to calculate this graph, we would need to make \\( \\binom{25}{9} = 2042975 \\) copies of \\( C_9 \\)! This graph is very large, but is triangle-free with \\( \\chi(BD(C_9)) = 4 \\).\n\\end{eg}\n\nHow do we know that the resulting graph is both triangle-free with greater chromatic number?\n\n\\begin{lemma}\n\t\\( \\chi(BD(G)) \\ge  k+1 \\)\n\\end{lemma}\n\\begin{proof}\n\tSuppose not and consider a proper \\( k \\)-coloring. We have \\( |X| = N = k(n-1) + 1 \\), which means that some color must be used on at least \\( n \\) vertices in \\( x \\). In other words, there is some \\( n \\)-element set \\( S_i \\le  X \\), all of whose members are colored the same, say with color \\( c \\). \\par\n\tThen, every vertex in \\( G_i \\) cannot be colored with \\( c \\), so \\( G_i \\) is colored with only \\( k - 1 \\) colors. This is impossible as \\( \\chi(G_i) = \\chi(G) = k \\) \\contra.\n\\end{proof}\n\n\\begin{lemma}\n\tIf \\( G \\) is triangle-free, then so is \\( BD(G) \\).\n\\end{lemma}\n\\begin{proof}\n\tA triangle in \\( BD(G) \\) must use some vertex \\( x \\in X \\), as all copies of \\( G \\) are triangle free. In other words, \\( x \\) must be connected to two other vertices, of which are neighbors. However, as all copies \\( G_i \\) of \\( G \\) are disjoint, and \\( x \\) is connected by construction to different \\( G_i \\), such a triangle cannot exist, and \\( BD(G) \\) is triangle-free.\n\\end{proof}\n\nIn conclusion, by repeatedly applying the operation \\( BD \\) to, say \\( K_2 \\), we can construct triangle-free graphs with arbitrarily large chromatic number.\n\n\\begin{theorem}\n\tFor all \\( k,L \\in \\mathbb{N}\\), there is a graph \\( G \\) with \\( \\chi(G) \\ge k \\) and cycles of length at most \\( L \\) (Erdo\\\"s).\n\\end{theorem}\n\n\\begin{note}\n\tBecause Erdo\\\"s has published so many papers, there is an Erdo\\\"s number, which is a distance of collaboration to Erdo\\\"s himself.\n\\end{note}\n\nWhat are some other reasons for finding large \\( \\chi \\)?\n\n\\begin{definition}\n\t\\( \\alpha(G) \\) denotes the independence number of \\( G \\), the max size of an independent set in \\( G \\).\n\\end{definition}\n\n\\begin{property}\n\tIf \\( G \\) has \\( n \\ge 1 \\) vertices, then \\( \\chi (G) \\ge \\frac{n}{\\alpha (G)} \\)\n\\end{property}\n\nWhy is this? Say \\( \\chi(G) = k \\). This means \\( V(G) \\) can be partitioned into \\( k \\) independent sets. The size of these independent sets is then at most \\( \\alpha(G) \\). Therefore, \\( n \\le k \\cdot \\alpha (G) \\), which then means \\( k \\ge  \\frac{n}{\\alpha (G)} \\).\n\nThis is an exceptional situation!\n\n\\begin{notation}\n\tFix some small constant \\( \\epsilon >0 \\). Write \\( a \\approx b \\) if \\( 1 - \\epsilon  \\le  \\frac{a}{b} \\le  1 + \\epsilon  \\). \n\\end{notation}\n\n\\begin{theorem}\n\tConsider all graphs with vertex set \\( [n] \\) (there are \\( 2^{\\binom{n}{2}} \\) of them). If \\( n \\) is large enough, then \\( \\approx 100\\% \\) of these graphs satisfy \\[\n\t\t\\omega \\approx 2\\log_2(n), \\alpha \\approx 2\\log_2(n), \\chi \\approx \\frac{n}{2\\log_2(n)} \\approx \\frac{n}{\\alpha }\n\t.\\] \n\\end{theorem}\n\nThis theorem is studied in an area called random graph theory. Essentially, we can ``guess'' such properties of graphs without running expensive calculations to find them.\n\nNext lecture, we will talk about upper bounds on \\( \\chi  \\) in terms of vertex degrees.\n\n\\exercise{1}\nShow that \\( \\chi(BD(G)) = k + 1 \\)\n\\exercise{2}\nShow that if \\( G \\) has no cycles of length 3, 4, or 5, then neither does \\( BD(G) \\). Conclude that for all \\( k \\), there is a graph with \\( \\chi(G) \\ge  k \\), and no 3, 4, 5 cycles.\n\n",
        "lec_23.tex": "\\lecture{23}{Wed 18 Oct 2023 14:00}{Chromatic Numbers and Maximum Degrees}\n\nHow is the chromatic number of a graph \\( G \\) related to the maximum degree of any vertex in \\( G \\)?\n\n\\begin{definition}\n\t\\( \\Delta (G) \\) refers to the \\textbf{maximum degree} of any vertex in \\( G \\).\n\\end{definition}\n\n\\begin{prop}\n\t\\( \\chi(G) \\le \\Delta(G) + 1 \\).\n\\end{prop}\n\n\\begin{proof}\n\tWe can use the greedy coloring algorithm. We color any vertex with the least available color in any order. For any vertex \\( v \\), we forbid at most \\( \\deg(v)\t \\) colors. In other words, if we have \\( \\Delta(G) + 1 \\) colors to use, there will always be one available color to color the vertex \\( v \\).\n\\end{proof}\n\n\\begin{note}\n\tThis is not necessarily a tight upper bound on \\( \\chi(G) \\)!\n\\end{note}\n\n\\begin{theorem}\n\t(Brooks) Let \\( G \\) be a connected graph. Then \\( \\chi(G) = \\Delta(G) + 1 \\) can only happen if \\( G \\) is complete, or an odd cycle.\n\\end{theorem}\n\nWe must ask ourselves again, when is this bound tight? If \\( G \\) is not complete or odd cycle, then when is \\( \\chi(G) = \\Delta(G) \\)?\n\nTake the graph consisting of five triangles in the shape of a pentagon, vertices of which are adjacent to all vertices in the neighboring triangles. This is a graph with \\( \\Delta = 8 \\), \\( \\alpha =2 \\), \\( \\chi \\ge \\frac{n}{\\alpha } = \\frac{15}{2} = 7.5 \\). In other words, we have \\( \\chi \\ge 8 \\). From Brooks' theorem, we have \\( \\chi \\le 8 \\). Therefore, \\( \\chi =8 \\).\n\n\\begin{conjecture}\n\t(Borodin-Kostochka) If \\( \\Delta(G) \\ge 9 \\), and \\( G \\) is not a complete graph, then \\( \\chi(G) \\le  \\triangle(G) - 1 \\). \n\\end{conjecture}\n\n\\begin{note}\n\tThis conjecture has been proved true for \\( \\Delta(G) \\ge 10^{14} \\)\n\\end{note}\n\n\\subsection{Planar Graphs}\n\nRemember that a tree is planar if it can be drawn in the plane without edges crossing.\n\n\\begin{eg}\n\tAll trees are planar. \\( K_4 \\) is planar. \\( K_5 \\) is not planar. All cycles are planar.\n\\end{eg}\n\nHow would one prove that a certain graph is not planar?\n\n\\begin{theorem}\n\t(Euler) For any connected planar graph, we can count the number of regions the graph separates the plane into. Let \\( n \\) be the number of vertices, and \\( m \\) be the number of edges in such a graph. Let \\( f \\) be the number of regions the graph separates the plane into. Then, we have \\( n - m + f = 2\\).\n\\end{theorem}\n",
        "lec_24.tex": "\\lecture{24}{Fri 20 Oct 2023 14:01}{Planar Graphs}\n\n\\begin{note}\n\tIf we forget this formula, we can reconstruct by examining small graphs.\n\\end{note}\n\n\\begin{intuition}\n\tWhen you add an edge to a connected planar graph without adding new vertices, the number of faces will go up by one. Similarly, if you add an edge without increasing the number of faces, then we must add one new vertex. Either way, \\( n-m+f \\) is constant.\n\\end{intuition}\n\n\\begin{proof}\n\tWe will proceed with induction on \\( m \\), the number of vertices in the connected planar graph.\n\t\\begin{description}\n\t\t\\item[Base case:] \\( m=0 \\). The only connected graph with 0 edges is an isolated vertex \\( K_1 \\) (assuming \\( \\varnothing \\) is not a graph). \\( K_1 \\) has \\( n = 1 \\) vertices, \\( f=1 \\) faces, and \\( m=0 \\) edges such that \\( n-m+f = 1-0+1 = 2 \\), as desired.\n\t\t\\item[Step:] \\( m\\ge 1\\). Suppose that for some for value of \\( m\\ge 1 \\), this statement holds for all connected planar graphs with \\( m-1 \\) edges. Now, consider a drawing of a connected planar graph \\( G \\) with \\( m \\) edges, \\( n \\) vertices, and \\( f \\) faces. We wish to show that \\( n-m+f=2 \\). We will break this into cases: \\par\n\t\t\t\\begin{description}\n\t\t\t\t\\item[Case 1:] \\( G \\) is a tree. Note that for all trees, \\( f = 1 \\) and \\( m = n - 1 \\). So, \\( n-m+f=(m+1)-m+1=2 \\), as desired.\n\t\t\t\t\\item[Case 2:] \\( G \\) is not a tree. Because \\( G \\) is connected, then \\( G \\) must contain a cycle \\( C \\). Note that every face in \\( G \\) lies either inside \\( C \\) or outside \\( C \\) (Jordan Curve Theorem). Let \\( e \\) be an edge on the cycle \\( C \\). Let \\( G' \\) be the graph obtained from \\( G \\) by deleting \\( e \\). Note that because \\( e \\) is in a cycle in \\( G \\), \\( G' \\) remains connected. Note that \\( |V(G')| = n \\), \\( |E(G')|=m-1 \\). It remains to find an expression for the number of faces in \\( G' \\). \\par\n\t\t\t\t\tSince \\( e \\) is on a cycle \\( C \\) in \\( G \\), deleting \\( e \\) merges two faces into one (Jordan Curve Theorem). Therefore, \\( G' \\) has \\( f-1 \\) faces!. By the inductive hypothesis, \\( n-(m-1)+(f-1)=2 \\). Therefore, \\( n-m+f=2 \\), as desired.\n\t\t\t\\end{description}\n\t\\end{description}\n\tBecause we have verified the base and step of induction, \\( n-m+f=2 \\) holds for all graphs with \\( m \\in \\mathbb{N} \\).\n\\end{proof}\n\n\\begin{corollary}\n\tIf \\( G \\) is a connected planar graph with \\( m\\ge 3 \\) edges and \\( n \\) vertices, then \\( m \\le 3n-6 \\).\n\\end{corollary}\n\n\\begin{proof}\n\tWe will proceed with a double-counting argument. If \\( m\\ge 3 \\), then every face is bounded by \\( \\ge 3 \\) edges. Also, every edge is on the boundary of \\( \\le 2 \\) faces. Then, we have \\( 2m \\ge 3f \\implies f \\le \\frac{2}{3}m \\). By Euler's, \\( 2 = n-m+f\\le n-m+\\frac{2}{3}m = n - \\frac{1}{3}m\\). Rearranging, we have \\( m \\le 3n - 6 \\).\n\\end{proof}\n\nThis corollary gives us a certificate that certain graphs are not planar. In other words, if \\( G \\) has too many edges relative to the number of vertices, then \\( m \\le 3n-6 \\) will not be satisfied.\n\n\\begin{eg}\n\t\\( K_5 \\) has \\( n=5 \\) and \\( m=10 \\). Then, \\( m > 3n-6 \\), so \\( K_5 \\) is not planar.\n\\end{eg}\n\n\\begin{note}\n\tThe corollary does \\textbf{NOT} say ``if \\( m \\le  3n - 6 \\), then \\( G \\) is planar''.\n\\end{note}\n\n\\begin{eg}\n\tAdding a very long trail to \\( K_5 \\) will satisfy \\( m\\le 3n-6 \\). But because the graph contains \\( K_5 \\), it is not planar.\n\\end{eg}\n\n\\exercise{1}\nShow that the corollary from Euler's formula holds for \\( n\\ge 3 \\) as well.\n\n\\exercise{2}\n\\( K_{3,3} \\) is the complete bipartite graph with 3 vertices in each part of the bipartition. Show that \\( K_{3,3} \\) is not planar. Hint: use the fact that \\( K_{3,3} \\) contains no triangles.\n",
        "lec_25.tex": "\\lecture{25}{Mon 23 Oct 2023 14:02}{Planar Graphs Continued}\n\nThere exists an upgraded version of Euler's formula for disconnected planar graphs:\n\n\\begin{theorem}\n\tLet \\( G \\) be a planar graph with \\( n \\) vertices, \\( m \\) edges, \\( f \\) faces, and \\( c \\) connected components. Then \\( n - m + f - c= 1 \\).\n\\end{theorem}\n\n\\begin{corollary}\n\tEvery non-empty planar graph has \\emph{a} vertex of degree at most 5.\n\\end{corollary}\n\n\\begin{proof}\n\tAssume without loss of generality that \\( G \\) is connected. Let \\( n \\) be the number of vertices in \\( G \\), and \\( m \\) be the number of edges. Assume, for the sake of contradiction, that there is no vertex with degree at most 5. That is, every vertex has degree at least 6. Then, we have \\( m \\ge \\frac{6}{2}n = 3n \\) from the handshake lemma. However, we have that \\( m \\le 3n-6 \\) (from the above corollary). This is a contradiction: therefore our assumption is false, and there must be a vertex with degree at most 5.\n\\end{proof}\n\n\\begin{corollary}\n\tIf \\( G \\) is a planar graph, then \\( \\chi(G) \\le 6 \\).\n\\end{corollary}\n\n\\begin{proof}\n\tWe will proceed with induction on the number of vertices of \\( G \\). Let \\( n \\) be the number of vertices of \\( G \\).\n\t\\begin{description}\n\t\t\\item[Base case] \\( n=5 \\). Then, we can color \\( G \\) in 5 colors.\n\t\t\\item[Step case] Assume \\( \\chi(G') \\le 6 \\) for every graph \\( G' \\) with at most \\( n -1  \\) vertices. By the corollary, \\( G \\) has a vertex \\( v \\) of degree at most 5. By the inductive hypothesis, \\( G' = G - v \\) has \\( \\chi(G') \\le 6 \\). Every proper coloring of \\( G' \\) with at most 6 colors can be extended to a proper coloring of \\( G \\) using at most 6 colors (we add \\( v \\) to \\( G' \\), which is prohibited from at most 5 colors). Therefore, \\( \\chi(G) \\le 6 \\), as desired.\n\t\\end{description}\n\tAs we have verified the base and step of induction, this corollary holds true for all \\( n\\ge 5 \\).\n\\end{proof}\n\n\\begin{theorem}\n\t(Appel-Haken) The Four Color theorem states that \\( \\chi(G) \\le 4 \\) for any planar graph \\( G \\).\n\\end{theorem}\n\n\\begin{note}\n\tThe proof of the Four Color theorem applies a strengthened version of Corollary 3 and a similar method of removing vertices.\n\\end{note}\n\n\\subsubsection{Graph Minors}\n\nWhat are graph minors?\n\n\\begin{definition}\n\t\\( G \\) \"contract\" \\( e \\), denoted \\( G / e \\), is the graph obtained from \\( G \\) by deleting \\( e \\) and contracting the two vertices of \\( e \\) into a single vertex.\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{minor} of a graph \\( G \\) is a graph that can be obtained from a subgraph of \\( G \\) by a sequence of contractions.\n\\end{definition}\n\n\\begin{observe}\n\tEvery minor of a planar graph is planar.\n\\end{observe}\n\n\\begin{eg}\n\tIn the Peterson graph, we can contract the 5 edges that connect to the star to the pentagon to get \\( K_5 \\) as a minor. This also shows that the Peterson graph is not planar.\n\\end{eg}\n\n\\begin{theorem}\n\t(Kuratowski-Wagner) A graph is planar if and only if it has no minor isomorphic to \\( K_5 \\) or \\( K_{3,3} \\).\n\\end{theorem}\n\n\\begin{conjecture}\n\t(Hadwiger) If \\( G \\) has no minor isomorphic to \\( K_t \\), then \\( \\chi(G) \\le t-1 \\).\n\\end{conjecture}\n\n\\begin{note}\n\tThis conjecture is known for all \\( t \\) at most 5. It known for \\( t=6 \\), proved by Robertson, Seymour, and Thomas, and it is 80 addition pages beyond the proof for the Four Color Theorem.\n\\end{note}\n",
        "lec_26.tex": "\\lecture{26}{Wed 25 Oct 2023 14:02}{Exam 2 Review}\n\n\\begin{eg}\n\tIs every Hamiltonian graph connected?\n\\end{eg}\n\nYes. In order to find a Hamiltonian cycle, the graph must be connected.\n\n\\begin{eg}\n\tIs every connected graph Hamiltonian?\n\\end{eg}\n\nNo. Take the graph that is a square with an edge coming off one of the corners, for example.\n\n\\begin{eg}\n\tHow many 3-colorings does the \\( n \\)-cycle have?\n\\end{eg}\n\nLet \\( F(n) \\) be the number of proper colorings of a cycle of length \\( n \\) with colors 1, 2, 3. We wish to find \\( F(10) \\). Note that \\( F(n) = 3\\cdot 2^{n-1}\\) minus the number of colorings of an \\( n \\)-vertex path where the first and last vertex are colored the same, denoted \\( X(n) \\). The key observation is that \\( X(n) = F(n-1) \\). It follows that \\( F(n) = 3\\cdot 2^{n-1}-F(n-1)  \\).\n\n\\begin{eg}\n\tIs \\( K_{3,3} \\) with the bottom edge removed planar?\n\\end{eg}\n\nYes. We can rearrange it to be. Note that it also satisfies Euler's formula, where \\( v = 6 \\), \\( m = 8 \\), and \\( f = 4 \\).\n\n\\exercise{1}\nHow many edges does the hypercube graph \\( Q_n \\) have?\n\n\\exercise{2}\nHow can we find the general formula for \\( F(n) = 3^{n-1} - F(n-1) \\)?\n",
        "lec_27.tex": "\\lecture{27}{Mon 30 Oct 2023 14:00}{Partially Ordered Sets}\n\nOne last bit about graphs:\n\n\\begin{note}\n\tIf \\( G \\) is a graph with chromatic number \\( k \\), then \\( G \\) contains a subgraph \\( H \\) such that \\( \\chi (H)=k \\) and \\( \\chi (H-v) < k \\) for all \\( v \\in  V(H) \\). Such graphs are called \\textbf{critical graphs}.\n\\end{note}\n\n\\section{Partially Ordered Sets}\n\n\\subsection{Relations}\n\nFinally, a new section!\n\n\\begin{definition}\n\tA \\textbf{binary relation} on a set \\( X \\) is a subset \\( \\mathcal{R} \\subseteq X^2 = X \\times X \\).\n\\end{definition}\n\nIn other words, \\( R \\) is a set of some ordered pairs \\( (x,y) \\) with \\( x,y \\in  X \\).\n\n\\begin{eg}\n\tLet \\( X=\\{a,b,c\\}   \\). Then, one such relation is \\( \\mathcal{R}=\\{(a,a),(a,b),(b,a),(b,c),(c,c)\\}   \\).\n\\end{eg}\n\n\\begin{note}\n\tThese relations can be related to a directed graph where there can be loops and multiple edges between vertices.\n\\end{note}\n\n\\begin{eg}\n\tThe empty set \\( \\varnothing \\) is a relation.\n\\end{eg}\n\nWe use the word relation because it is a set of pairs \\( (x,y) \\) where \\( x \\) is ``related'' to \\( y \\) in some sense. We say that \\( (x,y) \\in \\mathcal{R} \\) if \\( y \\) is \\( \\mathcal{R} \\)-related to \\( x \\).\n\n\\begin{eg}\n\tLet \\( X=\\{1,2,3,4,5\\}   \\). \\( \\mathcal{R}=\\{ (x,y) \\in X^2 \\colon x < y\\} \\) is another way of writing \\( \\{(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(3,4),(3,5),(4,5)\\}    \\)\n\\end{eg}\n\nNote that relations can exist on infinite sets as well!\n\n\\begin{eg}\n\t\\( \\{(n,m) \\in \\mathbb{N}^2 \\colon n \\le m\\}   \\) is an ordering relation of the natural numbers.\n\\end{eg}\n\n\\begin{eg}\n\t\\( \\{(x,y) \\in \\mathbb{R}^2 \\times \\mathbb{R}^2 \\colon x \\text{ and } y \\text{ are orthogonal}\\} \\) is another relation containing all pairs of orthogonal vectors.\n\\end{eg}\n\n\\begin{remark}\n\tIn this course, we will focus on relations on two elements in the same set. However, more generally, relations can exist between a set \\( X \\) and another set \\( Y \\) (subsets of \\( X \\times  Y \\)).\n\\end{remark}\n\n\\begin{eg}\n\tLet \\( X \\) be the set of GT students enrolled in Fall 2023, let \\( Y \\) be the set of classes offered at GT in Fall 2023. Then, \\[\n\t\t\\{(S,C) \\in X \\times Y \\colon S \\text{ is registered for } C\\}\n\t.\\] is one such valid, real-life relation.\n\\end{eg}\n\n\\begin{notation}\n\tWhen \\( \\mathcal{R} \\) is a relation, we often write \\( x \\mathcal{R}y \\) to mean \\( (x,y) \\in \\mathcal{R} \\)\n\\end{notation}\n\n\\begin{eg}\n\tWe write \\( x < y \\) instead of \\( (x,y) \\in < \\).\n\\end{eg}\n\n\\begin{eg}\n\tIf \\( X \\) is a set of size \\( n \\), how many binary relations on \\( X \\) are there?\n\\end{eg}\n\nThere are \\( n \\cdot n = n^2\\) elements in \\( X \\times X \\), such that there are \\( 2^{n^2} \\) relations. Note that this value is \\( |\\mathcal{P}(X^2)| \\).\n\n\\subsubsection{Properties of Relations}\n\n\\begin{property}\n\tLet \\( \\mathcal{R} \\subseteq X^2 \\) be a relation on \\( X \\). \\( R \\) is:\n\t\\begin{enumerate}\n\t\t\\item reflexive: for all \\( x \\in X \\), \\( x \\mathcal{R}x \\).\n\t\t\\item irreflexive: for all \\( x \\in X \\), not \\( x \\mathcal{R}x \\).\n\t\t\\item symmetric: for all \\( x,y \\in X \\), if \\( x\\mathcal{R}y \\), then \\(y\\mathcal{R}x \\).\n\t\t\\item asymmetric: for all \\( x,y \\in X \\), if \\( x \\mathcal{R}y \\), then not \\( y\\mathcal{R}x \\). Note that all asymmetric relations are irreflexive.\n\t\t\\item antisymmetric: for all \\( x,y \\in X \\), if \\( x \\mathcal{R}y \\) and \\( y\\mathcal{R}x \\), then \\( x=y \\).\n\t\t\\item transitive: for all \\( x,y,z \\in X \\), if \\( x \\mathcal{R}y \\) and \\( y\\mathcal{R}z \\), then \\( x\\mathcal{R}z \\).\n\t\\end{enumerate}\n\\end{property}\n\n\\begin{eg}\n\tLet \\( X=\\{1,2,3\\}   \\), \\( R=\\{(1,1),(1,3),(2,2),(3,3)\\}   \\). What properties does this relation fall under?\n\\end{eg}\n\nThis relation is reflexive, not irreflexive, not symmetric, not asymmetric, antisymmetric.\n\n\\begin{note}\n\tThe only relation on the empty set \\( \\varnothing \\) is the empty set \\( \\varnothing \\).\n\\end{note}\n\n\\exercise{1}\nWhat properties are the relations on the lecture notes?\n",
        "lec_28.tex": "\\lecture{28}{Wed 01 Nov 2023 14:00}{Relations Continued}\n\n\\begin{eg}\n\t\\( \\le  \\) is...\n\\end{eg}\n\n\\begin{itemize}\n\t\\item reflexive because \\( x\\le x \\) for all \\( x \\in \\mathbb{N} \\).\n\t\\item not symmetric because \\( 1 \\le 2 \\) but \\( 2 \\not\\le 1 \\).\n\t\\item not asymmetric because \\( 1 \\le 1 \\).\n\t\\item antisymmetric because \\( x \\le y \\) and \\( y \\le x \\) implies \\( x = y \\).\n\t\\item transitive because \\( x \\le y \\) and \\( y \\le z \\) implies \\( x \\le z \\).\n\\end{itemize}\n\n\\begin{eg}\n\t< is...\n\\end{eg}\n\n\\begin{itemize}\n\t\\item irreflexive because \\( x < x \\) is false for all \\( x \\in \\mathbb{N} \\).\n\t\\item asymmetric because \\( x < y \\) implies \\( y \\not< x \\).\n\t\\item antisymmetric because the conditional is vacuously true.\n\t\\item transitive because \\( x < y \\) and \\( y < z \\) implies \\( x < z \\).\n\\end{itemize}\n\n\\begin{eg}\n\t= is reflexive, symmetric, transitive, and antisymmetric.\n\\end{eg}\n\n\\begin{eg}\n\t``\\( x + y \\) is even'' is reflexive, symmetric, and transitive.\n\\end{eg}\n\n\\begin{eg}\n\t``\\( x+y \\) is odd'' is irreflexive and symmetric.\n\\end{eg}\n\n\\begin{eg}\n\t``\\( x \\) and \\( y \\) have the same last digit'' is reflexive, symmetric, and transitive.\n\\end{eg}\n\n\\begin{definition}\n\tAn \\textbf{equivalence relation} is a relation that is reflexive, symmetric, and transitive.\n\\end{definition}\n\n\\begin{eg}\n\tLet \\( X=\\{\\text{all triangles in } \\mathbb{R}^2\\}   \\). Let \\( \\mathcal{R} = \\{(T_{1},T_{2})\\in X^2 \\colon T_{1} \\text{ and } T_{2} \\text{ are congruent}\\)\\} is an example of an equivalnce relation.\n\\end{eg}\n\n\\begin{eg}\n\tLet \\( G \\) be a graph. Let \\( \\mathcal{R}= \\{(u,v) \\in V(G)^2 \\colon \\text{there is a } uv\\text{-path in } G  \\)\\}.\n\\end{eg}\n\nThis example is transitive because if \\( (u,v) \\in \\mathcal{R} \\) and \\( (v,w) \\in \\mathcal{R} \\) (there is a \\( uv \\)-path \\( P_{1} \\) and a \\( vw \\)-path \\( P_{2} \\)), then by putting \\( P_{1}  \\) and \\( P_{2} \\) together we get a \\( uw \\)-walk. We know that if there is a \\( uw \\)-walk, then there must be a \\( uw \\)-path, as desired.\n\n\\begin{eg}\n\tFor any set \\( X \\), \\( X^2 \\) is an equivalence relation.\n\\end{eg}\n\n\\begin{note}\n\tThe empty relation on \\( X \\) is not an equivalence relation, because nothing in \\( X \\) is related to itself (unless, of course, \\( X \\) is the empty set).\n\\end{note}\n\n\\begin{definition}\n\tA \\textbf{partition} of a set \\( X \\) is a set \\( P \\) such that every element of \\( P \\) is a non-empty subset of \\( X \\), the union of all of sets in \\( P \\) is \\( X \\), and the sets in \\( P \\) are pairwise disjoint. \n\\end{definition}\n\n\\begin{eg}\n\tLet \\( X=\\{1,2,3,4,5,6\\}   \\). Then, \\( P = \\{\\{1,2,3\\}, \\{4\\}, \\{5,6\\}    \\}   \\) is a valid partition of \\( X \\).\n\\end{eg}\n\nGiven a partition \\( P \\) of \\( X \\), define a relation \\( E_p \\) on \\( X \\) as follows: \\[\n\tE_p \\coloneq \\{(x,y)\\in X^2 \\colon x,y \\text{ are in the same set in } P\\}  \n.\\] \n\n\\begin{eg}\n\tLet \\( X=\\{1,2,3,4\\}   \\), \\( P=\\{\\{1,2\\} ,\\{3,4\\}   \\}   \\). Then, \\[ E_p=\\{(1,1),(1,2),(2,2),(2,1),(3,3),(3,4),(4,4),(4,3)\\}   .\\]\n\\end{eg}\n\n\\begin{property}\n\t\\( E_p \\) is an equivalence relation on \\( X \\) (In our homework!).\n\\end{property}\n\nIt turns out that \\emph{every} equivalence relation arises in this way.\n\n\\exercise{1}\nProve that the relation \\( x-y \\in \\mathbb{Z} \\) on \\( \\mathbb{R} \\) is an equivalence relaion.\n\n\\exercise{2}\nWhat are the partitions of the empty set?\n\n",
        "lec_29.tex": "\\lecture{29}{Fri 03 Nov 2023 14:01}{Partial Orderings}\n\nWhat do we mean when we say that every equivalence relation arises in this way?\n\n\\begin{definition}\n\tLet \\( E \\) be an equivalence relation on a set \\( X \\). For each \\( x \\in X \\), let \\( \\left[ x \\right]_E = \\left\\{ y \\in X \\colon y ~E~ x \\right\\}  \\). This subset is called the \\textbf{equivalence class} of \\( x \\).\n\\end{definition}\n\n\\begin{definition}\n\tThe \\textbf{quotient} of \\( X \\) by \\( E \\) is the set of all equivalence classes \\( \\frac{X}{E} \\) defined by \\( \\{[x]_E \\colon x \\in X\\}   \\).\n\\end{definition}\n\n\\begin{eg}\n\tLet \\( X = \\{0,1,2,\\ldots ,20\\}   \\). Let \\( E=\\{(x,y) \\in X^2 \\colon x \\text{ and } y \\text{ have the same last digit}\\}   \\). Then, \n\t\\begin{itemize}\n\t\t\\item \\( [5]_E =  \\{5,15\\}   \\).\n\t\t\\item \\( [11]_E=\\{1,11\\}   \\).\n\t\t\\item \\( [0]_E=\\{0,10,20\\}   \\).\n\t\t\\item \\( [10]_E=\\{0,10,20\\}   =[20]_E\\)!\n\t\t\\item \\( \\frac{X}{E}=\\{[0]_E,[1]_E,[2]_E,\\ldots ,[9]_E\\} \\).\n\t\t\\item \\( |\\frac{X}{E}|=10 \\).\n\t\\end{itemize}\n\tNote that we don't need to include \\( [10]_E \\) in \\( \\frac{X}{E} \\) because the element is already listed!\n\\end{eg}\n\n\\begin{theorem}\n\tIf \\( E \\) is an equivalence relation on a set \\( X \\), then \\( \\frac{X}{E} \\) is a partition of \\( X \\) and \\( E=E_{\\frac{X}{E}} \\)\n\\end{theorem}\n\nThe moral of this is that equivalence relations and partitions are two different ways of describing the same structure.\n\n\\begin{eg}\n\tLet \\( G \\) be a graph, \\( E=\\{(u,v) \\in V(G)^2 \\colon \\text{there is a } uv\\text{-path in }G \\} \\) is an equivalence relation on \\( V(G) \\). The equivalence classes are the connected components of \\( G \\).\n\\end{eg}\n\n\\begin{definition}\n\tA \\textbf{partial order} on a set \\( X \\) is a relation that is reflextive, antisymmetric, and transitive.\n\\end{definition}\n\n\\begin{eg}\n\t\\( \\le  \\) and \\( \\ge  \\) on \\( \\mathbb{N} \\) or \\( \\mathbb{R} \\) are partial orders.\n\\end{eg}\n\n\\begin{eg}\n\tFor any set \\( X \\), the relation \\( \\subseteq \\) or (\\( \\supseteq \\)) on \\( \\mathcal{P}(X) \\) is a partial order.\n\\end{eg}\n\n\\begin{eg}\n\tConsider the relation \\( R \\) on \\( \\mathbb{N}^2 \\): \\[\n\t\tR \\coloneq \\{((n_{1},m_{1}),(n_{2},m_{2})) \\in (\\mathbb{N}^2)^2 \\colon n_{1} \\le  n_{2}, m_{1} \\ge  m_{2}\\}\n\t.\\] (Show that) this is a partial order on \\( \\mathbb{N}^2 \\)\n\\end{eg}\n\n\\begin{definition}\n\tA partially ordered set, or a \\textbf{poset}, is a pair \\( (X,R) \\) where \\( X  \\) is a set and \\( R \\) is a partial order on \\( X \\).\n\\end{definition}\n\n\\begin{eg}\n\t\\( (P([3]),\\subseteq) \\) is a poset.\n\\end{eg}\n\n\\begin{definition}\n\tLet \\( (X,R) \\) be a poset. We say that an element \\( y \\in X \\) \\textbf{covers} an element \\( x \\in X \\) if (1) \\( x\\neq y \\), (2) \\( xRy \\), and (3) there is no \\( z \\in X \\) such that \\( x\\neq z \\),\\( y\\neq z \\), \\( xRz \\), and \\( zRy \\)\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{Hasse Diagram} of \\( (X,R) \\) is a graph with vertex set \\( X \\) and an edges from \\( x \\) to \\( y \\) if \\( y \\) covers \\( x \\) with the extra condition that if \\( y \\) covers \\( x \\), then \\( y  \\) is drawn above \\( x \\).\n\\end{definition}\n\n\\exercise{1}\nHow many equivalence relations/partitions are there on a set of size \\( n \\)?\n\n\\exercise{2}\nShow that if \\( R \\) is a partial order, so is \\[\n\tR^* = \\{(x,y) \\in X^2 \\colon yRx \\} \n.\\] \n",
        "lec_30.tex": "\\lecture{30}{Mon 06 Nov 2023 14:07}{Posets}\n\n\\begin{notation}\n\tWe can use symbols like \\( \\le  \\), \\( \\preceq \\), and \\( \\trianglelefteq \\) to denote arbitrary partial orders. If there is no chance of confusion, we can write \\( x\\le y \\) to mean \\( x \\mathcal{R} y \\) where \\( \\mathcal{R} \\) is a partial order.\n\\end{notation}\n\n\\begin{definition}\n\tLet \\( (X, \\le ) \\) be a poset. Two elements \\( x,y \\in X \\) are \\textbf{comparable} if and only if \\( x\\le y \\) or \\( y\\le x \\).\n\\end{definition}\n\n\\begin{definition}\n\tA \\textbf{total order} is a partial order in which every two elements are comparable.\n\\end{definition}\n\n\\begin{eg}\n\t\\( (\\mathbb{N},\\le ) \\) is a total order.\n\\end{eg}\n\n\\begin{eg}\n\t\\( (\\mathcal{P}([3]), \\subseteq) \\) is \\textbf{not} a total order. Consider elements \\( \\{1\\} \\) and \\( \\{2\\}   \\), which are not comparable.\n\\end{eg}\n\n\\begin{note}\n\tIf \\( (X, \\le ) \\) is a poset with \\( |X|=n < \\infty \\) and the order is total, then the Hasse diagram is just a vertical line. In other words, the elements of \\( x \\) can be listed as \\( x_{1},x_{2},x_{3},\\ldots ,x_n \\) such that \\( x_{1}<x_{2}<x_{3}<\\ldots <x_n \\).\n\\end{note}\n\nThe situation with infinite sets is more complicated! There are very many Hasse diagrams you can get for a poset with infinitely many elements.\n\n\\begin{definition}\n\tLet \\( (X, \\le ) \\) be a poset. A \\textbf{chain} in \\( X \\) is a set \\( A \\subseteq X \\) such that every two elements in \\( A \\) are comparable.\n\\end{definition}\n\n\\begin{definition}\n\tLet \\( (X, \\le ) \\) be a poset. An \\textbf{antichain} is a set \\( X \\subseteq X \\) such that no two distinc elements of \\( A \\) are comparable.\n\\end{definition}\n\n\\begin{definition}\n\tThe \\textbf{height} of a poset is the length of its longest chain. The \\textbf{width} of a poset is the size of its largest antichain.\n\\end{definition}\n\n\\exercise{1}\nConsider the poset \\( (\\mathcal{P}([n]), \\subseteq) \\). What is its height and width?\n",
        "lec_31.tex": "\\lecture{31}{Wed 08 Nov 2023 14:02}{Posets Continued}\n\nContinuing on with partial order:\n\n\\begin{prop}\n\tThe height of \\( (\\mathcal{P}([n]), \\subseteq) = n+1\\)\n\\end{prop}\n\n\\begin{proof}\n\tTo show that two numbers \\( a \\) and \\( b \\) are equal, we can show \\( a\\le b \\) and \\( b\\le a \\). Therefore, we wil show that the height \\( \\ge n+1 \\). We can do this by finding a chain of size \\( n+1 \\). The chain \\[\n\t\t\\{\\varnothing \\subset \\{1\\} \\subset \\{1,2\\} \\subset \\{1,2,3\\} \\subset \\ldots \\subset \\{1,2,\\ldots ,n\\}\\}\n\t.\\] is one such chain of size \\( n+1 \\). Next, we show that the height of this poset is at most \\( n+1 \\). We need to argue that every chain has size at most \\( n+1 \\). \\par\n\tTake an arbitrary chain \\( A_{1}\\subset A_{2}\\subset \\ldots \\subset A_k \\). We want to show that \\( k\\le n+1 \\). Note that \\( |A_{1}|\\ge 0 \\), \\( |A_{2}|\\ge |A_{1}|+1\\ge 0+1=1 \\), \\( |A_{3}|\\ge |A_{2}|+1\\ge 1+1=2 \\), etc. such that \\( |A_k| \\ge k-1 \\). However, \\( |A_k| \\le n \\) since \\( A_k \\subseteq [n] \\). Therefore, \\( k-1 \\le |A_k| \\le n \\) and \\( k\\le n+1 \\).\n\\end{proof}\n\nWhat about the width of the poset?\n\n\\begin{notation}\n\t\\( \\left\\lfloor x \\right\\rfloor \\) denotes the largest integer at most \\( x \\).\n\\end{notation}\n\n\\begin{notation}\n\t\\( \\left\\lceil x \\right\\rceil  \\) denotes the smallest integer at least \\( x \\).\n\\end{notation}\n\n\\begin{theorem}\n\t(Sperner) The width of \\( (\\mathcal{P}([n]), \\subseteq) = \\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor} \\).\n\\end{theorem}\n\n\\begin{proof}\n\tFor any \\( k \\), the set \\( A \\subseteq [n] \\colon |A| = k \\) is an antichain in \\( (\\mathcal{P}([n]), \\subseteq) \\) of size \\( k \\). Therefore, the width of our poset is at least \\( \\binom{n}{k} \\). Therefore, \\[\n\t\t\\text{width} \\ge \\max_{k=0}^{n}\\binom{n}{k} = \\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor }\n\t.\\] Now, we need to show that every antichain in \\( (\\mathcal{P}([n])) \\) has size at most \\( \\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor} \\). Take arbitrary antichain \\( \\mathcal{A} \\). In other words, \\( \\mathcal{A} \\) is a collection of subsets of \\( [n] \\), none of which is a subset of another one. We wish to show that \\( |\\mathcal{A}| \\le \\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor} \\). Let \n\t\\[\n\t\t\\mathbb{S} \\coloneq \\{\\text{all permutations of }[n]\\}  \\qquad |\\mathbb{S}| = n!\n\t.\\] \n\tSay that a set \\( A \\subseteq [n] \\) of size \\( |A| = k \\) is a prefix of a permutation \\( \\pi =(x_{1},x_{2},x_{3},\\ldots ,x_n) \\in \\mathbb{S}_n \\) if \\( A=\\{x_{1},x_{2},\\ldots ,x_k\\}   \\). For example, if \\( n=3 \\), \\( \\pi =(3,1,2) \\), then \\( \\{1,3\\}   \\) is one such prefix. Then, for a permutation \\( \\pi =(x1,x_{2},\\ldots ,x_n) \\), its prefixes are \\[\n\t\t\\varnothing \\qquad \\{x_{1}\\} \\qquad \\{x_{1},x_{2}\\} \\qquad \\{x_{1},x_{2},x_{3}\\} \\qquad \\ldots \\qquad \\underbrace{\\{x_{1},x_{2},\\ldots ,x_{n}\\}}_{[n]}\n\t.\\] Observe that \\( \\pi  \\) has exactly one prefix of each size between \\( 0 \\) and \\( n \\). Also, observe that the prefixes of \\( \\pi  \\) form a chain. Then, we look at \\[\n\t(*) = \\sum_{\\pi \\in \\mathbb{S}} \\underbrace{\\sum_{A \\in \\mathcal{A}} \\underbrace{1[A \\text{ is a prefix of } \\pi ]}_{A \\text{ is a prefix of } \\pi ? 1 : 0}}_{\\le 1} \\le \\sum_{\\pi  \\in \\mathbb{S}_n} 1 = n!\n\t.\\] Note that this is because no two sets in \\( \\mathcal{A} \\) are comparable, and therefore no two sets can belong in the same chain as mentioned before. Then, we switch the order of the summations: \\[\n\t(*) = \\sum_{A \\in \\mathcal{A}} \\sum_{\\pi  \\in \\mathbb{S}} 1[A \\text{ is a prefix of } \\pi ]\n\t.\\] How many permutations \\( \\pi  \\) are there such that fixed \\( A \\) is a prefix of \\( \\pi  \\)?\n\\end{proof}\n\n\\exercise{1}\nHow many chains of size \\( n+1 \\) are there in \\( (\\mathcal{P}([n]), \\subseteq) \\)?\n\n\\exercise{2}\nShow that if \\( B \\) is a set of size \\( \\neq k \\), then \\(\\{\\text{subsets of } [n] \\text{ of size } k \\}  \\cup \\{B\\}\\) is not an antichain.\n\n\\exercise{3}\nGiven a set \\( A \\subseteq [n] \\) of size \\( |A|=k \\), how many permutations \\( \\pi  \\in \\mathbb{S} \\) are there such that \\( A  \\) is a prefix of \\( \\pi  \\)?\n",
        "lec_32.tex": "\\lecture{32}{Fri 10 Nov 2023 14:05}{More Posets}\n\nContinuing with the proof from last time:\n\n\\begin{proof}\n\tWe know the first \\( k \\) elements in \\( \\pi  \\) must be equal to \\( A \\), such that we have \\( (n-k)! \\) ways to order the rest of the elements. There is also \\( k! \\) factorial ways to order the first \\( k \\) elements, because \\( A \\) is a set and doesn't care about order! Therefore, the answer to our subproblem (inner sum) is \\( k!\\cdot (n-k)! = \\frac{n!}{\\binom{n}{k}} \\ge \\frac{n!}{\\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor}}\\). Then, we have \\[\n\t\t\\sum_{A \\in \\mathcal{A}} \\frac{n!}{\\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor}} = |A| \\cdot \\frac{n!}{\\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor}} \\le (*) \\le n!\n\t.\\] Dividing on \\( n! \\) on both sides, and multiplying by \\( \\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor} \\), we havae \\( |A| \\le \\binom{n}{\\left\\lfloor \\frac{n}{2} \\right\\rfloor} \\), as desired.\n\\end{proof}\n\n\\subsection{Maximal vs Maximum Elements}\n\nSome more definitions:\n\n\\begin{definition}\n\tLet \\( (X,\\le ) \\) be a poset. An element \\( x \\in X \\) is \\textbf{maximal} if there is no \\( y \\in X \\) such that \\( x<y \\).\n\\end{definition}\n\n\\begin{definition}\n\tLet \\( (X,\\le ) \\) be a poset. An element \\( x \\in X \\) is \\textbf{maxmimum} if for all \\( y \\in X \\), \\( y \\le x \\).\n\\end{definition}\n\n\\begin{definition}\n\tLet \\( (X,\\le ) \\) be a poset. An element \\( x \\in X \\) is \\textbf{minimal} if there is no \\( y \\in X \\) such that \\( x>y \\).\n\\end{definition}\n\n\\begin{definition}\n\tLet \\( (X,\\le ) \\) be a poset. An element \\( x \\in X \\) is \\textbf{minimum} if for all \\( y \\in X \\), \\( y \\ge x \\).\n\\end{definition}\n\n\\begin{note}\n\tIf an element is a maximum, every element is comparable to it and it is greater than all other such elements. \n\\end{note}\n\n\\begin{note}\n\tThere can be be at most 1 maxmimum element, but multiple maximal elements. Also, there can be at most 1 minimum element, but multiple minimal elements.\n\\end{note}\n\n\\begin{note}\n\tThe poset with two incomparable elements is an example of a poset with no maximum or minimum element!\n\\end{note}\n\n\\begin{eg}\n\t\\( (\\mathbb{N}, \\le ) \\) and \\( (\\varnothing, \\varnothing) \\) are posets with no maximal elements.\n\\end{eg}\n\n\\begin{prop}\n\tA non-empty finite poset has a maximal element.\n\\end{prop}\n\n\\begin{proof}\n\tSuppose, for the sake of contradiction, that there is no maximal element. Since \\( X \\neq \\varnothing \\), we can take some \\( x_{0} \\in X\\). By assumption, \\( x_{0} \\) is not maximal, so there is some \\( x_{1}\\in X \\) such that \\( x_{0}<x_{1} \\). \\( x_{1} \\) is also not maximal, so there is some \\( x_{2} \\in X \\) such that \\( x_{1}<x_{2} \\), etc. This is a contradiction, because our set is finite!\n\\end{proof}\n\n\\begin{definition}\n\tLet \\( (X,\\le ) \\) be a poset. Define \\[\n\t\t\\chi_c(X) = \\text{minimum } k \\text{ such that } X \\text{ can be partitioned into } k \\text{ chains}\n\t.\\] \n\\end{definition}\n\n\\begin{note}\n\tIn other words, this is the minimum \\( k \\) such that \\( X \\) can be colored with \\( k \\) colors such that elements of the same color are comparable.\n\\end{note}\n\n\\begin{definition}\n\tLet \\( (X,\\le ) \\) be a poset. Define \\[\n\t\t\\chi_a(X) = \\text{minimum } k \\text{ such that } X \\text{ can be partitioned into } k \\text{ antichains}\n\t.\\] \n\\end{definition}\n\n\\begin{eg}\n\tFor the poset \\( X=([3],\\subseteq) \\), what is \\( \\chi_c \\) and \\( \\chi_a \\)? \n\\end{eg}\n\nWe know that \\( \\chi_c \\le 3 \\) by example. We know that \\( \\chi_c \\ge 3 \\) because there is an antichain of size 3, and each element in this antichain must be in different chains. Therefore, \\( \\chi_c = 3 \\).\n\nSimilarly, we know that \\( \\chi_a \\le 4 \\) by example. We also know that \\( \\chi_a \\ge 4 \\) because there is a chain of size 4, and each element in this chain must be in different antichains. Therefore, \\( \\chi_a = 4 \\).\n\n\\begin{note}\n\tJust like we have \\( \\chi(G) \\ge \\omega(G) \\) in a graph, for a poset \\( (X, \\le ) \\), we can write \\[\n\t\t\\chi_c(X) \\ge \\text{width}(X) \\qquad \\chi_a(X) > \\text{height}(X)\n\t.\\] \n\\end{note}\n\n\\begin{theorem}\n\t(Dilworth) If \\( (X, \\le ) \\) is a finite poset, then \\( \\chi_c(X) = \\text{width of } (X,\\le )\\).\n\\end{theorem}\n\n\\begin{theorem}\n\t(Dual Dilworth) If \\( (X, \\le ) \\) is a finite poset, then \\( \\chi_a(X) = \\text{height of } (X,\\le )\\).\n\\end{theorem}\n\n\\exercise{1}\nShow that if \\( (X,\\le ) \\) is a finite poset and \\( x \\in X \\) is any element, then there exists a minimal element \\( m \\in X \\) and a maximal element in \\( M \\in X \\) such that \\( m \\le x \\le M \\).\n",
        "lec_33.tex": "\\lecture{33}{Mon 13 Nov 2023 14:05}{Proof of Dilworth's Theorems}\n\nWe will proceed by proving Dual Dilworth/Mirsky's Theorem:\n\n\\begin{proof}\n\tLet \\( (X,\\le ) \\) be a finite poset. Let the height of this poset be \\( k \\). Because we already know that \\( \\chi_a \\ge k \\), it suffices to show \\( \\chi_a \\le k \\). We can show this by finding a coloring of \\( X \\) using \\( k \\) or fewer colors such that elements of the same color to be incomparable.\n\n\tFor an element \\( x \\in X \\), let \\( c(x) \\) be the maxmimum size of a chain whose maximum element is \\( x \\). \n\t\\begin{observe}\n\t\t\\( 1 \\le c(x) \\le k \\).\n\t\\end{observe}\n\t\\begin{observe}\n\t\tIf \\( c(x) = c(y) \\) and \\( x\\neq y \\), then \\( x \\) and \\( y \\) are incomparable.\n\t\\end{observe}\n\tWhy is this? Suppose that \\( x \\) and \\( y \\) are comparable. Say \\( x > y \\). Then, \\( x \\) is the maxmimum element of a chain with size \\( c(y)+1 \\), so \\( c(x) \\ge c(y)+1 > c(y) \\), which is a contradiction.\n\n\tTherefore, \\( c \\) is a coloring of \\( X \\) using \\( k \\) colors such that elements of the same color are incomparable, as desired.\n\\end{proof}\n\nNow, for the proof of Dilworth's theorem:\n\n\\begin{proof}\n\tLet \\( (X, \\le ) \\) be a finite poset. Let \\( k \\) be the width of this poset. We wish to show that \\( \\chi_c(X) \\le k \\), i.e. there is a partition of \\( X \\) into \\( k \\) chains. We will use strong induction on \\( n = |X|\\).\n\t\\begin{description}\n\t\t\\item[Base case:] \\( n=0 \\), i.e. \\( X = \\emptyset \\). Then, \\( \\chi_c(X) \\le k = 0 \\), as desired.\n\t\t\\item[Step case:] Assume that the theorem is true for all posets with \\( <n \\) elements. We want to show that this theorem holds for a poset \\( (X,\\le ) \\) with \\( n \\) elements. Take any antichain \\( A \\subseteq X \\) of size \\( |A|=k \\).\n\t\t\t\\begin{observe}\n\t\t\t\tNo strictly larger antichain exists.\n\t\t\t\\end{observe}\n\t\t\tIn other words, we cannot add an element to \\( A \\) while maintaining that \\( A \\) is an antichain. Therefore, \\( x \\in X\\setminus A \\) is comparable to some element of \\( A \\) i.e. it is \\( \\le  \\) or \\( \\ge  \\) some element in \\( a \\in A \\). Partition \\( X \\) into \n\t\t\t\\begin{align*}\n\t\t\t\tX^+ &= \\{x \\in X \\colon x \\ge a \\text{ for some } a \\in A\\}  \\\\\n\t\t\t\tX^- &= \\{x \\in X \\colon x \\le a \\text{ for some } a \\in A\\}  \n\t\t\t.\\end{align*}\n\t\t\tNote that \\( X^+ \\cap X^- = A \\). Why? It is clear that \\( A \\subseteq X^+\\cap X^- \\). For the other direction, suppose \\( x \\not\\in A \\) belongs to both \\( X^+ \\) and \\( X^- \\). This means that \\( x > a \\) for some \\( a \\in A \\) \\textbf{and} \\( x<b \\) for some \\( b \\in A \\). This means that \\( a<x<b \\implies a<b \\), which is a contradiction because \\( A \\) is an antichain, and elements in \\( A \\) are not comparable.\n\n\t\t\tConsider the poset \\( (X^+, \\le) \\). Its width is at most \\( k \\) because \\( A \\) is an antichain in \\( X^+ \\). By the inductive hypothesis, \\( X^+ \\) can be partitioned into \\( k \\) chains. Since all elements of \\( A \\) belong to different chains, we can list these \\( k \\) chains as \\( c_{1}^+,c_{2}^+,\\ldots ,c_k^+ \\) where the minimum element of \\( c_i^+ \\) is \\( a_i \\). Similarly, \\( X^- \\) can be partitioned into \\( k \\) chains. We can list these \\( k \\) chains as \\( c_{1}^-,c_{2}^-, \\ldots , c_k^- \\) where the maximum element of \\( c_i^- \\) is \\( a_i \\). Then \\( X \\) can be partitioned into \\( k \\) chains \\( c_{1},c_{2},\\ldots c_k \\) where \\( c_i=c_i^+ \\cup c_i^- \\), as desired.\n\n\t\t\tHowever, there occurs a problem: what if one of \\( X^+ \\) or \\( X^- \\) contains only \\( A \\)? Then \\( X^+=X \\), which means that \\( A \\) is the set of minimal elements, or \\( X^-=X \\), which means that \\( A  \\) is the set of maximal elements, and we therefore cannot use the inductive hypothesis. If there are antichains of size \\( k \\) that are not one of these two sets, then we can switch \\( A \\) to that antichain. Our theorem only doesn't work when every antichain \\( A \\) of size \\( k \\) is either the set of all minimal or maximal elements.\n\\end{description}\n\\end{proof}\n"
    }
}